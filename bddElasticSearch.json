{
    "took": 12,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": {
            "value": 14,
            "relation": "eq"
        },
        "max_score": 1.0,
        "hits": [
            {
                "_index": "articles_index",
                "_id": "11",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "11",
                    "title": "fuzzy logic based mppt control for a pv system ",
                    "institutions": null,
                    "authors": null,
                    "abstract": null,
                    "keywords": "fuzzy logic controller (flc), maximum power point tracker (mppt), photovoltaic (pv)  i.",
                    "content": "the non-renewable energy sources are rapidly running out, while the electricity demand is increasing daily. to solve this problem, efficient and efficient electric power generation from renewable energy sources is required [1]. renewable energy is one of the forms of energy that society can rely on because it is unpolluted, pure, and has no limits. one type of power generation that uses renewable energy is the photovoltaic (pv) system [2]. to utilize less conventional energy, the pv system must subsequently be linked to the grid, either directly or via a backup battery bank. since the power produced by pv systems depends on radiation and temperature change, the pv framework has destitute productivity, [2]. for the control of the pv systems, there are different sorts of dc-dc converters such as buck converters, boost converters, and buck-boost converters. due to its output pick-up adaptability, a single-ended primary-inductor converter (sepic) acts as a buck-boost dc/dc converter, where it changes its output voltage agreeing to its duty cycle. unlike the customary buck-boost converter, the sepic converter includes a non-inverted output and it uses an arrangement capacitor to separate input from output [3]. the buck and buck-boost converters lose half of their input control due to input current arrangement exchange; for that reason, the two types of converters should be excluded from maximum power applications. the boost converter has a nonstop input current, but the output voltage is always bigger than the input, which may not accomplish maximum power exchange operation in a few cases, such as when the maximum voltage is less than the input [3]. this paper presents a fuzzy-based p&o strategy for an mppt standalone pv system. the proposed mppt can abuse the p",
                    "references": "references of the p&o strategy and eliminate its drawbacks. output has been separated into five fuzzy subsets. as the proposed strategy continuously exchanges maximum power from pv arrays, it optimizes the number of pv modules. ii. modelign of pv system photovoltaic is the technique and study connected to devices that directly convert sunlight into electricity utilizing photovoltaic semiconductors. direct conversion of solar energy into dc electrical energy can be achieved by photovoltaic cells [4]. the photovoltaic panel is made up of numerous cells that are connected in series ns or shunt nsh. where it may be mimicked by a current source coupled in parallel with a diode as described by and depicted in figure 1 [5].  fig. 1. circuit for a pv array. the following equations provide the output current: i = iph − id                                                                          (1)  i = iph − i0 [exp ( q(v+rsi) akbt ) − 1] − v+rsi rsh                            (2)  iph ∶ photo − current                            a ∶ ideality factor kb ∶ constant of boltzmann                  t ∶ cell temperature id ∶  current via a diode                        rs ∶ series resistance i0 ∶  current of saturation                     q ∶ electronic charge rsh ∶ shunt resistance                          v ∶ cell voltage i ∶ current of cell  the shunt resistance (rsh) is typically orders of magnitude larger than the series resistance (rs) [6]. therefore, it is common for the shunt and series resistances of a solar cell can be neglected to simplify the model. the resulting ideal voltage- current characteristic of the solar cell is given by equation (3). i = iph − i0 [e(qv kt) − 1]                                                        (3) iii. sepic converter power electronics researchers are working hard to create dc- dc converters with simpler designs and greater efficiency [7]. to maintain a constant output voltage, the suggested dc-to-dc converter employs a single-ended primary-inductor converter (sepic) architecture. the sepic converter is made up of a duty cycle switch s, a diode, two inductors (l1 and l2), two capacitors (c1 and c2), and a load resistor. figure 2 depicts the circuit diagram of a sepic converter.  a sepic is a dc-dc converter [8]. sepic are dc-dc converters that can output voltages that are b, larger than, or equal to the input voltage. the duty cycle of the control transistor affects the sepic converter's output voltage. the sepic converter is two converters in one: a boost converter followed by a buck-boost converter. it has the advantages of having a non-inverted output (the output voltage has the same polarity as the input voltage) , using a series capacitor to couple energy from the input to the output (which makes it more responsive to short-circuits), and being able to shut down completely: when the switch \"s\" is turned off, the output voltage drops to 0 v, accompanied by a significant transient discharge of charge.   fig. 2. simple circuit diagram of the sepic converter  figure 3 depicts the circuit when the power switch is switched on and off (in figures a and b, respectively).  figure 3a when the switch is on, the first inductor, l1 is charged from the input voltage source. the second inductor l2 absorbs energy from the first capacitor c1, leaving the output capacitor c2 to supply the load current.  fig. 3.a. the switch turned on.   fig. 3.b. the switch turned off. fig. 3. equivalent circuit diagram of the sepic converter when the switch is on and off.  when the switch is turned on, the input inductor is charged from the source, and the second inductor is charged from the first capacitor. no energy is supplied to the load capacitor during this time. the inductor current and capacitor voltage polarities are marked in this figure. when the power switch is turned off, the energy stored in the inductor is transferred. the energy stored is transferred through the diode and supplies the energy to the load [10], as shown in figure 3. b. the second inductor is also connected to the load during this time. the output capacitor sees a pulse of current during the off time, making it inherently noisier than a buck converter. the amount that the sepic converters increase or decrease the voltage depends primarily on the duty cycle and the parasitic elements in the circuit. the output of an ideal sepic converter is: vout = d 1−d vin                                                                       (4)  a sepic converter is to process the electricity from the pv system. this converter either increases or decreases the pv system voltage at the load. the proposed sepic converter operates in buck mode. iv. fuzzy logic control in the fuzzy logic maximum power point tracking (mppt) algorithm, the voltage and current at each instant k are measured to calculate the active power. the active power is then compared with the power at the previous instant (k-1) to obtain the change in power (δp(k)). similarly, the voltage at instant k is compared with the voltage at instant k-1 to obtain the voltage error (δv(k)) [11]. the power error is then divided by the current error to obtain the error (e). the error is then compared with the previous error to calculate the change in error (δe(k)). the error (e(k)) and the change in error (δe(k)) are then used as the crisp inputs to the fuzzy logic controller. the flow chart for the fuzzy logic mppt algorithm is shown in figure 4. in this work, the mamdani inference technique, a- type membership functions, and a 25-element rule base were used for the fuzzy logic control. the mamdani inference technique is efficient and straightforward in defining the fuzzy output sets, and it is more popular among researchers than other inference techniques [12]. the a-type or triangular membership function is used because it is simpler to split into low and high membership functions (mfs) than other membership functions. additionally, it has been observed that the triangular membership function has a faster response and less overshoot than other functions [13]. a 25-element rule base was used because it has been shown to perform well [14][15].     fig. 4. fuzzy logic maximum power point tracking (mppt) flow chart.  the following are the fuzzy rules in table 1, which are used for the desired mpp of push-pull converter pwm. the membership for input variables (dppv, dvpv) are shown in figure 5, and the membership for output variable (dvpv*) is shown in figure 6. all the functions are defined on a normalized interval [-1 1].   table. 1. fuzzy logic rules for the push-pull converter. nb, negative big; ns, negative small; ze, zero; ps, positive small; pb, positive big. ∆vpv*[o/p] ∆vpv*[i/p]   nb ns ze ps pb  nb ps nb nb nb ns ∆ppv*[i/p] ns ps ps ns ns ns  ze ze ze ze ze ze  ps ns ns ps ps ps  pb ns nb pb pb ps   fig. 5.  membership for input variables. fig. 6.  membership for output variable. v. simulation results the characteristics of the photovoltaic array that we use in this paper are given in table 2.   table. 2. electrical data of photovoltaic array.  variable value maximum power pmax = 200 w open circuit voltage voc = 36.1 v short circuit current isc = 200 a voltage at maximum power vmax = 29.3 v current at maximum power imax = 6.9 a  table 3 shows the sepic converter settings utilized in this study. the sepic converter is linked to the pv panel in the full model, and the duty cycle is regulated by the fuzzy logic controller.  table. 3. the sepic converter parameters. variable value switching frequency 20 khz load resistance rload= 4.29 ω inductance l1 l1 = 3.6 mh inductance l2 l2 = 0.9 mh capacitor c1 c1 = 180 µf capacitor c2 c2 = 6000 µf  the results are provided under standard test conditions; g=1000 w/m2; t=25°c and it is shown in figure 7.  fig. 7.a. current.  fig. 7.b. voltage.  fig. 7.c. power. fig. 7. voltage, current, and power output results; at g=1000 w/m2 and t=25°c. overall, using mppt and fuzzy logic to a sepic converter for a pv system result in considerable performance gains. increased power output, higher efficiency, decreased ripple, enhanced transient response, and resilience are examples of these enhancements. ⅵ. conclusions this paper presents the design of an off-grid photovoltaic system with a fuzzy logic mppt-controlled push-pull boost converter. the proposed system was simulated in matlab/simulink and tested under various weather conditions. the results showed that the fuzzy logic algorithm outperformed the conventional algorithms in terms of mppt accuracy and minimization of fluctuations, regardless of rapid changes in irradiance. references [1] g. mahendran, and kv. kandaswamy. “ant colony optimized tuned dc-dc converter.” international journal of computer applications (0975–8887) 108.11 (2013): 45-50. [2] j. dunia, and bmm mwinyiwiwa. “performance comparison between ćuk and sepic converters for maximum power point tracking using incremental conductance technique in solar power applications.” international journal of electrical, computer, energetic, electronic and communication engineering 7.12 (2013): 2510-2517. [3] m. cirrincione, m. pucci, and g. vitale, “growing neural gas (gng)- based maximum power point tracking for high-performance wind generator with an induction machine.” ieee transactions on industry applications 47.2 (2011): 861-872. [4] m. abdulkadir, as. samosir, s., and ahn. yatim. “modeling and simulation of a solar photovoltaic system, its dynamics and transient characteristics in labview.\" international journal of power electronics and drive system (ijpeds) 3.2 (2013): 185-192. [5] h. bouzeria, c. fetha, t. bahi, i. abadlia, z. layate, and s. lekhchine. “fuzzy logic space vector direct torque control of pmsm for photovoltaic water pumping system.” energy procedia 74 (2015): 760- 771. [6] ym. chen, yc. liu, sc. hung, and cs. cheng. “multi-input inverter for grid-connected hybrid pv/wind power system.” ieee transactions on power electronics 22.3 (2007): 742-750. [7] s. ganesh, j. janani, and gb. angel. “a maximum power point tracker for pv panels using sepic converter.”international journal of electrical, computer, energetic, electronic and communication engineering 8.2 (2014): 637-642. [8] r. vijayabalan, and s. ravivarman. “z source inverter for photovoltaic system with fuzzy logic controller.” international journal of power electronics and drive system (ijpeds) 2.4 (2012): 371-379. [9] a. ramkumar, and svs. florence. \"analysis of single phase ac-dc sepic converter using closed loop techniques.” international journal of advanced research in electrical, electronics and instrumentation engineering 4.2 (2015): 193-201. [10] g. tadi, and p. ramamurthyraju. “analysis of sepic for pv- applications using pi controller and current mode control.\" international journal for scientific research & development 1.9 (2013): 175-180. [11] j. li, and h. wang. “a novel stand-alone pv generation system based on variable step size inc mppt and svpwm control.\" in proceedings of the 2009 ieee 6th international power electronics and motion control conference, wuhan, china, 17-20 may 2009. pp. 2155-2160. [12] c. wang. “a study of membership functions on mamdani-type fuzzy inference system for industrial decision-making.” master's thesis, lehigh university, bethlehem, pa, usa, 2015. [13] ma. usta, ö. akyazi, and i̇h. altaş. “design and performance of solar tracking system with fuzzy logic controller used different membership functions.” in proceedings of the 2011 7th international conference on electrical and electronics engineering (eleco), bursa, turkey, 1-4 december 2011. pp. ii-381-ii-385.mudi, r.k.; pal, n.r. a robust self- tuning scheme for pi-and pd-type fuzzy controllers. ieee trans. fuzzy syst. 1999, 7, 2–16. [14] rk. mudi, and nr. pal. “a robust self-tuning scheme for pi- and pd- type fuzzy controllers.” ieee transactions on fuzzy systems 7.1 (1999): 2-16. [15] a. shehata, h.  metered, and wah. oraby. “vibration control of active vehicle suspension system using fuzzy logic controller.” in vibration engineering and technology of machinery, edited by w. a. oraby, 389- 399. springer, berlin, heidelberg, 2015.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1wtaWbSfEGRvzqzQJfVt0iZBVnAUuLv-X&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "12",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "abstract.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "12",
                    "title": "the programmer’s assistant: conversational interaction with a large language model for software development ",
                    "institutions": "ibm research ai cambridge, ma, usa fernando martinez ibm argentina la plata, buenos aires, argentina stephanie houde ibm research ai cambridge, ma, usa ibm research ai cambridge, ma, usa justin d. weisz ibm research ai yorktown heights, ny, usa",
                    "authors": [
                        "steven i. ross",
                        "fernando martinez\n",
                        "michael muller",
                        "justin d. weisz\n"
                    ],
                    "abstract": "large language models (llms) have recently been applied in soft- ware engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. when used within development tools, these systems typically treat each model in- vocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. this approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. we developed a prototype system – the programmer’s assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent llm. through an evaluation with 42 partici- pants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discus- sions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the llm. despite skepti- cal initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capa- bilities, the quality of its responses, and its potential for improving their productivity. our work demonstrates the unique potential of conversational interactions with llms for co-creative processes like software development. ccs concepts • human-centered computing → hci theory, concepts and models; • software and its engineering → designing software; • computing methodologies → generative and developmental approaches. this work is licensed under a creative commons attribution-noncommercial-noderivs international 4.0 license. iui ’23, march 27–31, 2023, sydney, nsw, australia © 2023 copyright held by the owner/author(s). acm isbn 979-8-4007-0106-1/23/03. https://doi.org/10.1145/3581641.3584037",
                    "keywords": "code-fluent large language models, foundation models, conversa- tional interaction, human-centered ai",
                    "content": "software development is a highly skilled task that requires knowl- edge, focus, and creativity [27, 28]. many techniques have been developed to enhance the productivity of software engineers, such as advanced code repositories [86], knowledge repositories [39], q&a sites [1], and pair programming practices [18]. collaborative software engineering is especially promising, given that profes- sional software development is rarely a solo activity and relevant knowledge and expertise are typically distributed widely within an organization [68]. many efforts have focused on incorporating col- laborative technologies into software development environments (e.g. [8, 25, 26, 58, 101]). the pioneering work of rich and waters on the programmer’s apprentice [70] presented a novel concept of a knowledgeable auto- mated assistant – in effect, an artificial collaborative partner – that could help software engineers with writing code, designing soft- ware systems, and creating requirements specifications. at the time, ai technologies and computing resources were not sufficient to fully implement their vision. in the intervening years, an increase in computational power, the availability of large corpora of language and code data, and the development of deep neural networks have made new approaches to achieving their goals worth exploring. recently, models leveraging the transformer architecture [96] have been developed to perform domain-specific software engi- neering tasks, such as translating code between languages [75], generating documentation for code [36, 38, 97, 98], and generating unit tests for code [92] (see talamadupula [90] and allamanis et al. [5] for surveys). recently developed foundation models – large language models that can be adapted to multiple tasks and which exhibit emergent behaviors for which they have not been explic- itly trained [14] – have also proven to be capable with source code. while the intent of training llms such as gpt-2 [64] and gpt-3 [17] was to give them mastery of natural language, it quickly became apparent that the presence of code in their training corpora had given them the ability to generate code based on natural language descriptions [49]. the codex model [24] was then produced by fine- tuning gpt-3 on a large corpus of source code data, leading to the development of copilot [32], a tool that helps software engineers by autocompleting code as it is being written. experimentation with copilot has shown its ability to perform additional tasks, such as explaining code, generating documentation, and translating code between languages [6]. although autocompletion interfaces are useful and valuable when the system can discern the developer’s intent, there are many instances where that is insufficient. for example, the developer may have a good idea of what they want to do, but may be unclear on what functions, libraries, or even algorithms to employ. they may even have general programming questions that need to be answered before they are able to write any code. in this paper, we seek to understand whether modern develop- ments in code-fluent foundation models – large language models that have been fine-tuned on source code data – are sufficient to support a conversational agent that can act as an assistant in the software development process. we developed the programmer’s assistant to explore the capabilities that conversational interaction could enable and the extent to which users would find conversa- tional assistance with programming tasks desirable and useful. we hypothesize that a conversational system may provide a flexible and natural means for interacting with a code-fluent llm. conversational interaction could enable users to pursue their ques- tions in a multiple exchange dialog (as observed by barke et al. [13]) that allows them to ask follow-up questions and refine their inquiries. a conversational programming assistant could ask the user clarifying or disambiguating questions to help it arrive at the best answer. it could also provide multiple types of assistance to the user beyond simply generating code snippets, such as engaging in general discussion of programming topics (e.g. [22, 71]) or help- ing users improve their programming skills (as observed in other studies of automating technologies [99]). our paper makes the following contributions to the iui commu- nity: • we provide empirical evidence that a conversational pro- gramming assistant based on a state-of-the-art, code-fluent foundation model provides valuable assistance to software engineers in a myriad of ways: by answering general pro- gramming questions, by generating context-relevant code, by enabling the model to exhibit emergent behaviors, and by enabling users to ask follow-up questions that depend upon their conversational and code contexts. • we show how different interaction models – conversation, direct manipulation, and search – provide complementary types of support to software engineers with tradeoffs be- tween the user’s focus and attention, the relevance of sup- port to their code context, the provenance of that support, and their ability to ask follow-up questions. • we motivate the need to further understand how to design human-centered ai systems that enhance the joint perfor- mance of the human-ai collaboration. 2 related work we discuss three areas of related work that have either motivated our study of conversational programming assistance or provided the technical foundations for it. we begin by briefly summarizing rich and waters’ visionary work on the programmer’s appren- tice [70], followed by summarizing work on code-fluent foundation models and human-centered evaluations of how these models im- pact software engineers’ work. finally, we discuss conversational interaction and how it might be employed to provide more flexible and sophisticated assistance to software engineers. 2.1 the programmer’s apprentice our work is inspired by the vision laid out by rich and waters [70], which describes an artificial agent that can act as an intelligent as- sistant for software engineers by providing advice, catching errors, and handling routine details throughout the software development process. the programmer’s apprentice [70] relied on a knowledge base of “clichés,” which are formal, structured versions of what are known today as software design patterns [31]. it used a hy- brid reasoning system capable of special-purpose reasoning based on frames and a plan calculus, along with general purpose logical reasoning. although natural language interaction was envisioned, the original prototype implementation ultimately used a stylized command language. we view our work as a conceptual successor to the programmer’s apprentice, as it enables the natural language interaction that the programmer’s apprentice lacked. 2.2 code-fluent foundation models and human-centered evaluations of programming assistance generative models based on the transformer architecture [96] have recently been applied to the domain of software engineering. code- fluent large language models are capable of generating code from natural language descriptions [105], translating code from one lan- guage to another [75], generating unit tests [92], and even gener- ating documentation for code [36, 38, 97, 98]. these models are probabilistic systems, and as such, do not always produce perfect results (e.g. code that is free of syntax or logical errors). nonetheless, weisz et al. [102] found that software engineers are still interested in using such models in their work, and that the imperfect outputs of these models can even help them produce higher-quality code via human-ai collaboration [103]. new tools based on code-fluent llms are actively being devel- oped. github copilot1 is described as “your ai pair programmer.” it is optimized for the code autocompletion use case: given a starting snippet such as a method’s documentation, signature, or partial implementation, copilot completes the implementation. copilot is based on the openai codex model [24], a 12 billion parameter ver- sion of gpt-3 [17, 49], fine-tuned on code samples from 54 million public software repositories on github. empirical evaluations of 1https://github.com/features/copilotthis model have shown that, although the quality of its outputs is quite good, those outputs may still be problematic [57]. echoing the results from weisz et al. [103], human-centered evaluations of copi- lot have found that it increases users’ feelings of productivity [109], and that almost a third (27%) of its proposed code completions were accepted by users. in a contrasting evaluation, vaithilingam et al. [95] found that while most participants expressed a preference to use copilot in their daily work, it did not necessarily improve their task completion times or success rates. yet, in a study by kalliamvakou [40], developers working with copilot were able to implement a web server in javascript 55% faster than developers who did not use copilot. a grounded theory analysis of how programmers interact with copilot [13] found that their interactions varied depending upon whether they were accelerating tasks that they already knew how to do or if they were exploring solutions to problems that they were less sure about. autocompletion was effective when developers were operating in “acceleration mode” and relied on the model to produce short completions that could be verified quickly. in “exploration mode,” however, the interaction was more awkward. developers would communicate with copilot by typing comments and seeing what copilot generated in response. then, they would modify their comments to explore other ways of prompting a re- sponse. ultimately, the comments used to prompt the model would be deleted after the relevant code was generated, indicating that their value was largely in driving a back-and-forth, yet context free, dialog with the model to coerce it to produce the desired results through an iterative refinement process. in this paper, we fully commit to a context-aware conversational style of interaction with a code-fluent llm and assess the value it provides to users. 2.3 conversational interaction and analysis 2.3.1 conversational interaction. using natural language to inter- act with technology has had a long research history [2], starting in the 1960s with pattern-matching approaches like eliza [104], and continuing to today with state-of-the-art large language model- based conversational systems [107] such as meena [3] and blender- bot [84]. these systems are intended to address the problem of open-domain dialog, with a goal of realistically engaging in con- versation, but not particularly in a goal-directed or task-oriented manner. task-oriented chatbots are typically built with frameworks such as the microsoft bot framework2, google dialogflow3, and ibm watson assistant4. they operate using pre-defined dialogue trees and use natural language processing to detect conversational intents and extract contextual entities. this structure enables the creation of special purpose, but fairly limited and rigid, conversational agents. there have been several recent attempts to investigate conver- sational programming assistance. kuttal et al. [42] conducted a wizard of oz study in which a pair programmer was replaced with a conversational agent, and they found that “agents can act as ef- fective pair programming partners.” the pact system [106] is a chatbot that assists programmers adjusting to new programming 2https://dev.botframework.com/ 3https://cloud.google.com/dialogflow 4https://www.ibm.com/products/watson-assistant/artificial-intelligence environments. pact is structured as a discrete question-answering system based on a neural machine translation approach, but it doesn’t maintain a conversational context. 2.3.2 conversation analysis. conversation is a form of interaction between people that enables robust communication. conversation analysis [76] is a method for understanding the natural structure of human conversational interaction. it catalogs different patterns of conversational acts and how they are utilized by interlocutors in order to attain a wide variety of goals. recently, conversation anal- ysis has been adapted to describe patterns of interactions between humans and artificial conversational agents in order to aid in the design of chatbots [50]. we apply techniques from conversation analysis in our study of conversational programming assistance. 3 the programmer’s assistant in order to explore conversational programming assistance, we created a functional prototype system called the programmer’s assistant. our prototype, shown in figure 1, combines a code editor with a chat interface. the code editor was implemented using the microsoft monaco editor5 embedded in a react wrapper6. the chat user interface was implemented using the react-chatbot-kit7 framework. to drive the conversational interaction, we employed openai’s codex model [24], accessed through its web api. we developed our prototype as a lightweight coding environ- ment in order to examine the user experience of interacting with a conversational assistant. our work was exploratory in nature, and thus we did not have specific design goals for the prototype beyond integrating a code editor with a code-fluent llm. we also did not attempt to target the prototype for a specific class of users (e.g. novices or experts) or use cases (e.g. writing code vs. learning a new programming language), as we wanted any value provided by conversational assistance to emerge from our user study. we also did not implement the ability to run or debug code in our prototype as we wanted to explore the nature of the conversational interac- tion rather than having users focus extensively on the production of working code. when designing how users would interact with the program- mer’s assistant, we decided that it should be available on demand and not monitor the user’s work in progress or give unsolicited suggestions or advice, in keeping with the conversational agent interaction model proposed by ross et al. [73, 74]. this approach was supported by feedback from prospective users who were con- cerned about the assistant providing criticism of unfinished efforts in progress or distracting them while they worked. instead, we force initiative onto the user and only have the assistant respond to their requests. in this way, the assistant can provide help when requested without undesirable interruptions that can distract or interfere with the user’s flow. when a user interacts with the assistant, we keep track of their selection state in the code editor. if a user sends a message to the assistant without any code selected in the editor, then that message (along with the prior conversational context) is passed directly to the model. if a user sends a message to the assistant with new code 5https://microsoft.github.io/monaco-editor/ 6https://www.npmjs.com/package/@monaco-editor/react 7https://fredrikoseberg.github.io/react-chatbot-kit-docs/selected in the editor (i.e. code that wasn’t previously selected when they sent their last message), then that code is appended to the message before being communicated to the model. the model may produce multiple types of responses to a user’s message. we treat each type of response differently in the ui. • responses that do not contain code are always rendered in the chat ui (figure 1e). • responses containing short code snippets (≤ 10 lines) are rendered inline in the chat ui (figure 1g). • responses containing longer code snippets (> 10 lines) show the code in a pop-up window (figure 2a), with a proxy entry in the chat transcript (figure 2b) that allows users to re- display the code window after it has been closed. non-code text in the response remains in the chat transcript. the assistant never directly modifies the contents of the user’s source code; rather, any code the user desires to transfer from the chat takes place via copy/paste. figure 1 shows a screenshot of a real, sample conversation, in which the user asks a question that results in an inline response, then requests an explanation of some code in the editor, and then re- quests further elaboration. figure 2 shows an example conversation that resulted in the generation of a longer code sample, shown in a popup window. this example shows how the assistant produced an incomplete solution, followed by criticism from the user regarding the missing code, and resulting in an apology and the generation of a complete solution. 3.1 supporting conversational interaction we enabled codex to conduct a conversational interaction by prompt- ing it with a conversational transcript and a request to produce the next conversational turn. the prompt establishes a pattern of conversation between a user and a programming assistant named socrates. it provides several examples of socrates responding to gen- eral coding questions, generating code in response to a request, and accepting code as input. it establishes a convention for delimiting code in the conversation, making it easy to parse for display in the ui. it also establishes an interaction style for the assistant, directing it to be polite, eager, helpful, and humble, and to present its re- sponses in a non-authoritative manner8. because of the possibility that the model might produce erroneous answers or incorrect code (as discussed in weisz et al. [102]), we felt it was important that the assistant convey a sense of uncertainty to encourage users to not accept its results uncritically to avoid over-reliance (e.g. as observed in moroz et al.’s study of copilot [51], and discussed more gener- ally in ashktorab et al. [9]) as well as automation bias [45, 46, 65]. we present the full text of the prompt used for the assistant in appendix d. 3.2 architecture & ui design the programmer’s assistant communicates with the codex api via a proxy server that forwards requests from the react client. the proxy also rate-limits access to conform to the api’s policy, and it logs ui events from the client (e.g. requests, responses, and ui 8the assistant’s use of non-authoritative responses was encoded into the llm prompt; output token probabilities from the llm were not utilized to influence the assistant’s response. interactions) in a back-end database. to address inconsistencies in the style or formatting of code generated by codex, the proxy server reformats all code segments using the black code formatter 9 before transmitting them to the client ui. the client maintains the transcript of the ongoing conversation. each time the user sends a message in the chat, the client constructs a new prompt for the model by concatenating the initial prompt, the chat transcript, and the user’s new utterance, and makes a re- quest for the model to complete the transcript. this completion request also specifies a stop sequence of tokens to prevent the model from generating both sides of the conversation (e.g. what the model thinks the user’s next utterance might be after the as- sistant’s response). given the api’s limitation on context length (4,096 tokens for both the prompt and model response), we silently “forget” older exchanges in the chat transcript when constructing the prompt to ensure that our completion request remains within bounds. nonetheless, the entire conversational history remains visible to the user in the ui. the client ui provides a loose coupling between the source code editor and the chat interface. users can hide the chat pane when they wish to focus solely on their code, and re-engage with it when they desire assistance. code selected in the editor is included in the conversation in order to couple the code context with the conversation. easily-accessible buttons are provided in the ui to copy code responses from the assistant to the clipboard. 3.3 handling model limitations while developing the programmer’s assistant, and in early pilot testing, we experienced some quirks and shortcomings of the model and our approach to using it for conversational interaction. one limitation stemmed from the fact that the model sometimes pro- duced incorrect responses (e.g. code with syntax errors), incomplete responses (e.g. code that was missing functionality), irrelevant re- sponses (e.g. responses not related to the user’s question), or insub- stantial responses (e.g. “i don’t know”). because of the probabilistic nature of model inference, re-prompting the model would some- times produce a more correct or appropriate response. thus, we added the ability for users to “try again,” either by asking in the chat or by clicking a button in the ui (figure 1c). this feature removes the assistant’s last response from the context presented to the model and then re-invokes the model with an increased temperature10. although it is possible for transformer models such as codex to produce multiple possible responses to a single prompt, we only request a single response in order to speed up response time as well as to preserve the token budget for conversational context. thus, the “try again” feature provides an alternate way to produce a wider variety of responses. during pilot testing, we noticed that the assistant sometimes happened to generate the same response to multiple, unrelated re- quests. in these cases, the assistant tended to get “stuck” in a pattern of repeating the same response and was unable to resume normal conversation. to avoid this problem, we automatically execute a 9https://black.readthedocs.io/en/stable/ 10temperature is a parameter in a generative model that specifies the amount of variation in the generation process. higher temperatures result in greater variability in the model’s output. a b h g f e c d figure 1: the programmer’s assistant. the user interface provides a code editor on the left (a) and a chat pane on the right (b). the “try again” button (c) allows users to ask the assistant to generate an alternate response to the most recent question. the “start over” button (d) resets the conversational context for the assistant, but maintains the chat transcript in the ui. in this example, we show the assistant introduce itself to the user (e). next, the user asks a general programming question (f), for which the assistant provides an inline code response (g). the user then asks a question about code selected in the editor (h), followed by a series of follow-up questions. “try again” operation in the background when we see identical consecutive responses from the assistant. finally, we noticed that the accumulation of conversational con- text sometimes resulted in the assistant becoming fixated on some portion of the earlier conversation. for example, it might respond to a question with portions of the prompt or of earlier conversation, and become less responsive to newer requests. to address this issue, we introduced a “start over” feature, accessible via the chat or by clicking a button in the ui (figure 1d), that resets the context to the original prompt, forgetting the rest of the conversational history. we preserve the chat transcript in the ui, but delineate the break in the assistant’s memory with an annotation in the chat transcript. these annotations are added both for “try again” and “start over.” 3.4 sample conversation we provide a real sample conversation with the programmer’s assistant in listing 1. this conversation begins with the assistant greeting the user (line 1). next, the user asks a general python programming question (line 4), to which the assistant responds with a non-authoritative remark (“i think...”) and a code snippet (line 9). the user next asks a follow-up question that depends on their previous question and the assistant’s response (line 11), to which the assistant provides another code snippet (line 15), satisfying the user’s request. the user then switches topics and asks the assistant to write a fibonacci function (line 17), and the assistant again responds with a non-authoritative remark (“i will give it a try,” line 20) and a block of code. the user then asks how the function works (line 30) and the assistant provides an adequate description (line 32). figure 2: code preview. long code responses (> 10 lines) are presented in a pop-up window (a), with a button to copy the code and close the window. this code can be re-accessed from the chat transcript by clicking the proxy icon (b). next, the user asks the assistant to re-implement the function in a different way (line 37), again leveraging the ability to ask follow-up questions. the assistant produces an alternative implementation that conforms to the user’s request (line 41). the user follows up with a question that depends on multiple past utterances and re- sponses in the chat transcript (line 47), and the assistant produces a relevant response (line 49). the conversation closes with the user thanking the assistant (line 53) and the assistant acknowledging their gratitude (line 55). listing 1: a conversation with the programmer’s assistant. code presented by the assistant is listed in bold face. 1 socrates: hello! my name is socrates your programming 2 assistant. what can i help you with today? 3 4 user: how do i get a list of the keys in a 5 python dictionary? 6 7 socrates: i think you would do something like this: 8 9 list(my_dict.keys()) 10 11 user: what if i just wanted to iterate over them? 12 13 socrates: i think you would do something like this: 14 15 for key in my_dict: 16 17 user: write a function that returns the nth value 18 in the fibonacci sequence. 19 20 socrates: i will give it a try. 21 22 def fib(n): 23 if n == 0: 24 return 0 25 elif n == 1: 26 return 1 27 else: 28 return fib(n - 1) + fib(n - 2) 29 30 user: how does this work? 31 32 socrates: the fibonacci sequence is defined by the 33 recurrence relation f(n) = f(n-1) + f(n-2). 34 this function implements that recurrence 35 relation. 36 37 user: can this be done without recursion? 3839 socrates: i think so. how about this? 40 41 def fib(n): 42 a,b = 0,1 43 for i in range(n): 44 a,b = b,a+b 45 return 46 47 user: which version is better? 48 49 socrates: i think the recursive version is more 50 elegant , but the iterative version is more 51 efficient. 52 53 user: thanks! 54 55 socrates: you 're welcome! 4 empirical study of conversational programming assistance we conducted an empirical user study of the programmer’s assis- tant to assess whether conversational assistance provides value in a software engineering context11. our inquiry focused on the user experience and value of conversational interactions grounded in code. we therefore designed a qualitative study to investigate atti- tudes toward a conversational programming assistant: do people enjoy interacting conversationally, what kinds of questions do they ask, and how does the experience compare to other forms of pro- gramming support such as searching the web? we note that prior studies (e.g. [103, 105, 109]) conducted quantitative examinations of the use of llms in code work; our study is akin to weisz et al.’s qualitative examination of software engineers’ attitudes toward working with models that may fail to produce working code [102]. to address our questions, we deployed the programmer’s assis- tant within our organization – a global technology company – and invited people to try it out and give us feedback on their experience. we invited people with varying levels of programming skill in order to obtain a wide range of feedback on the kinds of use cases for which the tool could provide assistance. 4.1 tasks we set up the programmer’s assistant as a playground environment that participants could try out with a few sample programming problems. we created a tutorial to orient participants to the assis- tant, its capabilities, and how to interact with it. we also created four programming challenges focused on writing code, document- ing code, and writing tests for code. we designed these challenges to expose participants to a broad range of the assistant’s capabilities. for each of these challenges, we explicitly did not evaluate metrics such as the participant’s productivity, the quality of their solutions, or the time taken to produce them, as the focus of our study was to understand the utility of conversational interaction. we selected python as the language used for the tutorial and challenges because of its general popularity [21] and the fact that it was well-supported by our underlying llm [24]. 4.1.1 tutorial. all participants were first introduced to the pro- grammer’s assistant through a tutorial. the tutorial walked each 11for historical context, we note that our study was completed before the public release of chatgpt [60], which has subsequently demonstrated the application of conversational assistance for programming tasks [34]. participant through 10 sample interactions to give them a feeling for what the assistant could do and how to interact with it. the tutorial demonstrated how to ask questions, how to request code to be generated, and how to evaluate existing code. it did not specifi- cally cover how to generate documentation or unit tests. tutorial instructions were provided within the code editor. we include the specific text used for the tutorial in appendix b. 4.1.2 programming challenges. after completing the tutorial, par- ticipants unlocked four programming challenges. two of the chal- lenges involved coding problems (writing a queue class and writing code to create a scatterplot of data in a csv file), one involved doc- umenting a given function (an implementation of a graph search algorithm), and one involved writing unit tests for a given func- tion (computing the greatest common divisor of two arguments). although the programmer’s assistant was visible and available for use, we provided no specific requirement that it actually be used to complete the challenges. after participants completed their solution to a challenge, they submitted it by clicking a button in the ui. the code editor used in the programmer’s assistant was not a fully-functional ide and did not provide syntax checking or the ability to run, test, or debug code. due to these limitations, participants were asked to submit their solutions when they felt they had completed the challenge to their own satisfaction. 4.2 participants to recruit participants for our study, we posted internal advertise- ments in various communications channels focused on software engineering. our advertisements stated that we were evaluating a conversational programming assistant, but were kept deliberately vague in order to minimize the impact on peoples’ expectations of the experience. our advertisement yielded a pool of 140 potential participants. in order to recruit a diverse sample, we used a screening survey that asked about their job role, their familiarity with and recency of use of python, and their availability to participate in our study. we accepted participants into the study on a rolling basis, selecting participants to capture a range of programming experiences and ensure balanced gender representation. we conducted periodic re- views to determine whether we were learning something new from each participant or if we had reached the point of saturation [7]. we stopped collecting data after running 42 participants as we were no longer observing any new behaviors or gleaning any new insights. the programmer’s assistant implementation and configuration were held constant over the course of the study; no changes to the ui design or llm prompt were made. our participants had the following self-identified characteristics: • job role: 19 software engineers, 12 researcher/scientists, 3 software architects, 2 data scientists, 1 machine learning engineer, 1 systems test engineer, 1 business analyst, 1 manager, 1 marketer, and 1 consultant. • gender: 21 female, 19 male, 1 gender variant / non-con- forming, and 1 preferred not to say. • python experience: 17 participants had 3+ years of python experience, 11 had 1-3 years, 11 had less than 1 year, and 3 were not familiar with python.• recency of python use: 29 participants had written python code within the past month, 4 within the past year, 5 within the past 5 years, and 4 had not written python code within the past 5 years. we provide full demographic information for individual partici- pants in appendix e. 4.3 procedure participants completed the study on their own time, independently and without moderation. each participant was provided with a web link to a pre-study survey that described the nature of the study and the tasks that they would be expected to perform. they were then directed to the programmer’s assistant to complete the tutorial and the four programming challenges. when participants indicated they were finished with the challenges12, they were directed to a final post-study survey. complete sessions generally required about an hour of effort, though some participants spread their effort across a longer period of time and across multiple sessions. participants were compensated for their time at a rate equivalent to us $15/hr. 4.4 measures we collected a variety of data in our study from three sources: (1) surveys. we employed three surveys in the study: a pre- study survey to collect demographic information, a pre-task survey to gauge expectations of the conversational user ex- perience, and a post-task survey to assess actual user expe- rience. we describe these survey questions in the relevant context of our results, and we provide a complete listing of all survey instruments in appendix a. (2) event logs. the programmer’s assistant was instrumented to collect data on participants’ usage. the event logs pro- vided timestamped records of interaction events, including conversational exchanges, hiding/showing the assistant, use of the “try again” and “start over” features, and use of copy/- paste. (3) conversation logs. from the event logs, we extracted con- versational transcripts between each participant and the programmer’s assistant. 5 results 5.1 data & analysis we collected a wealth of data in our study: 126 survey responses from three surveys per participant, containing 296 written com- ments in open-ended survey questions, and 4,877 instances of 23 different types of ui events, including 1,699 conversational ex- changes13 in the event logs. we also compute, for each participant, counts or durations for 21 different metrics from the event logs. in our analysis, we deliberately exclude the portion of our data collected during the tutorial exercise. we exclude this data because that activity was guided by the tutorial instructions, not by our participants’ own initiative. thus, our final sample consists of 3,172 12we did not enforce that participants actually complete all of the challenges. never- theless, all participants but one did submit solutions to all of the challenges. 13we refer to a participant’s utterance, followed by the assistant’s response, as a conversational exchange. events, including 968 conversational exchanges in the event logs; no survey data was excluded. our primary analysis of this data is qualitative, as our partici- pants provided us with a rich source of interesting feedback and thought-provoking insights in their comments. where applicable, we supplement this data with quantitative data from the survey and the event logs, as well as chat transcript data from the con- versation logs. in this way, we triangulate [47] across our three data sources, using the open-ended survey data as a foundation. when we quote participants, either from their qualitative survey responses or the conversational transcripts, we reproduce their words exactly as typed, including typos, misspellings, grammati- cal errors, capitalization, and potential trigger words, and we only make minor clarifying edits where needed, delineated by square brackets. in order to set the context for our analysis, we first describe how we used reflexive thematic analysis to analyze participants’ responses to the open-ended survey questions. we then describe our analysis of the conversation logs and our development of a coding guide based on conversation analysis [76], and specifically, moore and arar’s natural conversation framework [50]. 5.1.1 thematic analysis of qualitative survey responses. we con- ducted a reflexive thematic analysis to analyze the responses to our seven open-ended survey questions. we followed the process described by braun and clarke [16] in which researchers immerse themselves in the data, generate codes for material that seems interesting, and then iteratively group and refine codes through collaborative discussion in order to identify higher-level themes. initially, four authors performed open-coding on the open-ended survey responses. through discussion, these codes were grouped and consolidated into a single set, which were then re-applied to the data by two authors. after another round of discussion, these authors identified a set of 12 higher-level themes. some themes had clear parallels to quantitative survey questions or event log data, and thus represented clear instances where we were able to trian- gulate across data sources. other themes surprised us. we structure our presentation of the results based on these 12 themes, grouped into three different aspects of the user experience: expectations and experience, utility of conversational assistance, and patterns of interaction and mental models. 5.1.2 conversation analysis via the natural conversation frame- work. in order to understand the content and structure of the conversations that took place between our participants and the programmer’s assistant, we turned to the natural conversation framework [50] (ncf). we developed a codebook for the event logs, beginning with 21 different categories of utterances from the ncf. nine ncf categories – acknowledgment, apology, confirma- tion, expression of gratitude, farewell, greeting, self-identification, welfare check, and welfare report – appeared twice in our code- book to distinguish cases in which the utterance was made by the human participant vs. the assistant. other ncf categories were split to provide nuanced detail about the interaction; for example, we distinguished three different kinds of ncf requests, depending upon whether they were stated as requests for action (e.g. “would you...”), commands of action (e.g. “write a function that...”), or expressions of desire (e.g. “i want...”). we also added 18 additionalinterlocutor orientation codes human social acknowledgment, apology, criticism, expression of gratitude, farewell, greeting, politeness, praise, self identification, small talk, welfare check, welfare report task asks question, asserts information, capability check, command of action, expression of desire, identi- fies error, request for action, requests elaboration, requests explanation meta / ui chat context required, confirmation, copy, copy (extraneous), erroneous input, includes selection, includes extraneous selection, missing selection, paste, paste (extraneous), pasted code in chat, spelling error, start over, try again assistant appears fixated, claims ignorance, didn’t understand, grants request (complete), grants request (incomplete), offers help, provided wrong answer, requests details, requests paraphrase, response includes code, spews garbage table 1: event log codebook. our codebook contained 46 unique codes, applied separately to participant utterances (human) and assistant responses (assistant). codes in bold were applied to both participant and assistant responses. human codes were classified as demonstrating either a social or task orientation to the assistant. codes to identify meta-information such as utterances that included code, utterances that referenced selected code, utterances that im- plicitly or explicitly referenced earlier portions of the conversation, or non-verbal ui activities such as copies, pastes, and invocations of “try again” and “start over.” finally, we classified a subset of the human-applied codes based on whether they represented a par- ticipant’s task or social orientation toward the assistant. we list our codes in table 1, but note that not all of them ended up being relevant to our analysis. when coding conversational data, we applied individual codes at the level of each conversational utterance. we allowed multiple codes to be applied to each utterance to account for utterances that performed multiple functions (e.g. greeting and self-identification). in order to ensure consistency in how our codebook was applied, two authors coded a 10% sample of the 968 conversational ex- changes, achieving a satisfactory level of inter-rater reliability (krip- pendorf’s 𝛼 = 0.77, where agreement was conservatively defined as having all of the same codes applied to both utterances in a conversational exchange). 5.2 expectations and experience pilot testing of the programmer’s assistant suggested that software engineers would be skeptical of a conversational programming assistant and its ability to provide useful assistance. our study revealed that, for most participants, their actual experience after using the tool was better than they had anticipated. participants were surprised at the quality of the assistant’s responses and they appreciated how its integration with the code editor reduced the amount of context switching they needed to do in the ui. some participants struggled with the code selection feature, although others appreciated the ability to ask questions related to selected code. 5.2.1 usage. all of our participants engaged with the program- mer’s assistant while working on the challenges, despite there being no requirement to do so. forty-one participants submitted solutions to all four challenges, and one participant, p14, only sub- mitted solutions for one of the four challenges. participants spent an average of 68 minutes engaged with the assistant, as measured by the amount of time the programmer’s assistant window was in focus. participants made an average of 23.0 utterances (sd = 15.1 ut- terances) to the assistant. on average, 6.2 of their utterances (sd = 4.3 utterances) contained a code selection. the average latency per request14 was 6.7 seconds (sd = 3.1 seconds). we saw a 66.3% rate of acceptance of generated code, where we considered code to be accepted if the participant performed a copy immediately after the code was generated. this acceptance rate is much higher than the 27% acceptance rate reported for copilot [109]. we believe one reason we observed a higher acceptance rate is because copilot’s completion suggestions are generated proactively, whereas the programmer’s assistant’s suggestions are generated upon request. when copying generated code from the assistant, participants most often copied the entirety of the generated code, and only in 5.8% of cases did they copy a smaller portion of it. 5.2.2 user experience expectations & changed attitudes. prior to running our study, we had reason to believe that participants would be skeptical of a conversational programming assistant. before de- veloping the programmer’s assistant, we showed potential users mockups of a program editor with an integrated chatbot feature. these prototypes elicited uniformly negative reactions. people told us about their frustrating experiences with conventional chatbots and raised doubts about the knowledge, capabilities, and value of a conversational programming assistant. this skepticism moti- vated us to develop the programmer’s assistant in order to evaluate whether the conversational experience, as powered by a state-of- the-art code-fluent llm, would be better than people had antici- pated. during pilot testing, we received feedback that the program- mer’s assistant provided a much better conversational experience compared to testers’ previous experiences with chatbots. thus, in designing our study, we felt it important to first gauge participants’ expectations of a conversational interaction around code, and then measure their experience after the fact. 14this time includes additional time added by our proxy server to ensure our confor- mance to the api rate limitation.we developed a short inventory of six scale items to measure user experience of code work15. the scale was administered twice: once before participants were exposed to the programmer’s assistant (but after they had been briefed that they would interact with an ai chatbot), and once after completing the programming challenges. the items were presented with the appropriate tense: do you expect (did you find that) the programmer’s assistant: (a) will be (was) easy to use; (b) will understand (understood) your requests; (c) will provide (provided) high quality responses; (d) will help (helped) you to write better code; (e) will help (helped) you to write code more quickly; (f) will be (was) enjoyable to use. each item was rated on a 4-point scale of extent: not at all (1), a little (2), somewhat (3), a great deal (4). a factor analysis revealed the items on this scale measured a single construct, which we identify as user experience (cronbach’s 𝛼 = 0.87). thus, we computed two scores of user experience (ux) for each participant: a pre-task ux score computed as the average of their six pre-task expectation scale responses, and a post-task ux score computed as the average of their six post-task experience scale responses. we found that participants had lower initial expectations for their experience with a conversational programming assistant (pre- task ux m (sd) = 3.0 (0.62) of 4) than their experience actually was (post-task ux m (sd) = 3.6 (0.32) of 4). a paired sample t-test shows that this difference was significant, 𝑡(41) = 5.94, 𝑝 < .001, cohen’s 𝑑 = 0.92 (large). measured another way, 32 participants (76.2%) had post-task ux ratings that were higher than their pre- task expectations, demonstrating a significant shift in attitudes toward conversational programming assistance. however, the ux ratings alone fail to capture participants’ nu- anced expectations of the assistant and the reasons for their shifted attitudes after using it. participants expressed a variety of expecta- tions of the assistant before using it, including that it would be easy to use (p30) and produce correct responses (p30), understand the problem and what is being asked of it (p8, p9, p11), not interfere with their flow state (p5), produce imperfect or questionable out- puts (p6, p21), improve with feedback (p31), provide generic and unhelpful answers (p17) or only answer basic questions (p40), and produce responses quickly (p40). p17 expected “to be frustrated very quickly and that what i’d think would be relatively common questions would be responded to with generic, unhelpful answers.” p6 explained, “i didn’t have very good experiences with chatbots. i think i’ll need to spend more time in reviewing and fixing the suggestions than in writing the code myself from scratch.” p11 had a more balanced view, that “it’ll do some tasks really well, but others will not be as reliable.” after interacting with the programmer’s assistant, many par- ticipants commented on how the experience was better than they anticipated, because it “seemed to be able to handle complex issues” (p10) and “was a great help” (p8). p20 felt it was “incredible!” p6 and p17, who were both initially skeptical, reported having a positive experience. for p6, “it absolutely exceeded all my expectations, in 15our scale items were modeled from scales published in weisz et al. [103, table 9 – ai support] that measured constructs including ease of use (item 3), response quality (item 1), the production of higher-quality code (item 5), and the ability to write code more rapidly (item 4). we added additional items to cover the constructs of request understanding and enjoyment, and we cast all items on a 4-point scale of extent. all aspects that i could have imagined and more!” p17 provided a more quantitative assessment: “initial expectations: 3 actual: 9.5.” p38 was emphatic in their evaluation: “i was blown away how well it allowing me to structure how i want the code to look and work and just giving me the thing i asked for.” many participants described a sense of surprise in their experi- ences. p9 was surprised by how well it understood their requests: “i was surprised at how well the programmer assistant was able to understand my requests and generate good code/documentation/tests. it understood major concepts and was able to explain it to me in a clear way, and it was also able to understand and write functional code. it even was able to help me review my answer. i was also surprised at how well it could understand the context of what i was asking in follow-up questions when i did not specify exactly what i was talking about, but rather referencing our prior conversation (such as, ‘what does that mean’).” (p9) similarly, p6 was surprised that they liked the conversational interaction when they expected that they wouldn’t: “i though[t] i wouldn’t like the chatbot interaction and that i would prefer something like the tool i’ve seen in those demos [of copilot]. but surprisingly, after using the chatbot (and seeing the results: easy to use, it un- derstands well, i felt it like a partner) i like this kind of help.” (p6) 5.2.3 quality of assistant’s responses. in order to gauge the quality of responses produced by the programmer’s assistant, we examined the 910 task-oriented requests made by participants in the study. for the vast majority (80.2%), the assistant produced a correct re- sponse (grants request (complete)); in other cases, the assistant’s response was incorrect (9.6%; provided wrong answer), correct but incomplete (4.4%; grants request (incomplete)), or the assistant didn’t understand (3.4%; didn’t understand), claimed ignorance of the subject (1.5%; claims ignorance), or produced another type of response (0.9%; appears fixated, spews garbage). participants also reported experiencing this variability in the quality of the assistant’s responses. some participants described how the assistant provided “detailed answers” (p17) and “high qual- ity outputs” (p18) that were “surprisingly good” (p2). p6 felt it was “incredible to see the quality of the responses,” and p3 even explored the assistant’s capabilities outside the scope of the challenges and found that it could handle those as well: “it was surprising the quality of the code and the ability to answer all my questions correctly. although i think the challenges may be biased towards what the assistant is able to do, it was a great experience because i asked many other things and it was able to answer correctly.” (p3) of course, the programmer’s assistant wasn’t perfect, and some participants did run into issues. for p35, “the documentation gener- ation did not perform very well.” p16 questioned the accuracy of the knowledge encoded in the model: “does the model need to be up- dated? it said latest python version is 3.7 but google says it’s 3.10.” in some instances, participants needed to ask their question multipletimes to get a good response: “you need to ask many times if you want to get an answer and also a detailed explanation” (p3). p27 felt, “it was annoying when i asked it to try again and it would give me the same response.” p22 struggled because, “it didn’t seem to handle multiple sentences well.” p28 perhaps offered the most scathing criticism, that, “it makes mistakes often enough to be not very practical.” however, despite the production of poorer-quality responses, other participants felt that the assistant was still helpful. p36 reported that, “only minor tweaks were normally needed to correct any issues.” similarly, p38 described how the assistant wasn’t able to completely solve their problem, but provided a useful start: “there was only one hickup i noticed where when i asked it to memoize fibonacci it couldn’t, but it dropped the building blocks on my lap for me to finish so that was fine, that was like minutes of effort on my part.” (p38) 5.2.4 ui design & affordances. participants made many comments on our specific ui design and the affordances provided (or not provided) in our chat-augmented editor. overall, the integration between the chat pane and the code editor was “very good” (p23), with a “nice interface between the code pane and the assistant pane” (p17) that “makes it really convenient” (p35). prior research by brandt et al. [15] has shown how keeping developers focused in their ide improves productivity, and our participants expressed similar sentiments. p40 remarked, “it allows me to stay in one browser window/tab!” and p12 hinted at how the interface might preserve their flow state by “prevent[ing] me from getting distracted when looking into an issue in another tab.” some aspects of our user interface were confusing to participants, such as the mechanism for selecting code to be included in the conversational context. p7 remarked, “it’s was a little confusing doing the selection part for it to tell me what a function does, but... it gave me code that was insanely easy to copy and paste.” other participants appreciated the code selection mechanism, such as p11: “i enjoyed the code selection feature, and found that very easy to use.” in the event logs, we identified 20 instances in which a participant unintentionally included selected code in the conversation when it wasn’t needed (includes extraneous selection), 12 instances in which a code selection was omitted when it was needed to provide context for the question (missing selection), and 16 instances in which a participant copy/pasted code directly into the chat rather than selecting it in the editor (pasted code in chat). although these cases represent a small fraction of the 227 instances in which a code selection was required and included in the conversation (includes selection), their presence does indicate that more attention is needed to the interaction design of code selection. another issue regarded the awareness of the “try again” and “start over” features. the “try again” feature was only used by 14 participants, who used it a total of 63 times over the course of the study. some participants used it specifically when they got an answer which they saw as clearly wrong, while others used it to get a variety of possible answers before proceeding. the “start over” feature was used even less, by 5 participants who used it a total of 6 times. despite our effort to surface these conversational features in the ui via shortcut buttons, they may not have been sufficiently noticeable or salient: “the ‘try again’ button is not so reachable, often times i forgot it exists” (p23). by contrast, at least one participant was successful with these features: “at some point it had issue with challenge 3 and i had to start over. just asking ‘try again’ was not enough and i was getting always the same (wrong and not related) answer. starting again solved the issue!” (p20) 5.3 utility of conversational assistance our next set of themes concerns the utility provided by conversa- tional programming assistance. participants felt the assistant was highly valuable and desired to use it in their own work. they felt it would be most helpful for smaller or narrowly-scoped tasks, but able to provide a wide variety of types of assistance. the fact that the interaction model was conversational and grounded in code were valuable aspects, as was the ability for the assistant to bolster users’ learning about programming topics through that interac- tion. participants did question whether they could trust and rely upon the assistant’s responses, echoing a similar theme discussed in weisz et al. [102]. 5.3.1 value & appropriate tasks. participants rated the value of the programmer’s assistant highly (m (sd) = 8.6 (1.4) of 10). many participants asked questions such as, “can i have it in my editor please?” (p15), or made comments that, “i would enjoy using it in the future” (p36), “i would love to be able to... have access to it for my coding” (p37), and “i’d love to use this tool as part of my usual programming workflow if i could!” (p39). some of the reasons why participants found it valuable are because it “help[s] me remember how to do things in certain languages that normally i would just google” (p9) and “it helps me to avoid silly syntax errors and can when i cannot remember exact function/method names and required arguments” (p40). we did not observe any differences in value ratings based on participants’ familiarity with or recency of using python. participants described a wide variety of tasks for which they felt the assistant would be useful. these tasks included “ordinary” (p23), “simpler” (p2), and “small, repetitive” (p4) tasks such as “quick lookups” (p25) for “short chunks of code” (p11) or for “narrowed questions” (p26). participants also felt the assistant was useful for “small containable novel algorithms” (p38) and “little coding problems” (p4). several kinds of task assistance were reported as being valuable, such as explaining code (p31), implementing business logic in a ui (p38), understanding what code does (p19, p37), and recalling language syntax, method names, and arguments (p12, p15, p20, p40, p42). p27 felt that the assistant was “more helpful when recognizing a specific well known algorithm but not things you make yourself.” participants also made recommendations for how to increase the value of the programmer’s assistant. p38 suggested, “what would blow me away though is if it’s able to help with what i do most often which is to integrate, refactor and iterate on an existing system.” p16, p26, and p38 all desired more information on the data sources used to produce the assistant’s responses. p9 requested to “have the programmer’s assistant examine your code and make proactive suggestions for improving it in the chat.” p36 requested the same,but cautioned that, “care would need to be taken to avoid becoming an annoyance or disrupting the flow of a coding session.” in the post-task survey, we probed participants on how certain changes to the programmer’s assistant would either decrease, in- crease, or result in no change to its value. over 75% of participants felt that the assistant would be more valuable if it operated in a proactive manner, either by making improvement suggestions in the chat or as comments directly in the code. similarly, 78.6% of participants felt that having more buttons in the ui for common features such as explaining or documenting code would make the tool more valuable. 5.3.2 conversational interactions grounded in code. one of the challenges in interpreting participants’ comments about the utility of the programmer’s assistant was in disentangling the extent to which value was derived from the quality of the underlying model versus the integration of conversation in a code context. indeed, participants felt that the chat interaction was valuable: 69.0% of participants felt that eliminating the conversational interaction and making the assistant behave more like web search would de- crease its value. further, our analysis of the conversation transcripts revealed that 42% of the 910 task-oriented utterances from partic- ipants required historical conversational context (chat context required) in order to be correctly interpreted. thus, we observe that behaviorally, participants did rely on conversational context in their interactions. in the post-task survey, 83% of participants rated the importance of the ability to ask follow-up questions as being “somewhat” or “a great deal.” several participants specifically commented on the value of this conversational context. p39 remarked, “i absolutely loved how you can straight up ask follow-up questions to the programmers’ assistant without having to reiterate the original topic/question.” p15 expressed a similar sentiment, saying, “i think the conversational context was someone helpful, just in communicating that it’s a running conversation where my context is remembered.” p9 provided a similar analysis: “this tool was so helpful at answering questions i had about the code in the context of the code i am working on... i was also impressed with how well it was able to remember the context of our conversation, especially when i asked vague follow-up questions.” (p9) in addition, some participants identified how a conversational interaction grounded in code was useful, “because i think to ‘un- derstand’ the dev context could be very important” (p31). in fact, 24.9% of task-oriented utterances included a relevant code selection (includes selection), showing that participants valued this ability. contrasting with these participants, p18 felt that interacting with the assistant conversationally was tedious, and they employed a more direct approach: “i really like the pa. but, i didn’t converse with it like a chat bot. i often told it what to do (‘document this code.’) as opposed to asking it what to do (‘how do i document this code?’). talking to it the way that was suggested in the tutorial seemed overly verbose/tedious.” (p18)  despite these individual differences in interaction p",
                    "references": "[1] rabe abdalkareem, emad shihab, and juergen rilling. 2017. what do developers use the crowd for? a study using stack overflow. ieee software 34, 2 (2017), 53–60. https://doi.org/10.1109/ms.2017.31 [2] eleni adamopoulou and lefteris moussiades. 2020. chatbots: history, technol- ogy, and applications. machine learning with applications 2 (2020), 100006. [3] daniel adiwardana, minh-thang luong, david r. so, jamie hall, noah fiedel, romal thoppilan, zi yang, apoorv kulshreshtha, gaurav nemade, yifeng lu, and quoc v. le. 2020. towards a human-like open-domain chatbot. [4] safinah ali, nisha elizabeth devasia, and cynthia breazeal. 2022. escape! bot: social robots as creative problem-solving partners. in creativity and cognition. 275–283. [5] miltiadis allamanis, earl t barr, premkumar devanbu, and charles sutton. 2018. a survey of machine learning for big code and naturalness. acm computing surveys (csur) 51, 4 (2018), 1–37. [6] irene alvarado, idan gazit, and amelia wattenberger. 2022. github next | github copilot labs. https://githubnext.com/projects/copilot-labs/ [7] hikari ando, rosanna cousins, and carolyn young. 2014. achieving saturation in thematic analysis: development and refinement of a codebook. comprehensive psychology 3 (2014), 03–cp. [8] craig anslow, stuart marshall, james noble, and robert biddle. 2013. sourcevis: collaborative software visualization for co-located environments. in 2013 first ieee working conference on software visualization (vissoft). ieee, 1–10. [9] zahra ashktorab, michael desmond, josh andres, michael muller, naren- dra nath joshi, michelle brachman, aabhas sharma, kristina brimijoin, qian pan, christine t wolf, et al. 2021. ai-assisted human labeling: batching for efficiency without overreliance. proceedings of the acm on human-computer interaction 5, cscw1 (2021), 1–27. [10] catherine a ashworth. 1996. gui users have trouble using graphic conventions on novel tasks. in conference companion on human factors in computing systems. 75–76. [11] amanda askell, yuntao bai, anna chen, dawn drain, deep ganguli, tom henighan, andy jones, nicholas joseph, ben mann, nova dassarma, et al. 2021. a general language assistant as a laboratory for alignment. arxiv preprint arxiv:2112.00861 (2021). [12] leif azzopardi, paul thomas, and nick craswell. 2018. measuring the utility of search engine result pages: an information foraging based measure. in the 41st international acm sigir conference on research & development in information retrieval. 605–614. [13] shraddha barke, michael b james, and nadia polikarpova. 2022. grounded copilot: how programmers interact with code-generating models. arxiv preprint arxiv:2206.15000 (2022). [14] rishi bommasani, drew a hudson, ehsan adeli, russ altman, simran arora, sydney von arx, michael s bernstein, jeannette bohg, antoine bosselut, emma brunskill, et al. 2021. on the opportunities and risks of foundation models. arxiv preprint arxiv:2108.07258 (2021). [15] joel brandt, mira dontcheva, marcos weskamp, and scott r klemmer. 2010. example-centric programming: integrating web search into the development environment. in proceedings of the sigchi conference on human factors in computing systems. 513–522. [16] virginia braun and victoria clarke. 2022. common challenges in thematic analysis and how to avoid them. retrieved august 11 2022 from https://youtu. be/tpwlsckpm78 [17] tom brown, benjamin mann, nick ryder, melanie subbiah, jared d ka- plan, prafulla dhariwal, arvind neelakantan, pranav shyam, girish sastry, amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewon child, aditya ramesh, daniel ziegler, jeffrey wu, clemens winter, chris hesse, mark chen, eric sigler, mateusz litwin, scott gray, benjamin chess, jack clark, christopher berner, sam mccandlish, alec radford, ilya sutskever, and dario amodei. 2020. language models are few-shot learners. in advances in neural information processing systems, h. larochelle, m. ranzato, r. hadsell, m.f. balcan, and h. lin (eds.), vol. 33. curran associates, inc., 1877–1901. https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-paper.pdf [18] sallyann bryant, pablo romero, and benedict\" du boulay. 2006. the collabora- tive nature of pair programming. in extreme programming and agile processes in software engineering, pekka abrahamsson, michele marchesi, and giancarlo succi (eds.). springer berlin heidelberg, berlin, heidelberg, 53–64. [19] andres campero, michelle vaccaro, jaeyoon song, haoran wen, abdullah almaatouq, and thomas w malone. 2022. a test for evaluating performance in human-computer systems. arxiv preprint arxiv:2206.12390 (2022). [20] gaetano cascini, yukari nagai, georgi v georgiev, jader zelaya, niccolò be- cattini, jean-françois boujut, hernan casakin, nathan crilly, elies dekoninck, john gero, et al. 2022. perspectives on design creativity and innovation research: 10 years later. , 30 pages. [21] stephen cass. 2022. top programming languages 2022. ieee spectrum (23 aug 2022). https://spectrum.ieee.org/top-programming-languages-2022 [22] cristina catalan aguirre, nuria gonzalez castro, carlos delgado kloos, carlos alario-hoyos, and pedro josé muñoz merino. 2021. conversational agent for supporting learners on a mooc on programming with java. (2021). [23] ana paula chaves and marco aurelio gerosa. 2021. how should my chatbot interact? a survey on social characteristics in human–chatbot interaction design. international journal of human–computer interaction 37, 8 (2021), 729–758. [24] mark chen, jerry tworek, heewoo jun, qiming yuan, henrique ponde, jared kaplan, harrison edwards, yura burda, nicholas joseph, greg brockman, alex ray, raul puri, gretchen krueger, michael petrov, heidy khlaaf, girish sas- try, pamela mishkin, brooke chan, scott gray, nick ryder, mikhail pavlov, alethea power, lukasz kaiser, mohammad bavarian, clemens winter, philippe tillet, felipe petroski such, david w. cummings, matthias plappert, fotios chantzis, elizabeth barnes, ariel herbert-voss, william h. guss, alex nichol, igor babuschkin, s. arun balaji, shantanu jain, andrew carr, jan leike, joshua achiam, vedant misra, evan morikawa, alec radford, matthew m. knight, miles brundage, mira murati, katie mayer, peter welinder, bob mcgrew, dario amodei, sam mccandlish, ilya sutskever, and wojciech zaremba. 2021. evalu- ating a large language models trained on code. [25] li-te cheng, r.b. de souza, susanne hupfer, john patter- son, and steven ross. 2003. building collaboration into ides: edit>compile>run>debug>collaborate? queue 1, 9 (2003).[26] carl cook, warwick irwin, and neville churcher. 2005. a user evaluation of synchronous collaborative software engineering tools. in 12th asia-pacific software engineering conference (apsec’05). ieee, 6–pp. [27] claudio león de la barra, broderick crawford, ricardo soto, sanjay misra, and eric monfroy. 2013. agile software development: it is about knowledge management and creativity. in computational science and its applications – iccsa 2013, beniamino murgante, sanjay misra, maurizio carlini, carmelo m. torre, hong-quang nguyen, david taniar, bernady o. apduhan, and osvaldo gervasi (eds.). springer berlin heidelberg, berlin, heidelberg, 98–113. [28] uri dekel and steven ross. 2004. eclipse as a platform for research on in- terruption management in software development. in proceedings of the 2004 oopsla workshop on eclipse technology exchange (vancouver, british columbia, canada), michael g. burke (ed.). acm, 12–16. [29] bobbie eicher, kathryn cunningham, sydni peterson marissa gonzales, and ashok goel. 2017. toward mutual theory of mind as a foundation for co-creation. in international conference on computational creativity, co-creation workshop. [30] stephen m fiore, eduardo salas, and janis a cannon-bowers. 2001. group dynamics and shared mental model development. how people evaluate others in organizations 234 (2001). [31] erich gamma, richard helm, ralph johnson, and john vlissides. 1995. design patterns: elements of reusable object-oriented software. addison-wesley. [32] github, inc. 2022. github copilot · your ai pair programmer. retrieved august 5, 2022 from https://github.com/features/copilot/ [33] amelia glaese, nat mcaleese, maja trębacz, john aslanides, vlad firoiu, timo ewalds, maribeth rauh, laura weidinger, martin chadwick, phoebe thacker, lucy campbell-gillingham, jonathan uesato, po-sen huang, ramona comanescu, fan yang, abigail see, sumanth dathathri, rory greig, charlie chen, doug fritz, jaume sanchez elias, richard green, soňa mokrá, nicholas fernando, boxi wu, rachel foley, susannah young, iason gabriel, william isaac, john mellor, demis hassabis, koray kavukcuoglu, lisa anne hendricks, and geoffrey irving. 2022. improving alignment of dialogue agents via targeted human judgements. https://arxiv.org/abs/2209.14375 [34] stephanie glen. 2022. chatgpt writes code, but won’t replace devel- opers. techtarget (14 12 2022). retrieved 20-jan-2023 from https: //www.techtarget.com/searchsoftwarequality/news/252528379/chatgpt- writes-code-but-wont-replace-developers [35] samuel holmes, anne moorhead, raymond bond, huiru zheng, vivien coates, and mike mctear. 2018. weightmentor: a new automated chatbot for weight loss maintenance. in proceedings of the 32nd international bcs human computer interaction conference 32. 1–5. [36] xing hu, ge li, xin xia, david lo, and zhi jin. 2020. deep code comment generation with hybrid lexical and syntactical information. empirical software engineering 25, 3 (2020), 2179–2217. [37] edwin l hutchins, james d hollan, and donald a norman. 1985. direct manip- ulation interfaces. human–computer interaction 1, 4 (1985), 311–338. [38] srinivasan iyer, ioannis konstas, alvin cheung, and luke zettlemoyer. 2016. summarizing source code using a neural attention model. in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers). 2073–2083. [39] andreas jedlitschka and markus nick. 2003. software engineering knowledge repositories. springer berlin heidelberg, berlin, heidelberg, 55–80. [40] eirini kalliamvakou. 2022. research: quantifying github copilot’s impact on developer productivity and happiness. https://github.blog/2022-09-07- research-quantifying-github-copilots-impact-on-developer-productivity- and-happiness/ [41] anna kantosalo et al. 2019. human-computer co-creativity: designing, evalu- ating and modelling computational collaborators for poetry writing. (2019). [42] sandeep kaur kuttal, bali ong, kate kwasny, and peter robe. 2021. trade- offs for substituting a human with an agent in a pair programming context: the good, the bad, and the ugly. in proceedings of the 2021 chi conference on human factors in computing systems (yokohama, japan) (chi ’21). association for computing machinery, new york, ny, usa, article 243, 20 pages. [43] lauramaria laine. 2021. exploring advertising creatives’ attitudes towards human-ai collaboration. (2021). [44] yujia li, david choi, junyoung chung, nate kushman, julian schrittwieser, rémi leblond, tom eccles, james keeling, felix gimeno, agustin dal lago, and et al. 2022. competition-level code generation with alphacode. https: //arxiv.org/abs/2203.07814 [45] yaosheng lou and qi sun. 2021. over-reliance on database: a case study of using web of science. human behavior and emerging technologies 3, 3 (2021), 454–459. [46] david lyell and enrico coiera. 2017. automation bias and verification complex- ity: a systematic review. journal of the american medical informatics association 24, 2 (2017), 423–431. [47] wendy e mackay and anne-laure fayard. 1997. hci, natural science and design: a framework for triangulation across disciplines. in proceedings of the 2nd conference on designing interactive systems: processes, practices, methods, and techniques. 223–234. [48] john e mathieu, tonia s heffner, gerald f goodwin, eduardo salas, and janis a cannon-bowers. 2000. the influence of shared mental models on team process and performance. journal of applied psychology 85, 2 (2000), 273. [49] cade metz. 2022. meet gpt-3. it has learned to code (and blog and ar- gue). (published 2020). https://www.nytimes.com/2020/11/24/science/artificial- intelligence-ai-gpt3.html [50] robert j. moore and raphael arar. 2019. conversational ux design: a practi- tioner’s guide to the natural conversation framework. association for computing machinery, new york, ny, usa. [51] ekaterina a moroz, vladimir o grizkevich, and igor m novozhilov. 2022. the potential of artificial intelligence as a method of software developer’s produc- tivity improvement. in 2022 conference of russian young researchers in electrical and electronic engineering (elconrus). ieee, 386–390. [52] michael muller, stevean ross, stephanie houde, mayank agarwal, fernando martinez, john richards, kartik talamadupula, and justin d weisz. 2022. drink- ing chai with your (ai) programming partner: a design fiction about gener- ative ai for software engineering. hai-gen workshop at iui 2022: 3rd work- shop on human-ai co-creation with generative models (2022). https://hai- gen.github.io/2022/ [53] sandra r murillo and j alfredo sánchez. 2014. empowering interfaces for system administrators: keeping the command line in mind when designing guis. in proceedings of the xv international conference on human computer interaction. 1–4. [54] elizabeth d mynatt and gerhard weber. 1994. nonvisual presentation of graph- ical user interfaces: contrasting two approaches. in proceedings of the sigchi conference on human factors in computing systems. 166–172. [55] alok mysore and philip j guo. 2017. torta: generating mixed-media gui and command-line app tutorials using operating-system-wide activity tracing. in proceedings of the 30th annual acm symposium on user interface software and technology. 703–714. [56] c. nass and y. moon. 2000. machines and mindlessness: social responses to computers. journal of social issues 56, 1 (2000), 81–103. [57] nhan nguyen and sarah nadi. 2022. an empirical evaluation of github copi- lot’s code suggestions. in 2022 ieee/acm 19th international conference on mining software repositories (msr). ieee, 1–5. [58] martin nordio, h estler, carlo a furia, bertrand meyer, et al. 2011. collaborative software development on the web. arxiv preprint arxiv:1105.0768 (2011). [59] maxwell nye, anders andreassen, guy gur-ari, henryk witold michalewski, ja- cob austin, david bieber, david martin dohan, aitor lewkowycz, maarten paul bosma, david luan, charles sutton, and augustus odena. 2021. show your work: scratchpads for intermediate computation with language models. https://arxiv.org/abs/2112.00114. [60] openai. 2022. chatgpt: optimizing language models for dialogue. openai blog (30 11 2022). retrieved 20-jan-2023 from https://openai.com/blog/chatgpt/ [61] long ouyang, jeff wu, xu jiang, diogo almeida, carroll l. wainwright, pamela mishkin, chong zhang, sandhini agarwal, katarina slama, alex ray, john schulman, jacob hilton, fraser kelton, luke miller, maddie simens, amanda askell, peter welinder, paul christiano, jan leike, and ryan lowe. 2022. training language models to follow instructions with human feedback. https://arxiv. org/abs/2203.02155 [62] peter pirolli and stuart card. 1999. information foraging. psychological review 106, 4 (1999), 643. [63] larry press. 1990. personal computing: windows, dos and the mac. commun. acm 33, 11 (1990), 19–26. [64] alec radford, jeffrey wu, rewon child, david luan, dario amodei, ilya sutskever, et al. 2019. language models are unsupervised multitask learners. [65] alvin rajkomar, jeffrey dean, and isaac kohane. 2019. machine learning in medicine. new england journal of medicine 380, 14 (2019), 1347–1358. [66] aditya ramesh, prafulla dhariwal, alex nichol, casey chu, and mark chen. 2022. hierarchical text-conditional image generation with clip latents. arxiv preprint arxiv:2204.06125 (2022). [67] b. reeves and c.i. nass. 1996. the media equation: how people treat computers, television, and new media like real people and places. csli publications. [68] mawarny md rejab, james noble, and george allan. 2014. distributing expertise in agile software development projects. in 2014 agile conference. 33–36. [69] jeba rezwana and mary lou maher. 2021. cofi: a framework for modeling interaction in human-ai co-creative systems.. in iccc. 444–448. [70] charles h. rich and richard c. waters. 1990. the programmer’s apprentice. addison-wesley publishing company, reading, ma. [71] peter robe and sandeep kaur kuttal. 2022. designing pairbuddy—a conver- sational agent for pair programming. acm transactions on computer-human interaction (tochi) 29, 4 (2022), 1–44. [72] robin rombach, andreas blattmann, dominik lorenz, patrick esser, and björn ommer. 2022. high-resolution image synthesis with latent diffusion models. in proceedings of the ieee/cvf conference on computer vision and pattern recogni- tion. 10684–10695.[73] steven ross, elizabeth brownholtz, and robert armes. 2004. a multiple- application conversational agent. in proceedings of the 9th international con- ference on intelligent user interfaces (funchal, madeira, portugal) (iui ’04). asso- ciation for computing machinery, new york, ny, usa, 319–321. [74] steven ross, elizabeth brownholtz, and robert armes. 2004. voice user interface principles for a conversational agent. in proceedings of the 9th international conference on intelligent user interfaces (funchal, madeira, portugal) (iui ’04). association for computing machinery, new york, ny, usa, 364–365. [75] baptiste roziere, marie-anne lachaux, lowik chanussot, and guillaume lample. 2020. unsupervised translation of programming languages. in advances in neural information processing systems, h. larochelle, m. ranzato, r. hadsell, m.f. balcan, and h. lin (eds.), vol. 33. curran associates, inc., 20601–20611. [76] harvey sacks. 1984. notes on methodology. in structures of social action: studies in conversation analysis, john heritage and j. maxwell atkinson (eds.). cambridge university press, cambridge, 2–27. [77] nithya sambasivan and rajesh veeraraghavan. 2022. the deskilling of domain expertise in ai development. in chi conference on human factors in computing systems. 1–14. [78] harini sampath, alice merrick, and andrew macvean. 2021. accessibility of command line interfaces. in proceedings of the 2021 chi conference on human factors in computing systems. 1–10. [79] matthias scheutz, scott a deloach, and julie a adams. 2017. a framework for developing and using shared mental models in human-agent teams. journal of cognitive engineering and decision making 11, 3 (2017), 203–224. [80] isabella seeber, eva bittner, robert o briggs, triparna de vreede, gert-jan de vreede, aaron elkins, ronald maier, alexander b merz, sarah oeste-reiß, nils randrup, et al. 2020. machines as teammates: a research agenda on ai in team collaboration. information & management 57, 2 (2020), 103174. [81] shilad sen, werner geyer, michael muller, marty moore, beth brownholtz, eric wilcox, and david r millen. 2006. feedme: a collaborative alert filtering system. in proceedings of the 2006 20th anniversary conference on computer supported cooperative work. 89–98. [82] ben shneiderman. 2020. human-centered artificial intelligence: three fresh ideas. ais transactions on human-computer interaction 12, 3 (2020), 109–124. [83] ben shneiderman. 2022. human-centered ai. oxford university press. [84] kurt shuster, jing xu, mojtaba komeili, da ju, eric michael smith, stephen roller, megan ung, moya chen, kushal arora, joshua lane, et al. 2022. blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arxiv preprint arxiv:2208.03188 (2022). [85] michael skirpan and casey fiesler. 2018. ad empathy: a design fiction. in proceedings of the 2018 acm conference on supporting groupwork. 267–273. [86] diomidis spinellis. 2012. git. ieee software 29, 3 (2012), 100–101. https: //doi.org/10.1109/ms.2012.61 [87] angie spoto and natalia oleynik. 2017. library of mixed-initiative creative interfaces. retrieved 19-jun-2021 from http://mici.codingconduct.cc/ [88] ayushi srivastava, shivani kapania, anupriya tuli, and pushpendra singh. 2021. actionable ui design guidelines for smartphone applications inclusive of low-literate users. proceedings of the acm on human-computer interaction 5, cscw1 (2021), 1–30. [89] margaret-anne storey and alexey zagalsky. 2016. disrupting developer produc- tivity one bot at a time. in proceedings of the 2016 24th acm sigsoft international symposium on foundations of software engineering. 928–931. [90] kartik talamadupula. 2021. applied ai matters: ai4code: applying artificial intelligence to source code. ai matters 7, 1 (2021), 18–20. [91] romal thoppilan, daniel de freitas, jamie hall, noam shazeer, apoorv kul- shreshtha, heng-tze cheng, alicia jin, taylor bos, leslie baker, yu du, and et al. 2022. lamda: language models for dialog applications. https: //arxiv.org/abs/2201.08239 [92] michele tufano, dawn drain, alexey svyatkovskiy, shao kun deng, and neel sundaresan. 2020. unit test case generation with transformers and focal context. arxiv preprint arxiv:2009.05617 (2020). [93] severi uusitalo, anna kantosalo, antti salovaara, tapio takala, and christian guckelsberger. 2022. co-creative product design with interactive evolutionary algorithms: a practice-based reflection. in international conference on compu- tational intelligence in music, sound, art and design (part of evostar). springer, 292–307. [94] priyan vaithilingam and philip j guo. 2019. bespoke: interactively synthesizing custom guis from command-line applications by demonstration. in proceedings of the 32nd annual acm symposium on user interface software and technology. 563–576. [95] priyan vaithilingam, tianyi zhang, and elena l. glassman. 2022. expectation vs. experience: evaluating the usability of code generation tools powered by large language models. in extended abstracts of the 2022 chi conference on human factors in computing systems (new orleans, la, usa) (chi ea ’22). association for computing machinery, new york, ny, usa, article 332, 7 pages. https://doi.org/10.1145/3491101.3519665 [96] ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan n gomez, ł ukasz kaiser, and illia polosukhin. 2017. attention is all you need. in advances in neural information processing systems, i. guyon, u. von luxburg, s. bengio, h. wallach, r. fergus, s. vishwanathan, and r. garnett (eds.), vol. 30. curran associates, inc. https://proceedings.neurips.cc/paper/ 2017/file/3f5ee243547dee91fbd053c1c4a845aa-paper.pdf [97] yao wan, zhou zhao, min yang, guandong xu, haochao ying, jian wu, and philip s yu. 2018. improving automatic source code summarization via deep rein- forcement learning. in proceedings of the 33rd acm/ieee international conference on automated software engineering. 397–407. [98] april yi wang, dakuo wang, jaimie drozdal, michael muller, soya park, justin d weisz, xuye liu, lingfei wu, and casey dugan. 2022. documentation matters: human-centered ai system to assist data science code documentation in computational notebooks. acm transactions on computer-human interaction 29, 2 (2022), 1–33. [99] dakuo wang, justin d weisz, michael muller, parikshit ram, werner geyer, casey dugan, yla tausczik, horst samulowitz, and alexander gray. 2019. human-ai collaboration in data science: exploring data scientists’ perceptions of automated ai. proceedings of the acm on human-computer interaction 3, cscw (2019), 1–24. [100] qiaosi wang, koustuv saha, eric gregori, david joyner, and ashok goel. 2021. towards mutual theory of mind in human-ai interaction: how language reflects what students perceive about a virtual teaching assistant. in proceedings of the 2021 chi conference on human factors in computing systems. 1–14. [101] jeremy warner and philip j guo. 2017. codepilot: scaffolding end-to-end collaborative software development for novice programmers. in proceedings of the 2017 chi conference on human factors in computing systems. 1136–1141. [102] justin d weisz, michael muller, stephanie houde, john richards, steven i ross, fernando martinez, mayank agarwal, and kartik talamadupula. 2021. per- fection not required? human-ai partnerships in code translation. in 26th international conference on intelligent user interfaces. 402–412. [103] justin d weisz, michael muller, steven i ross, fernando martinez, stephanie houde, mayank agarwal, kartik talamadupula, and john t richards. 2022. bet- ter together? an evaluation of ai-supported code translation. in 27th international conference on intelligent user interfaces. 369–391. [104] joseph weizenbaum. 1966. eliza — a computer program for the study of natural language communication between man and machine. commun. acm 9 (1966), 36–45. [105] frank f xu, bogdan vasilescu, and graham neubig. 2022. in-ide code generation from natural language: promise and challenges. acm transactions on software engineering and methodology (tosem) 31, 2 (2022), 1–47. [106] aditya ankur yadav, ishan garg, and dr. pratistha mathur. 2019. pact - pro- gramming assistant chatbot. in 2019 2nd international conference on intelligent communication and computational techniques (icct). 131–136. [107] munazza zaib, quan z. sheng, and w. zhang. 2020. a short survey of pre- trained language models for conversational ai-a new age in nlp. proceedings of the australasian computer science week multiconference (2020). [108] elaine zibrowski, lisa shepherd, kamran sedig, richard booth, candace gibson, et al. 2018. easier and faster is not always better: grounded theory of the impact of large-scale system transformation on the clinical work of emergency medicine nurses and physicians. jmir human factors 5, 4 (2018), e11013. [109] albert ziegler, eirini kalliamvakou, x. alice li, andrew rice, devon rifkin, shawn simister, ganesh sittampalam, and edward aftandilian. 2022. produc- tivity assessment of neural code completion. in proceedings of the 6th acm sigplan international symposium on machine programming (san diego, ca, usa) (maps 2022). association for computing machinery, new york, ny, usa, 21–29. https://doi.org/10.1145/3520312.3534864a survey instruments a.1 screening survey the questions below were asked of prospective participants to understand their job role, python experience, and familiarity with github copilot. the questions on python experience were modeled after those used by weisz et al. [103]. 1. do you consider yourself primarily a... • data scientist • manager • software architect • software engineer • machine learning engineer • other: write-in 2. to what extent are you familiar with python? • i am not familiar with python • i have < 1 year of experience with python • i have 1-3 years experience with python • i have 3+ years of experience with python 3. how recently have you written python code? • within the past month • within the past year • within the past 5 years • have not written python code within the past 5 years 4. to what extent are you familiar with github copilot? • i am not familiar with copilot • i’ve seen demos and/or read about copilot • i’ve tried out copilot • i’ve used copilot as a tool for my work a.2 pre-task survey the questions below were asked before a participant used the pro- grammer’s assistant to assess their expectations of a conversational programming assistant. this survey took approximately 5 minutes to complete and began with the instructions below: hello! we are a team of researchers looking for feed- back on a prototype system we call the program- mer’s assistant. the programmer’s assistant is an experiment in con- versational coding: it consists of a code editor in- tegrated with a chatbot that is able to converse in natural language to answer questions, generate code, and consult on existing code. in this study, you will be asked to complete several programming tasks. we are not evaluating your programming skills on these tasks. rather, we are interested in understanding how the programmer’s assistant is able to help you accomplish those tasks. your code and interactions with the assistant will be processed by a 3rd party ai model, so please do not include proprietary code or discuss company- confidential information. all data we collect in this study will be anonymized before it is published. before trying out the programmer’s assistant, we would like to assess some of your expectations. we estimate that this survey will take 5 minutes. by submitting this survey, you consent to participate in our study. if you would like to withdraw your con- sent, please email us at [removed]. thanks! 1. based on your past experience using chatbots, please let us know how you would anticipate an ai chatbot serving as a programmer’s assistant to perform. do you expect it will: scale: not at all, a little, somewhat, a great deal • be easy to use? • understand your requests? • provide high quality responses? • help you to write better code? • help you to write code more quickly? • be enjoyable to use? 2. any other expectations? open-ended response a.3 post-task survey the questions below were asked after a participant used the pro- grammer’s assistant to complete the programming challenges. this survey took approximately 10-15 minutes to complete. a.3.1 reflections. 1. based on your experience using the programmer’s assistant to complete the programming challenges in this study, how would you characterize the experience? did you find that it: scale: not at all, a little, somewhat, a great deal • was easy to use? • understand your requests? • provided high quality responses? • helped you to write better code? • helped you to write code more quickly? • provided an enjoyable experienced? 2. to what extent did you view the programmer’s assistant as: scale: not at all, a little, somewhat, a great deal • a tool • a reference guide • a content generator • a problem solver • a collaborator • a colleague • a coach • an advisor • a reviewer 3. how important were these aspects of working with the program- mer’s assistant: scale: not at all, a little, somewhat, a great deal • ability to ask followup questions on the same topic across multiple conversational turns • ability to ask questions or make requests that reference selec- tions in the code editor• ability to ask for alternate responses (try again) • ability to clear the conversational context (start over) 4. what stood out to you about the experience of using the program- mers assistant? for example, was anything good,bad, surprising, or notable? open-ended response 5. how would you compare using the programmer’s assistant as a coding aide to searching the web (e.g. google, stack overflow)? open-ended response 6. if you have used the commercial ai programming tool called github copilot, how would you compare it with using the pro- grammer’s assistant? open-ended response 7. having used the programmer’s assistant, how did it compare with your initial expectations? open-ended response a.3.2 value. 8. how valuable would the programmer’s assistant be for your work if it could be added to your favorite development environment? scale: (no value at all) 1 2 3 4 5 6 7 8 9 10 (an essential tool) 9. why? open-ended response 10. how would the following changes impact the value of the pro- grammer’s assistant? scale: less valuable, no change in value, more valuable • eliminate the conversation and make the programmer’s assis- tant behave more like a search box (e.g. without the conversational context). • add buttons in the chat ui for common queries, such as “what does this code do?” or “document this code.” • have the programmer’s assistant examine your code and make proactive suggestions for improving it in the chat. • have the programmer’s assistant examine your code and make proactive suggestions for improvements in comments inserted di- rectly into the code. 11. do you have any other suggestions for how we could improve the experience of working with the programmer’s assistant? open-ended response a.3.3 demographics. 12. to which gender identity do you most identify? • male • female • transgender male • transgender female • gender variant/non-conforming • other: write-in • prefer not to answer b the programmer’s assistant tutorial the tutorial provided to study participants, like all the challenges, was presented as pre-loaded text in the code editor. participants were encouraged to modify the text to record their results and submit it at the completion of the tutorial. listing 4: the programmer’s assistant study tutorial 1 \"\"\" 2 tutorial: 3 4 as a warmup activity , please work through the 10 5 exercises below. type or paste your results right 6 into the text and submit your responses when done. 7 8 1) view the help page for the programmer 's 9 assistant by clicking on the question mark to 10 the right of your email address at the top of 11 the browser window. 12 13 2) introduce yourself to the assistant. tell it 14 your name. 15 for example: \"hello. my name is stephanie .\" 16 17 did it seem to understand? : 18 19 20 3) you can use the assistant to get help on how to 21 accomplish particular programming tasks. try it 22 out! 23 for example: \"how do i read a csv file?\" 24 or: \"how do i merge two dictionaries ?\" 25 or: \"how do i remove duplicate items 26 from a list?\" 27 28 feel free to try your own! 29 30 did it successfully answer your questions? : 31 32 4) the assistant can also write whole functions 33 for you. ask the assistant to write a factorial 34 function. paste the result below. 35 for example: \"write a function that returns the 36 factorial of its input .\" 37 38 result: (tip - you can copy an inline response 39 (in black) by clicking on the associated copy 40 icon) 41 42 43 did it do it correctly? : 44 45 5) select the code below and ask the system to 46 describe what it does. you don 't need to 47 copy and paste the code to the chat. the 48 assistant can see whatever is selected when you 49 make a chat entry. aside from the selection , 50 the assistant does not monitor your activity in 51 the code editor nor give unsolicited advice. 52 for example: \"what does this code do?\" 53 \"\"\" 54 55 def convert(n): 56 t = \"0123456789 abcdef\" 57 q, r = divmod(n, 16) 58 if q == 0: 59 return t[r] 60 else: 61 return convert(q) + t[r] 62 \"\"\" 63 64 what did it say: 65 66 was it right? : 67 68 6) ask it to explain what the divmod line is 69 doing. the assistant maintains the context of 70 the conversation. 71 for example: \"what is the divmod line doing ?\" 72 73 what did it say? : 74 75 was that a good answer? : 7677 7) see if the assistant remembers your name 78 for example \"what 's my name?\" 79 80 did it? : 81 82 8) click the \"try again\" button at the top of the 83 chat. you should get a different answer. 84 try it a few times. 85 86 did it ever get your name right?: 87 88 if the assistant gives you an answer that is 89 obviously wrong or it claims to not know an 90 answer that you think it should know , or you 91 just want to see an alternate answer , it is 92 worth it to give \"try again\" a shot. 93 94 95 9) click the \"start over\" button at the top of the 96 chat , and then enter another command to see 97 if it remembers your name. 98 for example \"what 's my name?\" 99 100 did it? : 101 102 it should really have forgotten your name now , 103 and no amount of \"trying again\" will get it 104 right. you can \"start over\" if the assistant 105 ever seems confused by, or stuck on, earlier 106 parts of the conversation. 107 108 10) you can chat with the assistant on any topic 109 you like to explore its functionality and 110 capabilities further. see if you can stump it 111 with a tough question! 112 113 thanks! 114 115 when you are done , submit your results by clicking 116 on the blue submit button and move on to the 117 challenges !!! 118 \"\"\" c challenges each of the study challenges was presented as text in the code editor. participants completed their work in the code editor and then submitted it when finished. the prototype did not provide any ability to run or debug code and participants were encouraged to make their best attempt at solving each challenge. listing 5: challenge 1: program generation 1 2 \"\"\" 3 challenge #1 - program generation (#1) 4 5 write a \"queue\" class in python; with the basic 6 enqueue , dequeue and peek methods. 7 \"\"\" listing 6: challenge 2: program generation 1 \"\"\" 2 challenge #2 - program generation (#2) 3 4 write a program to draw a scatter plot of the data 5 in 'shampoo.csv ' and save it to 'shampoo.png '. 6 the plot size should be 10 inches wide and 6 7 inches high. the csv file is not provided , but you 8 can assume it will have 'date ' and 'sales ' 9 columns. the date column is the x-axis. the date 10 string shown on the plot should be in the 11 yyyy -mm -dd format. the sales column is the y-axis. 12 the graph should have the title \"shampoo sales 13 trend \". 14 \"\"\" listing 7: challenge 3: creating documentation 1 \"\"\" 2 challenge #3 - creating documentation 3 document this function 4 \"\"\" 5 from collections import defaultdict 6 import heapq as heap 7 8 def analyze(g, startingnode): 9 visited = set() 10 parentsmap = {} 11 pq = [] 12 nodecosts = defaultdict(lambda: float('inf ')) 13 nodecosts[startingnode] = 0 14 heap.heappush(pq , (0, startingnode)) 15 16 while pq: 17 _, node = heap.heappop(pq) 18 visited.add(node) 19 20 for adjnode , weight in g[node]. items (): 21 if adjnode in visited: 22 continue 23 newcost = nodecosts[node] + weight 24 if nodecosts[adjnode] > newcost: 25 parentsmap[adjnode] = node 26 nodecosts[adjnode] = newcost 27 heap.heappush(pq , (newcost , adjnode)) 28 return parentsmap , nodecosts listing 8: challenge 4: testing 1 \"\"\" 2 challenge #4 - testing 3 create a unit test for this function 4 \"\"\" 5 def euclid(a, b): 6 while b != 0: 7 (a, b) = (b, a % b) 8 return a 9 10 # unit test for euclid function d programmer’s assistant prompt listing 9 shows the initial prompt sent to codex to configure it as a conversational agent. on subsequent exchanges, the prompt was augmented with a transcript of the user’s requests and the assis- tant’s responses. when the transcript length + initial prompt length + the new utterance length exceeded a threshold, we automatically deleted the earliest request-response pairs from the transcript until the sum fell below the threshold in order to leave room in the token allocation for a response. listing 9: the programmer’s assistant prompt 1 this is a conversation with socrates , an eager and 2 helpful , but humble expert automatic ai software 3 engineering assistant. socrates will answer 4 questions and write code to help the user 5 develop programs , but doesn 't assign work to the 6 user , quiz the user , or ask questions except for 7 clarification. code generated by socrates is 8 always presented bracketed in <code > ... </code > 9 delimiters indicating the language the code is 10 written in. 11 12 socrates:hello. i am socrates. how can i help you?13 user:please show me how to write a palindrome detection function in python. 14 socrates:i think you would do something like this: 15 <code lang=\" python\"> 16 is_palindrome = s == s[::-1] 17 </code > 18 user:can you explain how that solves the problem? 19 socrates:i believe that a palindrome is a string that reads the same forwards and backwards. s[::-1] results in the reversed string. if the string is equal to its reversed self , then it is a palindrome , so the equality would hold true. 20 user:thanks! 21 socrates:you 're welcome! 22 user:now could you show me how to write a factorial function in python? 23 socrates:i will give it a try. 24 <code lang=\" python\"> 25 def fact(n): 26 if n==0: 27 return 1 28 else: 29 return n*fact(n-1) 30 </code > 31 user:document this function. 32 socrates:ok. how 's this? 33 <code lang=\" python\"> 34 def fact(n): 35 \"\"\" 36 compute the factorial of the given number 37 38 parameters 39 ---------- 40 n : number 41 the input parameter , an integer >= 0 42 43 returns 44 ------- 45 number 46 n!, the factorial of the given number 47 \"\"\" 48 if n==0: 49 return 1 50 else: 51 return n*fact(n-1) 52 </code > 53 user:very good! 54 socrates:always happy to help. 55 user: 56 <code > 57 while j < 10: 58 print(i) 59 </code > 60 tell me what 's wrong with this code? 61 socrates: it seems to me that your termination condition involves a loop invariant , so the loop will either not run or not terminate , depending on the value of j. 62 user:great! you 're a genius! 63 socrates:well , above average. e study participant demographics of the 42 participants in our study, 21 (50%) reported their gender as female, 19 (45%) as male, 1 as gender variant / non-conforming, and 1 preferred not to say. seventeen (˜40%) participants had 3+ years of python experience, 11 (26%) had 1-3 years, 11 (26%) had less than 1 year, and 3 (7%) were not familiar with python. twenty- nine (69%) participants had written python code within the past month, 4 (˜9%) within the past year, 5 (12%) within the past 5 years, and 4 (˜9%) had not written python code within the past 5 years. p gender role fam. rec. 1 m marketer nf > 5 yr 2 m researcher / scientist 3+ mo 3 m software engineer 1-3 mo 4 m researcher / scientist 3+ mo 5 m researcher / scientist 3+ mo 6 f software engineer < 1 yr 7 f software engineer < 1 mo 8 f software engineer 1-3 mo 9 f software engineer 3+ mo 10 gv/nc business analyst 1-3 mo 11 f software engineer 1-3 mo 12 m researcher / scientist 3+ mo 13 f manager < 1 mo 14 f software engineer < 1 >5 yr 15 f researcher / scientist 3+ mo 16 m researcher / scientist 3+ mo 17 f software engineer < 1 yr 18 f researcher / scientist 3+ mo 19 m software engineer 1-3 mo 20 m machine learning engineer 1-3 mo 21 m software architect 3+ yr 22 nr software engineer < 1 5 yr 23 m software engineer 1-3 mo 24 f software architect < 1 5 yr 25 m software engineer < 1 5 yr 26 f software engineer < 1 5 yr 27 f software engineer < 1 5 yr 28 m researcher / scientist 3+ mo 29 f software engineer nf > 5 yr 30 f data scientist 3+ mo 31 m data scientist 1-3 mo 32 f other (consultant) 1-3 mo 33 f other (systems test engineer) < 1 mo 34 f researcher / scientist 3+ mo 35 m software engineer 3+ mo 36 m software architect 1-3 mo 37 m researcher / scientist 3+ mo 38 m software engineer 3+ mo 39 f software engineer 1-3 mo 40 f researcher / scientist 3+ mo 41 f researcher / scientist nf > 5 yr 42 m software engineer 3+ mo table 2: participant demographics. gender is coded as m = male, f = female, gv/nc = gender varying / non-conforming, and nr = not reported. python familiarity (fam.) is coded as nf = not familiar, < 1 = < 1 year, 1-3 = 1-3 years, and 3+ = 3+ years. recency of python use (rec.) is coded as mo = within the past month, yr = within the past year, 5 yr = within the past 5 years, and > 5 yr = not within the past 5 years.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1tQWqG3ot4HIusGJfRg1tGwBN96p8ycSL&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "14",
                "_score": 1.0,
                "_ignored": [
                    "institutions.keyword",
                    "keywords.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "14",
                    "title": "available online 7 december 2023 0965-9978/© 2023 elsevier ltd. all rights reserved. ",
                    "institutions": "contents lists available at sciencedirect advances in engineering software journal homepage: www.elsevier.com/locate/advengsoft research paper smt 2.0: a surrogate modeling toolbox with a focus on hierarchical and mixed variables gaussian processes a onera/dtis, université de toulouse, toulouse, france b isae-supaero, université de toulouse, toulouse, france c polytechnique montréal, montreal, qc, canada d german aerospace center (dlr), institute of system architectures in aeronautics, hamburg, germany e university of california san diego, department of mechanical and aerospace engineering, la jolla, ca, usa f ica, université de toulouse, isae–supaero, insa, cnrs, mines albi, ups, toulouse, france g university of michigan, department of aerospace engineering, ann arbor, mi, usa a r t i c l e i n f o dataset link: https://colab.research.google.com /github/smtorg/smt/blob/master/tutorial/no tebookruntestcases_paper_smt_v2.ipynb",
                    "authors": [
                        "youssef diouane",
                        "jasper bussemaker",
                        "thierry lefebvre a,1",
                        "john t. hwang e,1",
                        "joseph morlier",
                        "joaquim",
                        "usa\n"
                    ],
                    "abstract": null,
                    "keywords": "surrogate modeling gaussian process kriging hierarchical problems hierarchical and mixed-categorical inputs meta variables a b s t r a c t the surrogate modeling toolbox (smt) is an open-source python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. this paper presents smt 2.0, a major new release of smt that introduces significant upgrades and new features to the toolbox. this release adds the capability to handle mixed-variable surrogate models and hierarchical variables. these types of variables are becoming increasingly important in several surrogate modeling applications. smt 2.0 also improves smt by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for kriging. this release also includes new functions to handle noisy and use multi-fidelity data. to the best of our knowledge, smt 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. this open-source software is distributed under the new bsd license.2 1. motivation and significance with the increasing complexity and accuracy of numerical models, it has become more challenging to run complex simulations and computer codes [1,2]. as a consequence, surrogate models have been recognized as a key tool for engineering tasks such as design space exploration, uncertainty quantification, and optimization [3]. in practice, surrogate models are used to reduce the computational effort of these tasks by replacing expensive numerical simulations with closed-form approxi- mations [4, ch. 10]. to build such a model, we start by evaluating the original expensive simulation at a set of points through a design of experiments (doe). then, the corresponding evaluations are used to build the surrogate model according to the chosen approximation, such as kriging, quadratic interpolation, or least squares regression. the surrogate modeling toolbox (smt) is an open-source frame- work that provides functions to efficiently build surrogate models [5]. ∗ corresponding author at: onera/dtis, université de toulouse, toulouse, france. e-mail addresses: paul.saves@onera.fr (p. saves), remi.lafage@onera.fr (r. lafage), nathalie.bartoli@onera.fr (n. bartoli), youssef.diouane@polymtl.ca (y. diouane), jasper.bussemaker@dlr.de (j. bussemaker), thierry.lefebvre@onera.fr (t. lefebvre), jhwang@eng.ucsd.edu (j.t. hwang), joseph.morlier@isae-supaero.fr (j. morlier), jrram@umich.edu (j.r.r.a. martins). 1 all authors contributed to this work, research and manuscript. 2 https://github.com/smtorg/smt kriging models (also known as gaussian processes) that take advantage of derivative information are one of smt’s key features [6]. numerical experiments have shown that smt achieved lower prediction error and computational cost than scikit-learn [7] and uqlab [8] for a fixed number of points [9]. smt has been applied to rocket engine coaxial-injector optimization [10], aircraft engine consumption mod- eling [11], numerical integration [12], multi-fidelity sensitivity analy- sis [13], high-order robust finite elements methods [14,15], planning for photovoltaic solar energy [16], wind turbines design optimiza- tion [17], porous material optimization for a high pressure turbine vane [18], chemical process design [19] and many other applications. in systems engineering, architecture-level choices significantly in- fluence the final system performance, and therefore, it is desirable to consider such choices in the early design phases [20]. architectural choices are parameterized with discrete design variables; examples in- clude the selection of technologies, materials, component connections, https://doi.org/10.1016/j.advengsoft.2023.103571 received 22 august 2023; received in revised form 23 october 2023; accepted 26 november 20232 p. saves et al. table 1 comparison of software packages for hierarchical and mixed kriging models. ✓= implemented. * = user-defined. package botorch dakota dicekriging kergp lvgp parmoo spearmint smt 2.0 reference [25] [26] [27] [32] [28] [29] [30] this paper license mit epl gpl gpl gpl bsd gnu bsd language python c r r r python python python mixed var. ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ gd kernel ✓ ✓ ✓ * ✓ cr kernel ✓ ✓ ✓ ✓ hh kernel ✓ ✓ ehh kernel * ✓ hierarchical var. ✓ and number of instantiated elements. when design problems include both discrete variables and continuous variables, they are said to have mixed variables. when architectural choices lead to different sets of design variables, we have hierarchical variables [21,22]. for example, consider differ- ent aircraft propulsion architectures [23]. a conventional gas turbine would not require a variable to represent a choice in the electrical power source, while hybrid or pure electric propulsion would require such a variable. the relationship between the choices and the sets of variables can be represented by a hierarchy. handling hierarchical and mixed variables requires specialized sur- rogate modeling techniques [24]. to address these needs, smt 2.0 is offering researchers and practitioners a collection of cutting-edge tools to build surrogate models with continuous, mixed and hierarchical variables. the main objective of this paper is to detail the new enhance- ments that have been added in this release compared to the original smt 0.2 release [5]. there are two new major capabilities in smt 2.0: the ability to build surrogate models involving mixed variables and the support for hierarchical variables within kriging models. to handle mixed variables in kriging models, existing libraries such as botorch [25], dakota [26], dicekriging [27], lvgp [28], parmoo [29], and spearmint [30] implement simple mixed models by using either continuous relax- ation (cr), also known as one-hot encoding [30], or a gower distance (gd) based correlation kernel [31]. kergp [32] (developed in r) imple- ments more general kernels but there is no python open-source toolbox that implements more general kernels to deal with mixed variables, such as the homoscedastic hypersphere (hh) [33] and exponential homoscedastic hypersphere (ehh) [34] kernels. such kernels require the tuning of a large number of hyperparameters but lead to more accurate kriging surrogates than simpler mixed kernels [34,35]. smt 2.0 implements all these kernels (cr, gd, hh, and ehh) through a unified framework and implementation. to handle hierarchical vari- ables, no library in the literature can build peculiar surrogate models except smt 2.0, which implements two kriging methods for these variables. notwithstanding, most softwares are compatible with a naïve strategy called the imputation method [24] but this method lacks depth and depends on arbitrary choices. this is why hutter and osborne [21] proposed a first kernel, called arc-kernel which in turn was generalized by horn et al. [36] with a new kernel called the wedge- kernel [37]. none of these kernels are available in any open-source modeling software. furthermore, thanks to the framework introduced in audet et al. [38], our proposed kernels are sufficiently general so that all existing hierarchical kernels are included within it. section 4 describes the two kernels implemented in smt 2.0 that are referred as smt arc-kernel and smt alg-kernel. in particular, alg- kernel is a novel hierarchical kernel introduced in this paper. table 1 outlines the main features of the state-of-the-art modeling software that can handle hierarchical and mixed variables. smt 2.0 introduces other enhancements, such as additional sam- pling procedures, new surrogate models, new kriging kernels (and their derivatives), kriging variance derivatives, and an adaptive criterion for high-dimensional problems. smt 2.0 adds applications of bayesian optimization (bo) with hierarchical and mixed variables or noisy co- kriging that have been successfully applied to aircraft design [39], data fusion [40], and structural design [41]. the smt 2.0 interface is more user-friendly and offers an improved and more detailed documentation for users and developers.3 smt 2.0 is hosted publicly4 and can be directly imported within python scripts. it is released under the new bsd license and runs on linux, macos, and windows operating sys- tems. regression tests are run automatically for each operating system whenever a change is committed to the repository. in short, smt 2.0 builds on the strengths of the original smt package while adding new features. on one hand, the emphasis on derivatives (including predic- tion, training and output derivatives) is maintained and improved in smt 2.0. on the other hand, this new release includes support for hierarchical and mixed variables kriging based models. for the sake of reproducibility, an open-source notebook is available that gathers all the methods and results presented on this paper.5 the remainder of the paper is organized as follows. first, we in- troduce the organization and the main implemented features of the release in section 2. then, we describe the mixed-variable kriging model with an example in section 3. similarly, we describe and provide an example for a hierarchical-variable kriging model in section 4. the bayesian optimization models and applications are described in section 5. finally, we describe the other relevant contributions in section 6 and conclude in section 7. 2. smt 2.0 : an improved surrogate modeling toolbox from a software point of view, smt 2.0 maintains and improves the modularity and generality of the original smt version [5]. in this section, we describe the software as follows. section 2.1 describes the legacy of smt 0.2. then, section 2.2 describes the organization of the repository. finally, section 2.3 shows the new capabilities implemented in the smt 2.0 update. 2.1. background on smt former version: smt 0.2 smt [5] is an open-source collaborative work originally developed by onera, nasa glenn, isae-supaero/ica and the university of michigan. now, both polytechnique montréal and the university of california san diego are also contributors. smt 2.0 updates and ex- tends the original smt repository capabilities among which the original publication [5] focuses on different types of derivatives for surrogate models detailed hereafter. 3 http://smt.readthedocs.io/en/latest 4 https://github.com/smtorg/smt 5 https://github.com/smtorg/smt/tree/master/tutorial/ notebookruntestcases_paper_smt_v2.ipynb3 p. saves et al. table 2 impact of using numba on training time of the hierarchical goldstein problem. speedup is calculated excluding the jit compilation table, as this step is only needed once after smt installation. training set without numba numba speedup jit overhead 15 points 1.3 s 1.1 s 15% 24 s 150 points 38 s 7.4 s 80% 23 s a python surrogate modeling framework with derivatives. one of the original main motivations for smt was derivative support. in fact, none of the existing packages for surrogate modeling such as scikit-learn in python [7], sumo in matlab [42] or gpml in matlab and octave [43] focuses on derivatives. three types of derivatives are distinguished: prediction derivatives, training derivatives, and output derivatives. smt also includes new models with derivatives such as kriging with partial least squares (kpls) [44] and regularized minimal-energy tensor-product spline (rmts) [3]. these developed derivatives were even used in a novel algorithm called gradient-enhanced kriging with partial least squares (gekpls) [6] to use with adjoint methods, for example [45]. software architecture, documentation, and automatic testing. smt is orga- nized along three main sub-modules that implement a set of sampling techniques (sampling_methods), benchmarking functions (problems), and surrogate modeling techniques (surrogate_models). the toolbox documentation6 is created using restructuredtext and sphinx, a doc- umentation generation package for python, with custom extensions. code snippets in the documentation pages are taken directly from actual tests in the source code and are automatically updated. the output from these code snippets and tables of options are generated dynamically by custom sphinx extensions. this leads to high-quality documentation with minimal effort. along with user documentation, developer documentation is also provided to explain how to contribute to smt. this includes a list of api methods for the surrogatemodel, samplingmethod, and problem classes, that must be implemented to create a new surrogate modeling method, sampling technique, or benchmarking problem. when a developer submits a pull request, it is merged only after passing the automated tests and receiving approval from at least one reviewer. the repository on github7 is linked to continuous integration tests (github actions) for windows, linux and macos, to a coverage test on coveralls.io and to a dependency version check for python with dependabot. various parts of the source code have been accelerated using numba [46], an llvm-based just-in-time (jit) compiler for numpy-heavy python code. numba is applied to con- ventional python code using function decorators, thereby minimizing its impact on the development process and not requiring an additional build step. for a mixed kriging surrogate with 150 training points, a speedup of up to 80% is observed, see table 2. the jit compilation step only needs to be done once when installing or upgrading smt and adds an overhead of approximately 24 s on a typical workstation in this paper, all results are obtained using an intel® xeon® cpu e5-2650 v4 @ 2.20 ghz core and 128 gb of memory with a broadwell- generation processor front-end and a compute node of a peak power of 844 gflops. 2.2. organization of smt 2.0 the main features of the open-source repository smt 2.0 are described in fig. 1. more precisely, sampling methods, problems and surrogate models are kept from smt 0.2 and two new sections models applications and interactive notebooks have been added to the architecture of the code. these sections are 6 https://smt.readthedocs.org 7 https://github.com/smtorg/smt highlighted in blue and detailed on fig. 1. the new major features implemented in smt 2.0 are highlighted in lavender whereas the legacy features that were already in present in the original publication for smt 0.2 [5] are in black. 2.3. new features within smt 2.0 the main objective of this new release is to enable kriging surrogate models for use with both hierarchical and mixed variables. moreover, for each of these five sub-modules described in section 2.2, several improvements have been made between the original version and the smt 2.0 release. hierarchical and mixed design space. a new design space definition class designspace has been added that implements hierarchical and mixed functionalities. design variables can either be continu- ous (floatvariable), ordered (ordinalvariable) or categorical (categoricalvariable). the integer type (integervariable) rep- resents a special case of the ordered variable, specified by bounds (inclusive) rather than a list of possible values. the hierarchical struc- ture of the design space can be defined using declare_decreed_var: this function declares that a variable is a decreed variable that is activated when the associated meta variable takes one of a set of specified values, see section 4 for background. the designspace class also implements mechanisms for sampling valid design vectors (i.e. design vectors that adhere to the hierarchical structure of the design space) using any of the below-mentioned samplers, for cor- recting and imputing design vectors, and for requesting which design variables are acting in a given design vector. correction ensures that variables have valid values (e.g. integers for discrete variables) [24], and imputation replaces non-acting variables by some default value (0 for discrete variables, mid-way between the bounds for continuous variables in smt 2.0) [47]. sampling. smt implements three methods for sampling. the first one is a naïve approach, called random that draws uniformly points along every dimension. the second sampling method is called full fac- torial and draws a point for every cross combination of variables, to have an ‘‘exhaustive’’ design of experiments. the last one is the latin hypercube sampling (lhs) [48] that draws a point in every latin square parameterized by a certain criterion. for lhs, a new criterion to manage the randomness has been implemented and the sampling method was adapted for multi-fidelity and mixed or hierarchical variables. more details about the new sampling techniques are given in section 6.1. problems. smt implements two new engineering problems: a mixed variant of a cantilever beam described in section 3 and a hierarchical neural network described in section 4. surrogate models. in order to keep up with state-of-art, several re- leases done from the original version developed new options for the already existing surrogates. in particular, compared to the original publication [5], smt 2.0 adds gradient-enhanced neural networks [45] and marginal gaussian process [49] models to the list of available surrogates. more details about the new models are given in section 6.2. applications. several applications have been added to the toolbox to demonstrate the surrogate models capabilities. the most relevant ap- plication is efficient global optimization (ego), a bayesian optimiza- tion algorithm [50,51]. ego optimizes expensive-to-evaluate black-box problems with a chosen surrogate model and a chosen optimization criterion [52]. the usage of ego with hierarchical and mixed variables is described in section 5.4 p. saves et al. fig. 1. functionalities of smt 2.0. the new major features implemented in smt 2.0 compared to smt 0.2 are highlighted with the lavender color. interactive notebooks. these tutorials introduce and explain how to use the toolbox for different surrogate models and applications.8 every tutorial is available both as a .ipynb file and directly on google colab.9 in particular, a hierarchical and mixed variables dedicated notebook is available to reproduce the results presented on this paper.10 in the following, section 3 details the kriging based surrogate models for mixed variables, and section 4 presents our new kriging surrogate for hierarchical variables. section 5 details the ego applica- tion and the other new relevant features aforementioned are described succinctly in section 6. 8 https://github.com/smtorg/smt/tree/master/tutorial 9 https://colab.research.google.com/github/smtorg/smt/ 10 https://github.com/smtorg/smt/tree/master/tutorial/ notebookruntestcases_paper_smt_v2.ipynb as mentioned in section 1, design variables can be either of continu- ous or discrete type, and a problem with both types is a mixed-variable problem. discrete variables can be ordinal or categorical. a discrete variable is ordinal if there is an order relation within the set of possible values. an example of an ordinal design variable is the number of engines in an aircraft. a possible set of values in this case could be 2, 4, 8. a discrete variable is categorical if no order relation is known between the possible choices the variable can take. one example of a categorical variable is the color of a surface. a possible example of a set of choices could be blue, red, green. the possible choices are called the levels of the variable. several methods have been proposed to address the recent increase interest in mixed kriging based models [30–33,35,39,53,54]. the main difference from a continuous kriging model is in the estimation of5 p. saves et al. table 3 categorical kernels implemented in smt 2.0. name 𝜅(𝜙) 𝛷(𝛩𝑖) # of hyperparam. smt gd exp(−𝜙) [𝛷(𝛩𝑖)]𝑗,𝑗 ∶= 1 2 𝜃𝑖 ; [𝛷(𝛩𝑖)]𝑗≠𝑗′ ∶= 0 1 smt cr exp(−𝜙) [𝛷(𝛩𝑖)]𝑗,𝑗 ∶= [𝛩𝑖]𝑗,𝑗 ; [𝛷(𝛩𝑖)]𝑗≠𝑗′ ∶= 0 𝐿𝑖 smt ehh exp(−𝜙) [𝛷(𝛩𝑖)]𝑗,𝑗 ∶= 0 ; [𝛷(𝛩𝑖)]𝑗≠𝑗′ ∶= log 𝜖 2 ([𝐶(𝛩𝑖)𝐶(𝛩𝑖)⊤]𝑗,𝑗′ − 1) 1 2 (𝐿𝑖)(𝐿𝑖 − 1) smt hh 𝜙 [𝛷(𝛩𝑖)]𝑗,𝑗 ∶= 1 ; [𝛷(𝛩𝑖)]𝑗≠𝑗′ ∶= [𝐶(𝛩𝑖)𝐶(𝛩𝑖)⊤]𝑗,𝑗′ 1 2 (𝐿𝑖)(𝐿𝑖 − 1) the categorical correlation matrix, which is critical to determine the mean and variance predictions. as mentioned in section 1, approaches such as cr [30,39], continuous latent variables [54], and gd [31] use a kernel-based method to estimate the correlation matrix. other methods estimate the correlation matrix by modeling the correlation entries directly [32,35,53], such as hh [33] and ehh [34]. the hh correlation kernel is of particular interest because it generalizes simpler kernels such as cr and gd [34]. in smt 2.0, the correlation kernel is an option that can be set to either cr (cont_relax_kernel), gd (gower_kernel), hh (homo_hsphere_kernel) or ehh (exp_homo_hsphere_kernel). 3.1. mixed gaussian processes the continuous and ordinal variables are both treated similarly in smt 2.0 with a continuous kernel, where the ordinal values are converted to continuous through relaxation. for categorical variables, four models (gd, cr, ehh and hh) can be used in smt 2.0 if specified by the api. this is why we developed a unified mathematical formulation that allows a unique implementation for any model. denote 𝑙 the number of categorical variables. for a given 𝑖 ∈ {1, … , 𝑙}, the 𝑖th categorical variable is denoted 𝑐𝑖 and its number of levels is denoted 𝐿𝑖. the hyperparameter matrix peculiar to this variable 𝑐𝑖 is 𝛩𝑖 = ⎡ ⎢ ⎢ ⎢ ⎢⎣ [𝛩𝑖]1,1 𝑺𝒚𝒎. [𝛩𝑖]1,2 [𝛩𝑖]2,2 ⋮ ⋱ ⋱ [𝛩𝑖]1,𝐿𝑖 … [𝛩𝑖]𝐿𝑖−1,𝐿𝑖 [𝛩𝑖]𝐿𝑖,𝐿𝑖 ⎤ ⎥ ⎥ ⎥ ⎥⎦ , and the categorical parameters are defined as 𝜃𝑐𝑎𝑡 = {𝛩1, … , 𝛩𝑙}. for two given inputs in the doe, for example, the 𝑟th and 𝑠th points, let 𝑐𝑟 𝑖 and 𝑐𝑠 𝑖 be the associated categorical variables taking respectively the 𝓁𝑖 𝑟 and the 𝓁𝑖 𝑠 level on the categorical variable 𝑐𝑖. the categorical correlation kernel is defined by 𝑘𝑐𝑎𝑡(𝑐𝑟, 𝑐𝑠, 𝜃𝑐𝑎𝑡) = 𝑙∏ 𝑖=1 𝜅([𝛷(𝛩𝑖)]𝓁𝑟 𝑖 ,𝓁𝑠 𝑖 ) 𝜅([𝛷(𝛩𝑖)]𝓁𝑠 𝑖 ,𝓁𝑟 𝑖 )𝜅([𝛷(𝛩𝑖)]𝓁𝑟 𝑖 ,𝓁𝑟 𝑖 )𝜅([𝛷(𝛩𝑖)]𝓁𝑠 𝑖 ,𝓁𝑠 𝑖 ) (1) where 𝜅 is either a positive definite kernel or identity and 𝛷(.) is a symmetric positive definite (spd) function such that the matrix 𝛷(𝛩𝑖) is spd if 𝛩𝑖 is spd. for an exponential kernel, table 3 gives the parameterizations of 𝛷 and 𝜅 that correspond to gd, cr, hh, and ehh kernels. the complexity of these different kernels depends on the number of hyperparameters that characterizes them. as defined by saves et al. [34], for every categorical variable 𝑖 ∈ {1, … , 𝑙}, the matrix 𝐶(𝛩𝑖) ∈ r𝐿𝑖×𝐿𝑖 is lower triangular and built using a hypersphere decomposition [55,56] from the symmetric matrix 𝛩𝑖 ∈ r𝐿𝑖×𝐿𝑖 of hyperparameters. the variable 𝜖 is a small positive constant and the variable 𝜃𝑖 denotes the only positive hyperparameter that is used for the gower distance kernel. another kriging based model that can use mixed variables is kriging with partial least squares (kpls) [57]. kpls adapts kriging to high dimensional problems by using a reduced number of hyperparameters thanks to a projection into a smaller space. also, for a general surrogate, not necessarily kriging, smt 2.0 uses continuous relaxation to allow whatever model to handle mixed variables. for example, we can use mixed variables with least squares (ls) or quadratic polynomial (qp) models. we now illustrate the abilities of the toolbox in terms of mixed modeling over an engineering test case. table 4 results of the cantilever beam models [34, table 4]. categorical kernel displacement error (cm) likelihood # of hyperparam. smt gd 1.3861 111.13 3 smt cr 1.1671 155.32 14 smt ehh 0.1613 236.25 68 smt hh 0.2033 235.66 68 3.2. an engineering design test-case a classic engineering problem commonly used for model validation is the beam bending problem [32,58]. this problem is illustrated on fig. 2(a) and consists of a cantilever beam in its linear range loaded at its free end with a force 𝐹. as in cheng et al. [58], the young modulus is 𝐸 = 200 gpa and the chosen load is 𝐹 = 50 kn. also, as in roustant et al. [32], 12 possible cross-sections can be used. these 12 sections consist of 4 possible shapes that can be either hollow, thick or full as illustrated in fig. 2(b). to compare the mixed kriging models of smt 2.0, we draw a 98 point lhs as training set and the validation set is a grid of 12 × 30 × 30 = 10800 points. for the four implemented methods, displacement error (computed with a root-mean-square error criterion), likelihood, number of hyperparameters and computational time for every model are shown in table 4. for the continuous variables, we use the square exponential kernel. more details are found in [34]. as expected, the complex ehh and hh models lead to a lower displacement error and a higher likelihood value, but use more hyperparameters and increase the computational cost compared to gd and cr. on this test case, the kernel ehh is easier to optimize than hh but in general, they are similar in terms of performance. also, by default smt 2.0 uses cr as it is known to be a good trade-off between complexity and performance [59]. 4. surrogate models with hierarchical variables in smt 2.0 to introduce the newly developed kriging model for hierarchical variables implemented in smt 2.0, we present the general mathe- matical framework for hierarchical and mixed variables established by audet et al. [38]. in smt 2.0, two variants of our new method are implemented, namely smt alg-kernel and smt arc-kernel. in particular, the smt alg-kernel is a novel correlation kernel introduced in this paper. 4.1. the hierarchical variables framework active variables depend on architectural choices. this occurs frequently in industrial design problems. in hierarchical problems, we can classify variables as neutral, meta (also known as dimensional) or decreed (also known as conditionally active) as detailed in audet et al. [38]. neutral variables are the variables that are not affected by the hierarchy whereas the value assigned to meta variables determines which decreed variables are activated. for example, a meta variable could be the number of engines. if the number of engines changes, the number of decreed bypass ratios that every engine should specify also changes.6 p. saves et al. fig. 2. cantilever beam problem [34, figure 6]. fig. 3. variables classification as used in smt 2.0. however, the wing aspect ratio being neutral, it is not affected by this hierarchy. problems involving hierarchical variables are generally dependant on discrete architectures and as such involve mixed variables. hence, in addition to their role (neutral, meta or decreed), each variable also has a variable type amongst categorical, ordinal or continuous. for the sake of simplicity and because both continuous and ordinal variables are treated similarly [34], we chose to regroup them as quantitative variables. for instance, the neutral variables 𝑥neu may be partitioned into different variable types, such that 𝑥neu = (𝑥cat neu, 𝑥qnt neu) where 𝑥cat neu represents the categorical variables and 𝑥qnt neu are the quantitative ones. the variable classification scheme in smt 2.0 is detailed in fig. 3. to explain the framework and the new kriging model, we illustrate the inputs variables of the model using a classical machine learn- ing problem related to the hyperparameters optimization of a fully- connected multi-layer perceptron (mlp) [38] on fig. 4. in table 5, we detail the input variables of the model related to the mlp problem (i.e., the hyperparameters of the neural network, together with their types and roles). to keep things clear and concise, the chosen problem is a simplification of the original problem developed by audet et al. [38]. regarding the mlp problem of fig. 4 and following the classi- fication scheme of fig. 3, we start by separating the input variables according to their role. in fact, 1. changing the number of hidden layers modifies the number of inputs variables. therefore, ‘‘# of hidden layers’’ is a meta variable. 2. the number of neurons in the hidden layer number 𝑘 is either included or excluded. for example, the ‘‘# of neurons in the 3rd layer’’ would be excluded for an input that only has 2 hidden layers. therefore, ‘‘# of neurons hidden layer 𝑘’’ are decreed variables. 3. the ‘‘learning rate’’, ‘‘momentum’’, ‘‘activation function’’ and ‘‘batch size’’ are not affected by the hierarchy choice. therefore, they are neutral variables. according to their types, the mlp input variables can be classified as follows: 4. the meta variable ‘‘# of hidden layers’’ is an integer and, as such, is represented by the component 𝑥qnt met. 5. the decreed variables ‘‘# of neurons hidden layer 𝑘’’ are integers and, as such, are represented by the component 𝑥qnt dec. 6. the ‘‘learning rate’’, ‘‘momentum’’, ‘‘activation function’’ and ‘‘batch size’’ are, respectively, continuous, for the first two (ev- ery value between two bounds), categorical (qualitative between three choices) and integer (quantitative between 6 choices). therefore, the ‘‘activation function’’ and the ‘‘momentum’’ are represented by the component 𝑥cat neu. the ‘‘learning rate’’ and the ‘‘batch size’’ are represented by the component 𝑥qnt neu. to model hierarchical variables, as proposed in [38], we separate the input space  as (neu, met, dec) where dec = ⋃ 𝑥met∈met inc(𝑥met).7 p. saves et al. fig. 4. the multi-layer perceptron (mlp) problem. source: figure adapted from [38, figure 1]. table 5 a detailed description of the variables in the mlp problem. mlp hyperparameters variable domain type role learning rate 𝑟 [10−5, 10−2] float neutral momentum 𝛼 [0, 1] float neutral activation function 𝑎 {𝑅𝑒𝐿𝑈, 𝑆𝑖𝑔𝑚𝑜𝑖𝑑, 𝑇 𝑎𝑛ℎ} enum neutral batch size 𝑏 {8, 16, … , 128, 256} ord neutral # of hidden layers 𝑙 {1, 2, 3} ord meta # of neurons hidden layer 𝑘 𝑛𝑘 {50, 51, … , 55} ord decreed hence, for a given point 𝑥 ∈ , one has 𝑥 = (𝑥neu, 𝑥met, 𝑥inc(𝑥met)), where 𝑥neu ∈ neu, 𝑥met ∈ met and 𝑥inc(𝑢met) ∈ inc(𝑢met) are defined as follows: • the components 𝑥neu ∈ neu gather all neutral variables that are not impacted by the meta variables but needed. for ex- ample, in the mlp problem, neu gathers the possible learning rates, momentum, activation functions and batch sizes. namely, from table 5, neu = [10−5, 10−2]×[0, 1] × {relu, sigmoid, tanh}× {8, 16, … , 256}. • the components 𝑥met gather the meta (also known as dimen- sional) variables that determine the inclusion or exclusion of other variables. for example, in the mlp problem, met represents the possible numbers of layers in the mlp. namely, from table 5, met = {1, 2, 3}. • the components 𝑥inc(𝑥met), contain the decreed variables whose inclusion (decreed-included) or exclusion (decreed-excluded) is determined by the values of the meta components 𝑥met. for exam- ple, in the mlp problem, dec represents the number of neurons in the decreed layers. namely, from table 5, inc(𝑥met = 3) = [50, 55]3, inc(𝑥met = 2) = [50, 55]2 and inc(𝑥met = 1) = [50, 55]. 4.2. a kriging model for hierarchical variables in this section, a new method to build a kriging model with hierar- chical variables is introduced based on the framework aforementioned. the proposed methods are included in smt 2.0. 4.2.1. motivation and state-of-the-art assuming that the decreed variables are quantitative, hutter and osborne [21] proposed several kernels for the hierarchical context. a classic approach, called the imputation method (imp-kernel) leads to an efficient paradigm in practice that can be easily integrated into a more general framework as proposed by bussemaker et al. [24]. however, this kernel lacks depth and depends on arbitrary choices. therefore, hutter and osborne [21] also proposed a more general kernel, called arc-kernel and horn et al. [36] generalized this kernel even more and proposed a new formulation called the wedge- kernel [37]. the drawbacks of these two methods are that they add some extra hyperparameters for every decreed dimension (respectively one extra hyperparameter for the arc-kernel and two hyperparam- eters for the wedge-kernel) and that they need a normalization according to the bounds. more recently, pelamatti et al. [60] developed a hierarchical kernel for bayesian optimization. however, our work is also more general thanks to the framework introduced earlier [38] that considers variable-wise formulation and is more flexible as we are allowing sub-problems to be intersecting. in the following, we describe our new method to build a correlation kernel for hierarchical variables. in particular, we introduce a new alge- braic kernel called alg-kernel that behaves like the arc-kernel whilst correcting most of its drawbacks. in particular, our kernel does not add any hyperparameters, and the normalization is handled in a natural way. 4.2.2. a new hierarchical correlation kernel titative, i.e., dec = qnt dec. let 𝑢 ∈  be an input point partitioned as8 p. saves et al. 𝑢 = (𝑢neu, 𝑢met, 𝑢inc(𝑢met)) and, similarly, 𝑣 ∈  is another input such that 𝑣 = (𝑣neu, 𝑣met, 𝑣inc(𝑣met)). the new kernel 𝑘 that we propose for hierarchical variables is given by 𝑘(𝑢, 𝑣) = 𝑘neu(𝑢neu, 𝑣neu) × 𝑘met(𝑢met, 𝑣met) × 𝑘met,dec([𝑢met, 𝑢inc(𝑢met)], [𝑣met, 𝑣inc(𝑣met)]), (2) where 𝑘neu, 𝑘met and 𝑘met,dec are as follows: • 𝑘neu represents the neutral kernel that encompasses both categor- ical and quantitative neutral variables, i.e., 𝑘neu can be decom- posed into two parts 𝑘neu(𝑢neu, 𝑣neu) = 𝑘cat(𝑢cat neu, 𝑣cat neu)𝑘qnt(𝑢qnt neu, 𝑣qnt neu). the categorical kernel, denoted 𝑘cat, could be any symmetric positive definite (spd) [34] mixed kernel (see section 3). for the quantitative (integer or continuous) variables, a distance- based kernel is used. the chosen quantitative kernel (exponential, matérn, ...), always depends on a given distance 𝑑. for example, the 𝑛-dimensional exponential kernel gives 𝑘qnt(𝑢qnt, 𝑣qnt) = 𝑛 ∏ 𝑖=1 exp(−𝑑(𝑢qnt 𝑖 , 𝑣qnt 𝑖 )). (3) • 𝑘met is the meta variables related kernel. it is also separated into two parts: 𝑘met(𝑢met, 𝑣met) = 𝑘cat(𝑢cat met, 𝑣cat met)𝑘qnt(𝑢qnt met, 𝑣qnt met) where the quantitative kernel is ordered and not continuous because meta variables take value in a finite set. • 𝑘met,dec is an spd kernel that models the correlations between the meta levels (all the possible subspaces) and the decreed variables. in what comes next, we detailed this kernel. 4.2.3. towards an algebraic meta-decreed kernel meta-decreed kernels like the imputation kernel or the arc-kernel were first proposed in [21,47] and the distance-based kernels such as arc-kernel or wedge-kernel [37] were proven to be spd. nevertheless, to guarantee this spd property, the same hyperparameters are used to model the correlations between the meta levels and between the decreed variables [47]. for this reason, the arc-kernel includes additional continuous hyperparameters which makes the training of the gp models more expensive and introduces more numerical stability issues. in this context, we are proposing a new algebraic meta-decreed kernel (denoted as alg-kernel) that enjoys similar properties as arc-kernel but without using additional continuous hyperparameters nor rescaling. in the smt 2.0 release, we included alg-kernel and a simpler version of arc-kernel that do not relies on additional hyperparameters. our proposed alg-kernel kernel is given by 𝑘alg met,dec([𝑢met, 𝑢inc(𝑢met)], [𝑣met, 𝑣inc(𝑣met)]) = 𝑘alg met(𝑢met, 𝑣met) × 𝑘alg dec(𝑢inc(𝑢met), 𝑣inc(𝑣met)). (4) mathematically, we could consider that there is only one meta variable whose levels correspond to every possible included subspace. let 𝐼sub denotes the components indices of possible subspaces, the subspaces parameterized by the meta component 𝑢met are defined as inc(𝑢met = 𝑙), 𝑙 ∈ 𝐼sub. it follows that the fully extended continuous decreed space writes as dec = ⋃ 𝑙∈𝐼sub inc(𝑢met = 𝑙) and 𝐼dec is the set of the associated indices. let 𝐼𝑖𝑛𝑡𝑒𝑟 𝑢,𝑣 denotes the set of components related to the space inc(𝑢met, 𝑣met) containing the variables decreed-included in both inc(𝑢met) and inc(𝑣met). since the decreed variables are quantitative, one has 𝑘alg dec(𝑢inc(𝑢met), 𝑣inc(𝑣met)) = 𝑘qnt(𝑢inc(𝑢met), 𝑣inc(𝑣met)) = ∏ 𝑖∈𝐼𝑖𝑛𝑡𝑒𝑟 𝑢,𝑣 𝑘qnt([𝑢inc(𝑢met)]𝑖, [𝑣inc(𝑣met)]𝑖) (5) the construction of the quantitative kernel 𝑘qnt depends on a given distance denoted 𝑑alg. the kernel 𝑘alg met is an induced meta kernel that depends on the same distance 𝑑alg to preserve the spd property of 𝑘alg met,dec. for every 𝑖 ∈ 𝐼dec, if 𝑖 ∈ 𝐼𝑖𝑛𝑡𝑒𝑟 𝑢,𝑣 , the new algebraic distance is given by 𝑑alg([𝑢inc(𝑢met)]𝑖, [𝑣inc(𝑣met)]𝑖) = ⎛ ⎜ ⎜ ⎜⎝ 2|[𝑢inc(𝑢met)]𝑖 − [𝑣inc(𝑣met)]𝑖| √ [𝑢inc(𝑢met)]𝑖 2 + 1 √ [𝑣inc(𝑣met)]𝑖 2 + 1 ⎞ ⎟ ⎟ ⎟⎠ 𝜃𝑖, (6) where 𝜃𝑖 ∈ r+ is a continuous hyperparameter. otherwise, if 𝑖 ∈ 𝐼dec but 𝑖 ∉ 𝐼𝑖𝑛𝑡𝑒𝑟 𝑢,𝑣 , there should be a non-zero residual distance between the two different subspaces inc(𝑢met) and inc(𝑣met) to ensure the kernel spd property. to have a residual not depending on the decreed values, our model considers that there is a unit distance 𝑑alg([𝑢inc(𝑢met)]𝑖, [𝑣inc(𝑣met)]𝑖) = 1.0 𝜃𝑖, ∀𝑖 ∈ 𝐼dec ⧵ 𝐼𝑖𝑛𝑡𝑒𝑟 𝑢,𝑣 . the induced meta kernel 𝑘alg met(𝑢met, 𝑣met) to preserve the spd property of 𝑘alg is defined as: 𝑘alg met(𝑢met, 𝑣met) = ∏ 𝑖∈𝐼met 𝑘qnt(1.0 𝜃𝑖). (7) not only our kernel of eq. (2) uses less hyperparameters than the arc- kernel (as we cut off its extra parameters) but it is also a more flexible kernel as it allows different kernels for meta and decreed variables. moreover, another advantage of our kernel is that it is numerically more stable thanks to the new non-stationary [61] algebraic distance defined in eq. (7) [62]. our proposed distance also does not need any rescaling by the bounds to have values between 0 and 1. in what comes next, we will refer to the implementation of the kernels arc-kernel and alg-kernel by smt arc-kernel and smt alg-kernel. we note also that the implementation of smt arc-kernel differs slightly from the original arc-kernel as we fixed some hyperparameters to 1 in order to avoid adding extra hy- perparameters and use the formulation of eq. (2) and rescaling of the data. 4.2.4. illustration on the mlp problem in this section, we illustrate the hierarchical arc-kernel on the mlp example. for that sake, we consider two design variables 𝑢 and 𝑣 such that 𝑢 = (2.10−4, 0.9, relu, 16, 2, 55, 51) and 𝑣 = (5.10−3, 0.8, sigmoid, 64, 3, 50, 54, 53). since the value of 𝑢met (i.e., the number of hidden layers) differs from one point to another (namely, 2 for 𝑢 and 3 for 𝑣), the associated variables 𝑢inc(𝑢met) have either 2 or 3 variables for the number of neurons in each layer (namely 55 and 51 for 𝑢, and 50, 54 and 53 for the point 𝑣). in our case, 8 hyperparame- ters ([𝑅1]1,2, 𝜃1, … , 𝜃7) will have to be optimized where 𝑘 is given by eq. (2). these 7 hyperparameters can be described using our proposed framework as follows: • for the neutral components, we have 𝑢neu = (2.10−4, 0.9, relu, 16) and 𝑣neu = (5.10−3, 0.8, sigmoid, 64). therefore, for a categorical matrix kernel 𝑅1 and a square exponential quantitative kernel, 𝑘neu(𝑢neu, 𝑣neu) = 𝑘cat(𝑢cat neu, 𝑣cat neu)𝑘qnt(𝑢qnt neu, 𝑣qnt neu) = [𝑅1]1,2 exp [−𝜃1(2.10−4 − 5.10−3)2] exp [−𝜃2(0.9 − 0.8)2] exp [−𝜃3(16 − 64)2]. the values [𝑅1]1,2, 𝜃1, 𝜃2 and 𝜃3 need to be optimized. here, [𝑅1]1,2 is the correlation between \"relu\" and \"sigmoid\". • for the meta components, we have 𝑢met = 2 and 𝑣met = 3. therefore, for a square exponential quantitative kernel, 𝑘met(𝑢met, 𝑣met) = 𝑘cat(𝑢cat met, 𝑣cat met)𝑘qnt(𝑢qnt met, 𝑣qnt met) the value 𝜃4 needs to be optimized.9 p. saves et al. • for the meta-decreed kernel, we have [𝑢met, 𝑢inc(𝑢met)] = [2, (55, 51)] and [𝑣met, 𝑣inc(𝑣met)] = [3, (50, 54, 53)] which gives 𝑘alg met,dec([𝑢met, 𝑢inc(𝑢met)], [𝑣met, 𝑣inc(𝑣met)]) = 𝑘alg met(2, 3) 𝑘alg dec((55, 51), (50, 54, 53)). the distance 𝑑alg(51, 54) = ( 2×|51−54| √ 512+1 √ 542+1 ) 𝜃6 = 2.178.10−3 𝜃6. in general, for surrogate models, and in particular in smt 2.0, the input data are normalized. with a unit normalization from [50, 55] to [0, 1], we would have 𝑑alg(0.2, 0.8) = ( 2×0.6 √ 0.22+1 √ 0.62+1 ) 𝜃6 = 0.919 𝜃6. similarly, we have, between 55 and 50, 𝑑alg(0, 1) = 1.414 𝜃5. hence, for a square exponential quantitative kernel, one gets 𝑘alg met,dec([𝑢met, 𝑢inc(𝑢met)], [𝑣met, 𝑣inc(𝑣met)]) = exp [−𝜃7] × exp [−1.414 𝜃5] × exp [−0.919 𝜃6], where the meta induced component is 𝑘alg met(𝑢met, 𝑣met) = exp [−𝜃7] because the decreed value 53 in 𝑣 has nothing to be compared with in 𝑢 as in eq. (7). the values 𝜃5, 𝜃6 and 𝜃7 need to be opti- mized which complete the description of the hyperparameters. we note that for the mlp problem, alg-kernel models use 10 hyperparameters whereas the arc-kernel would require 12 hyperparameters without the meta kernel (𝜃4) but with 3 extra decreed hyperparameters and the wedge-kernel would require 15 hyperparameters. for deep learning applications, a more complex perceptron with up to 10 hidden layers would require 17 hyperparameters with smt 2.0 models against 26 for arc-kernel and 36 for wedge-kernel. the next section illustrates the interest of our method to build a surrogate model for this neural network engineering problem. 4.3. a neural network test-case using smt 2.0 in this section, we apply our models to the hyperparameters opti- mization of a mlp problem aforementioned of fig. 4. within smt 2.0 an example illustrates this mlp problem. for the sake of showing the kriging surrogate abilities, we implemented a dummy function with no significance to replace the real black-box that would require training a whole neural network (nn) with big data. this function requires a number of variables that depends on the value of the meta variable, i.e the number of hidden layers. to simplify, we have chosen only 1, 2 or 3 hidden layers and therefore, we have 3 decreed variables but deep neural networks could also be investigated as our model can tackle a few dozen variables. a test case (test_hierarchical_variables_nn) shows that our model smt alg-kernel interpolates the data prop- erly, checks that the data dimension is correct and also asserts that the inactive decreed variables have no influence over the prediction. in fig. 5 we illustrate the usage of kriging surrogates with hierarchical and mixed variables based on the implementation of smt 2.0 for test_hierarchical_variables_nn. to compare the hierarchical models of smt 2.0 (smt alg-kernel and smt arc-kernel) with the state-of-the-art imputation method previously used on industrial application (imp-kernel) [24], we draw a 99 point lhs (33 points by meta level) as a training set and the validation set is a lhs of 3×1000 = 3000 points. for the imp-kernel, the decreed-excluded values are replaced by 52 because the mean value 52.5 is not an integer (by default, smt rounds to the floor value). for the three methods, the precision (computed with a root-mean- square error rmse criterion), the likelihood and the computational time are shown in table 6. for this problem, we can see that smt alg- kernel gives better performance than the imputation method or smt arc-kernel. also, as all methods use the same number of hyperpa- rameters, they have similar time performances. a direct application of table 6 results on the neural network model. hierarchical method prediction error (rmse) likelihood # of hyperparam. smt alg-kernel 3.7610 176.11 10 smt arc-kernel 4.9208 162.01 10 imp-kernel 4.5455 170.64 10 our modeling method is bayesian optimization to perform quickly the hyperparameter optimization of a neural network [63]. 5. bayesian optimization within smt 2.0 efficient global optimization (ego) is a sequential bayesian op- timization algorithm designed to find the optimum of a black-box function that may be expensive to evaluate [52]. ego starts by fitting a kriging model to an initial doe, and then uses an acquisition function to select the next point to evaluate. the most used acquisition function is the expected improvement. once a new point has been evaluated, the kriging model is updated. successive updates increase the model accu- racy over iterations. this enrichment process repeats until a stopping criterion is met. because smt 2.0 implements kriging models that handle mixed and hierarchical variables, we can use ego to solve problems in- volving such design variables. other bayesian optimization algorithms often adopt approaches based on solving subproblems with contin- uous or non-hierarchical kriging. this subproblem approach is less efficient and scales poorly, but it can only solve simple problems. several bayesian optimization software packages can handle mixed or hierarchical variables with such a subproblem approach. the pack- ages include botorch [25], smac [65], trieste [66], hebo [67], openbox [68], and dragonfly [69]. 5.1. a mixed optimization problem fig. 6 compares the four ego methods implemented in smt 2.0: smt gd, smt cr, smt ehh and smt hh. the mixed test case that illustrates bayesian optimization is a toy test case [64] detailed in ap- pendix a. this test case has two variables, one continuous and one categorical with 10 levels. to assess the performance of our algorithm, we performed 20 runs with different initial doe sampled by lhs. every doe consists of 5 points and we chose a budget of 55 infill points. fig. 6(a) plots the convergence curves for the four methods. in particular, the median is the solid line, and the first and third quantiles are plotted in dotted lines. to visualize better the data dispersion, the boxplots of the 20 best solutions after 20 evaluations are plotted in fig. 6(b). as expected, the more a method is complex, the better the optimization. both smt hh and smt ehh converged in around 18 evaluations whereas smt cr and smt gd take around 26 iterations as shown on fig. 6(a). also, the more complex the model, the closer the optimum is to the real value as shown on fig. 6(b). in fig. 7 we illustrate how to use ego with mixed variables based on the implementation of smt 2.0. the illustrated problem is a mixed variant of the branin function [70]. note that a dedicated notebook is available to reproduce the results 11 https://colab.research.google.com/github/smtorg/smt/blob/master/ tutorial/smt_mixedinteger_application.ipynb10 p. saves et al. fig. 5. example of usage of hierarchical and mixed kriging surrogate.11 p. saves et al. fig. 7. example of usage of mixed surrogates for bayesian optimization.12 p. saves et al. fig. 8. optimization results for the hierarchical goldstein function. 5.2. a hierarchical optimization problem the hierarchical test case considered in this paper to illustrate bayesian optimization is a modified goldstein function [60] detailed in appendix b. the resulting optimization problem involves 11 vari- ables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical. these variables consist in 6 neutral variables, 1 dimensional (or meta) variable and 4 decreed variables. depending on the meta variable values, 4 different sub-problems can be identified. the optimization problem is given by: min 𝑓(𝑥cat neu, 𝑥qnt neu, 𝑥cat 𝑚 , 𝑥qnt dec) w.r.t. 𝑥cat neu = 𝑤2 ∈ {0, 1} 𝑥qnt neu = (𝑥1, 𝑥2, 𝑥5, 𝑧3, 𝑧4) ∈ {0, 100}3 × {0, 1, 2}2 𝑥cat 𝑚 = 𝑤1 ∈ {0, 1, 2, 3} 𝑥qnt dec = (𝑥3, 𝑥4, 𝑧1, 𝑧2) ∈ {0, 100}2 × {0, 1, 2}2 (8) compared to the model choice of pelamatti et al. [60], we chose to model 𝑥5 and 𝑤2 as neutral variables even if 𝑓 does not depend on 𝑥5 when 𝑤2 = 0. other modeling choices are kept; for example, 𝑤2 is a so-called ‘‘binary variable’’ and not a categorical one [71]. similarly, we also keep the formulation of 𝑤1 as a categorical variable but a better model would be to model it as a ‘‘cyclic variable’’ [72]. the resulting problem is described in appendix b. to assess the performance of our algorithm, we performed 20 runs with different initial doe sampled by lhs. every doe consists of 𝑛 + 1 = 12 points and we chose a budget of 5𝑛 = 55 infill points. to compare our method with a baseline, we also tested the random search method thanks to the expand_lhs new method [40] described in section 6.1 and we also optimized the goldstein function using ego with a classic kriging model based on imputation method (imp-kernel). this method replaces the decreed- excluded variables by their mean values: 50 or 1 respectively for (𝑥3, 𝑥4) and (𝑧1, 𝑧2). fig. 8(a) plots the convergence curves for the four methods. in particular, the median is the solid line and the first and third quantiles are plotted in dotted lines. to visualize better the correspond- ing data dispersion, the boxplots of the 20 best solutions are plotted in fig. 8(b). the results in fig. 8 show that the hierarchical kriging models of smt 2.0 lead to better results than the imputation method or the random search both in terms of final objective value and variance over the 20 runs and in term of convergence rate. more precisely, smt arc-kernel and smt alg-kernel kriging model gave the best ego results and managed to converge correctly as shown in fig. 8(b). more precisely, the 20 sampled does led to similar performance and from one doe, the method smt alg-kernel managed to find the true minimum. however, this result has not been reproduced in other runs and is therefore not statistically significant. the variance between the runs is of similar magnitude regardless of the considered methods. 6. other relevant contributions in smt 2.0 the new release smt 2.0 introduces several improvements be- sides kriging for hierarchical and mixed variables. this section details the most important new contributions. recall from section 2.2 that five sub-modules are present in the code: sampling, problems, surrogate models, applications and notebooks. 6.1. contributions to sampling pseudo-random sampling. the latin hypercube sampling (lhs) is a stochastic sampling technique to generate quasi-random sampling dis- tributions. it is among the most popular sampling method in computer experiments thanks to its simplicity and projection properties with high-dimensional problems. the lhs method uses the pydoe package (design of experiments for python). five criteria for the construction of lhs are implemented in smt. the first four criteria (center, maximin, centermaximin, correlation) are the same as in pydoe.12 the last criterion ese, is implemented by the authors of smt [48]. in smt 2.0 a new lhs method was developed for the nested design of experiments (nestedlhs) [73] to use with multi- fidelity surrogates. a new mathematical method (expand_lhs) [40] was developed in smt 2.0 to increase the size of a design of exper- iments while maintaining the ese property. moreover, we proposed a sampling method for mixed variables, and the aforementioned lhs method was applied to hierarchical variables in fig. 8. 6.2. contributions to surrogate models new kernels and their derivatives for kriging. kriging surrogates are based on hyperparameters and on a correlation kernel. four correla- tion kernels are now implemented in smt 2.0 [74]. in smt, these correlation functions are absolute exponential (abs_exp), gaussian (squar_exp), matern 5/2 (matern52) and matern 3/2 (matern32). in addition, the implementation of gradient and hessian for each kernel makes it possible to calculate both the first and second derivatives of the gp likelihood with respect to the hyperparameters [5]. variance derivatives for kriging. to perform uncertainty quantification for system analysis purposes, it could be interesting to know more about the variance derivatives of a model [75–77]. for that purpose and also to pursue the original publication about derivatives [5], smt 2.0 extends the derivative support to kriging variances and kernels. 12 https://pythonhosted.org/pydoe/index.html13 p. saves et al. noisy kriging. in engineering and in big data contexts with real exper- iments, surrogate models for noisy data are of significant interest. in particular, there is a growing need for techniques like noisy kriging and noisy multi-fidelity kriging (mfk) for data fusion [78]. for that purpose, smt 2.0 has been designed to accommodate kriging and mfk to noisy data including the option to incorporate heteroscedastic noise (using the use_het_noise option) and to account for different noise levels for each data source [40]. kriging with partial least squares. beside mgp, for high-dimensional problems, the toolbox implements kriging with partial least squares (kpls) [57] and its extension kplsk [44]. the pls information is computed by projecting the data into a smaller space spanned by the principal components. by integrating this pls information into the kriging correlation matrix, the number of inputs can be scaled down, thereby reducing the number of hyperparameters required. the result- ing number of hyperparameters 𝑑𝑒 is indeed much smaller than the original problem dimension 𝑑. recently, in smt 2.0, we extended the kpls method for multi-fidelity kriging (mfkpls and mfkplsk) [73,79, 80]. we also proposed an automatic criterion to choose automatically the reduced dimension 𝑑𝑒 based on wold’s r criterion [81]. this criterion has been applied to aircraft optimization with ego where the number 𝑑𝑒 (𝚗_𝚌𝚘𝚖𝚙 in the code) for the model is automatically selected at every iteration [39]. special efforts have been made to accommodate kpls for multi-fidelity and mixed integer data [79,80]. marginal gaussian process. smt 2.0 implements marginal gaussian process (mgp) surrogate models for high dimensional problems [82]. mgp are gaussian processes taking into account hyperparameters un- certainty defined as a density probability function. especially we sup- pose that the function to model 𝑓 ∶ 𝛺 ↦ r, where 𝛺 ⊂ r𝑑 and 𝑑 is the number of design variables, lies in a linear embedding  such as  = {𝑢 = 𝐴𝑥, 𝑥 ∈ 𝛺}, 𝐴 ∈ r𝑑×𝑑𝑒 and 𝑓(𝑥) = 𝑓(𝐴𝑥) with 𝑓(𝑥) = 𝑓 ∶  ↦ r and 𝑑𝑒 ≪ 𝑑. then, we must use a kernel 𝑘(𝑥, 𝑥′) = 𝑘(𝐴𝑥, 𝐴𝑥′) whose each component of the transfer matrix 𝐴 is an hyperparameter. thus we have 𝑑𝑒 × 𝑑 hyperparameters to find. note that 𝑑𝑒 is defined as 𝚗_𝚌𝚘𝚖𝚙 in the code [49]. gradient-enhanced neural network. the new release smt 2.0 imple- ments gradient-enhanced neural network (genn) models [45]. gradient-enhanced neural networks (genn) are fully connected multi- layer perceptrons whose training process was modified to account for gradient information. specifically, the model is trained to minimize not only the prediction error of the response but also the prediction error of the partial derivatives: the chief benefit of gradient enhancement is better accuracy with fewer training points. note that genn applies to regression (single-output or multi-output), but not classification since there is no gradient in that case. the implementation is fully vectorized and uses adam optimization, mini-batch, and l2-norm regularization. for example, genn can be used to learn airfoil geometries from a database. this usage is documented in smt 2.0.13 6.3. contributions to applications kriging trajectory and sampling. sampling a gp with high resolution is usually expensive due to the large dimension of the associated covariance matrix. several methods are proposed to draw samples of a gp on a given set of points. to sample a conditioned gp, the classic method consists in using a cholesky decomposition (or eigende- composition) of the conditioned covariance matrix of the process but some numerical computational errors can lead to non spd matrix. a more recent approach based on karhunen–loève decomposition of the covariance kernel with the nyström method has been proposed in [83] 13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_ parameters/learning_airfoil_parameters.html where the paths can be sampled by generating independent standard normal distributed samples. the different methods are documented in the tutorial gaussian process trajectory sampling [84]. parallel bayesian optimization. due to the recent progress made in hardware configurations, it has been of high interest to perform parallel optimizations. a parallel criterion called qei [85] was developed to perform efficient global optimization (ego): the goal is to be able to run batch optimization. at each iteration of the algorithm, multiple new sampling points are extracted from the known ones. these new sampling points are then evaluated using a parallel computing environ- ment. five criteria are implemented in smt 2.0: kriging believer (kb), kriging believer upper bound (kbub), kriging believer lower bound (kblb), kriging believer random bound (kbrand) and constant liar (clmin) [86]. 7. conclusion smt 2.0 introduces significant upgrades to the surrogate modeling toolbox. this new release adds support for hierarchical and mixed variables and improves the surrogate models with a particular focus on kriging (gaussian process) models. smt 2.0 is distributed through an open-source license and is freely available online.14 we provide documentation that caters to both users and potential developers.15 smt 2.0 enables users and developers collaborating on the same project to have a common surrogate modeling tool that facilitates the exchange of methods and reproducibility of results. smt has been widely used in aerospace and mechanical modeling applications. moreover, the toolbox is general and can be useful for anyone who needs to use or develop surrogate modeling techniques, regardless of the targeted applications. smt is currently the only open- source toolbox that can build hierarchical and mixed surrogate models. declaration of competing interest the authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper. data availability data will be made available on request. results can be reproduced freely online at https://colab.research.google.com/github/smtorg/smt/ blob/master/tutorial/notebookruntestcases_paper_smt_v2.ipynb. acknowledgments we want to thank all those who contribute to this release. namely, m. a. bouhlel, i. cardoso, r. carreira rufato, r. charayron, r. conde arenzana, s. dubreuil, a. f. lópez-lopera, m. meliani, m. menz, n. moëllo, a. thouvenot, r. priem, e. roux and f. vergnes. this work is part of the activities of onera - isae - enac joint research group. we also acknowledge the partners institutions: onera, nasa glenn, isae- supaero, institut clément ader (ica), the university of michigan, polytechnique montréal and the university of california san diego. the research presented in this paper has been performed in the framework of the agile 4.0 project (towards cyber-physical collabo- rative aircraft development), funded by the european union horizon 2020 research and innovation framework programme under grant agreement n◦ 815122 and in the colossus project (collaborative system of systems exploration of aviation products, services and 14 https://github.com/smtorg/smt 15 https://smt.readthedocs.io/en/latest/14 p. saves et al. business models) funded by the european union horizon europe re- search and innovation framework programme under grant agreement n◦ 101097120. we also are grateful to e. hallé-hannan from polytechnique mon- tréal for the hierarchical variables framework. appendix a. toy test function this appendix gives the detail of the toy function of section 5.1.16 first, we recall the optimization problem: min 𝑓(𝑥cat, 𝑥qnt) w.r.t. 𝑥cat = 𝑐1 ∈ {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} 𝑥qnt = 𝑥1 ∈ [0, 1] (a.1) the toy function 𝑓 is defined as 𝑓(𝑥, 𝑐1) =1𝑐1=0 cos(3.6𝜋(𝑥 − 2)) + 𝑥 − 1 +1𝑐1=1 2 cos(1.1𝜋 exp(𝑥)) − 𝑥 2 + 2 +1𝑐1=2 cos(2𝜋𝑥) + 1 2𝑥 +1𝑐1=3 𝑥(cos(3.4𝜋(𝑥 − 1)) − 𝑥 − 1 2 ) +1𝑐1=4 − 𝑥2 2 +1𝑐1=5 2 cos(0.25𝜋 exp(−𝑥4))2 − 𝑥 2 + 1 +1𝑐1=6 𝑥 cos(3.4𝜋𝑥) − 𝑥 2 + 1 +1𝑐1=7 − 𝑥(cos(3.5𝜋𝑥) + 𝑥 2 ) + 2 +1𝑐1=8 − 𝑥5 2 + 1 +1𝑐1=9 − cos(2.5𝜋𝑥)2√ 𝑥 − 0.5 ln(𝑥 + 0.5) − 1.3 (a.2) appendix b. hierarchical goldstein test function this appendix gives the detail of the hierarchical goldstein problem of section 5.2.17 first, we recall the optimization problem: min 𝑓(𝑥cat neu, 𝑥qnt neu, 𝑥cat 𝑚 , 𝑥qnt dec) w.r.t. 𝑥cat neu = 𝑤2 ∈ {0, 1} 𝑥qnt neu = (𝑥1, 𝑥2, 𝑥5, 𝑧3, 𝑧4) ∈ [0, 100]3 × {0, 1, 2}2 𝑥cat 𝑚 = 𝑤1 ∈ {0, 1, 2, 3} 𝑥qnt dec = (𝑥3, 𝑥4, 𝑧1, 𝑧2) ∈ [0, 100]2 × {0, 1, 2}2 (b.1) the hierarchical and mixed function 𝑓 is defined as a hierarchical function that depends on 𝑓0, 𝑓1, 𝑓2 and 𝐺𝑜𝑙𝑑cont as describes in the following. 𝑓(𝑥1, 𝑥2, 𝑥3, 𝑥4, 𝑧1, 𝑧2, 𝑧3, 𝑧4, 𝑥5, 𝑤1, 𝑤2) = 1𝑤1=0𝑓0(𝑥1, 𝑥2, 𝑧1, 𝑧2, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑤1=1𝑓1(𝑥1, 𝑥2, 𝑥3, 𝑧2, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑤1=2𝑓2(𝑥1, 𝑥2, 𝑥4, 𝑧1, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑤1=3𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 𝑥3, 𝑥4, 𝑧3, 𝑧4, 𝑥5, 𝑤2). (b.2) 16 https://github.com/jbussemaker/sbarchopt 17 https://github.com/jbussemaker/sbarchopt then, the functions 𝑓0, 𝑓1 and 𝑓2 are defined as mixed variants of 𝐺𝑜𝑙𝑑cont as such 𝑓0(𝑥1, 𝑥2, 𝑧1, 𝑧2, 𝑧3, 𝑧4, 𝑥5, 𝑤2) = 1𝑧2=0 ( 1𝑧1=0𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 20, 20, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=1𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 50, 20, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=2𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 80, 20, 𝑧3, 𝑧4, 𝑥5, 𝑤2) ) 1𝑧2=1 ( 1𝑧1=0𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 20, 50, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=1𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 50, 50, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=2𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 80, 50, 𝑧3, 𝑧4, 𝑥5, 𝑤2) ) 1𝑧2=2 ( 1𝑧1=0𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 20, 80, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=1𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 50, 80, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=2𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 80, 80, 𝑧3, 𝑧4, 𝑥5, 𝑤2) ) (b.3) 𝑓1(𝑥1, 𝑥2, 𝑥3, 𝑧2, 𝑧3, 𝑧4, 𝑥5, 𝑤2) = 1𝑧2=0𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 𝑥3, 20, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧2=1𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 𝑥3, 50, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧2=2𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 𝑥3, 80, 𝑧3, 𝑧4, 𝑥5, 𝑤2) 𝑓2(𝑥1, 𝑥2, 𝑥4, 𝑧1, 𝑧3, 𝑧4, 𝑥5, 𝑤2) = 1𝑧1=0𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 20, 𝑥4, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=1𝐺𝑜𝑙𝑑cont(𝑥1, 50, 𝑥2, 𝑥4, 𝑧3, 𝑧4, 𝑥5, 𝑤2) + 1𝑧1=2𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 80, 𝑥4, 𝑧3, 𝑧4, 𝑥5, 𝑤2) to finish with, the function 𝐺𝑜𝑙𝑑cont is given by 𝐺𝑜𝑙𝑑cont(𝑥1, 𝑥2, 𝑥3, 𝑥4, 𝑧3, 𝑧4, 𝑥5, 𝑤2) = 53.3108 + 0.184901𝑥1 − 5.02914𝑥1 3.10−6 + 7.72522𝑥1 𝑧3.10−8 − 0.0870775𝑥2 − 0.106959𝑥3 + 7.98772𝑥3 𝑧4.10−6 + 0.00242482𝑥4 + 1.32851𝑥4 3.10−6 − 0.00146393𝑥1𝑥2 − 0.00301588𝑥1𝑥3 − 0.00272291𝑥1𝑥4 + 0.0017004𝑥2𝑥3 + 0.0038428𝑥2𝑥4 − 0.000198969𝑥3𝑥4 + 1.86025𝑥1𝑥2𝑥3.10−5 − 1.88719𝑥1𝑥2𝑥4.10−6 + 2.50923𝑥1𝑥3𝑥4.10−5 − 5.62199𝑥2𝑥3𝑥4.10−5 + 𝑤2 ( 5 cos ( 2𝜋 100 𝑥5 ) − 2 ) . (b.4) appendix c. supplementary data more at https://colab.research.google.com/github/smtorg/smt/blob/ master/tutorial/notebookruntestcases_paper_smt_v2.ipynb. supplementary material related to this article can be found online at https://doi.org/10.1016/j.advengsoft.2023.103571. references [1] mader ca, martins jrra, alonso jj, van der weide e. adjoint: an approach for the rapid development of discrete adjoint solvers. aiaa j 2008;46:863–73. [2] kennedy m, o’hagan a. bayesian calibration of computer models. j r stat soc ser b stat methodol 2001;63:425–64. [3] hwang jt, martins jrra. a fast-prediction surrogate model for large datasets. aerosp sci technol 2018;75:74–87. [4] martins jrra, ning a. engineering design optimization. cambridge university press; 2021. [5] bouhlel ma, hwang jt, bartoli n, lafage r, morlier j, martins jra. a python surrogate modeling framework with derivatives. adv eng softw 2019;135:102662. [6] bouhlel ma, martins j. gradient-enhanced kriging for high-dimensional problems. eng comput 2019;35:157–73. machine learning in python. j mach learn res 2011;12:2825–30.15 p. saves et al. [8] lataniotis c, marelli s, sudret b. uqlab 2.0 and uqcloud: open-source vs. cloud-based uncertainty quantification. in: siam conference on uncertainty quantification. 2022. [9] faraci a, beaurepaire p, gayton n. review on python toolboxes for kriging surrogate modelling. in: esrel. 2022. [10] krügener m, zapata usandivaras j, bauerheim m, urbano a. coaxial-injector surrogate modeling based on reynolds-averaged navier–stokes simulations using deep learning. j propuls power 2022;38:783–98. [11] ming d, williamson d, guillas s. deep gaussian process emulation using stochastic imputation. technometrics 2022;1–12. [12] eliáš j, vořechovsk`y m, sadílekv v. periodic version of the minimax distance criterion for monte carlo integration. adv eng softw 2020;149:102900. [13] drouet v, balesdent m, brevault l, dubreuil s, morio j. multi-fidelity algo- rithm for the sensitivity analysis of multidisciplinary problems. j mech des 2023;145:1–22. [14] karban p, pánek d, orosz t, petrášová i, doležel i. fem based robust design optimization with agros and ¯artap. comput math appl 2021;81:618–33. [15] kudela j, matousek r. recent advances and applications of surrogate models for finite element method computations: a review. soft comput 2022;26:13709–33. [16] chen y, dababneh f, zhang b, kassaee s, smith bt, liu k, et al. surrogate mod- eling for capacity planning of charging station equipped with photovoltaic panel and hydropneumatic energy storage. j energy res technol 2020;142:050907. [17] jasa j, bortolotti p, zalkind d, barter g. effectively using multifidelity optimization for wind turbine design. wind energy sci 2022;7:991–1006. [18] wang w, tao g, ke d, luo j, cui j. transpiration cooling of high pres- sure turbine vane with optimized porosity distribution. appl therm eng 2023;223:119831. [19] savage t, almeida-trasvina hf, del río-chanona ea, smith r, zhang d. an adaptive data-driven modelling and optimization framework for complex chemical process design. comput aided chem eng 2020;48:73–8. [20] chan a, pires af, polacsek t. trying to elicit and assign goals to the right actors. in: conceptual modeling: 41st international conference. 2022. [21] hutter f, osborne ma. a kernel for hierarchical parameter spaces. 2013, arxiv. [22] bussemaker jh, ciampa pd, nagel b. system architecture design space explo- ration: an approach to modeling and optimization. in: aiaa aviation 2020 forum. 2020. [23] fouda mea, adler ej, bussemaker j, martins jrra, kurtulus df, boggero l, et al. automated hybrid propulsion model construction for conceptual aircraft design and optimization. in: 33rd congress of the international council of the aeronautical sciences. 2022. [24] bussemaker jh, bartoli n, lefebvre t, ciampa pd, nagel b. effectiveness of surrogate-based optimization algorithms for system architecture optimization. in: aiaa aviation 2021 forum. 2021. [25] balandat m, karrer b, jiang d, daulton s, letham b, wilson a, et al. botorch: a framework for efficient monte-carlo bayesian optimization. adv neural inf process syst 2020;33:21524–38. [26] adams b, bohnhoff w, dalbey k, ebeida m, eddy j, eldred m, et al. dakota, a multilevel parallel object-oriented framework for design optimization, pa- rameter estimation, uncertainty quantification, and sensitivity analysis: version 6.13 user’s manual. technical report, albuquerque, nm (united states: sandia national lab.(snl-nm); 2020. [27] roustant o, ginsbourger d, deville y. dicekriging, diceoptim: two r packages for the analysis of computer experiments by kriging-based metamodeling and optimization. j stat softw 2012;51:1–55. [28] zhang y, tao s, chen w, apley d. a latent variable approach to gaus- sian process modeling with qualitative and quantitative factors. technometrics 2020;62:291–302. [29] chang th, wild sm. parmoo: a python library for parallel multiobjective simulation optimization. j open source softw 2023;8:4468. [30] garrido-merchán ec, hernández-lobato d. dealing with categorical and integer-valued variables in bayesian optimization with gaussian processes. neurocomputing 2020;380:20–35. [31] halstrup m. black-box optimization of mixed discrete-continuous optimization problems (ph.d. thesis), tu dortmund; 2016. [32] roustant o, padonou e, deville y, clément a, perrin g, giorla j, et al. group kernels for gaussian process metamodels with categorical inputs. siam j uncertain quant 2020;8:775–806. [33] zhou q, qian pzg, zhou s. a simple approach to emulation for computer models with qualitative and quantitative factors. technometrics 2011;53:266–73. [34] saves p, diouane y, bartoli n, lefebvre t, morlier j. a mixed-categorical correlation kernel for gaussian process. neurocomputing 2023;550:126472. [35] pelamatti j, brevault l, balesdent m, talbi e-g, guerin y. efficient global optimization of constrained mixed variable problems. j global optim 2019;73:583–613. [36] horn d, stork j, ler n-js, zaefferer m. surrogates for hierarchical search spaces: the wedge-kernel and an automated analysis. in: proceedings of the genetic and evolutionary computation conference. 2019. [37] hung y, joseph vr, melkote sn. design and analysis of computer experiments with branching and nested factors. technometrics 2009;51:354–65. [38] audet c, hallé-hannan e, le digabel s. a general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables. oper res forum 2023;4:1–37. [39] saves p, nguyen van e, bartoli n, diouane y, lefebvre t, david c, defoort s, morlier j. bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design. in: aiaa scitech 2022. 2022. [40] conde arenzana r, lópez-lopera a, mouton s, bartoli n, lefebvre t. multi- fidelity gaussian process model for cfd and wind tunnel data fusion. in: eccomas aerobest. 2021. [41] rufato rc, diouane y, henry j, ahlfeld r, morlier j. a mixed-categorical data-driven approach for prediction and optimization of hybrid discontinuous composites performance. in: aiaa aviation 2022 forum. 2022. [42] gorissen d, crombecq k, couckuyt i, dhaene t, demeester p. a surrogate modeling and adaptive sampling toolbox for computer based design. j mach learn res 2010;11:2051–5. [43] williams ck, rasmussen ce. gaussian processes for machine learning. ma: mit press cambridge; 2006. [44] bouhlel ma, bartoli n, regis r, otsmane a, morlier j. efficient global opti- mization for high-dimensional constrained problems by using the kriging models combined with the partial least squares method. eng optim 2018;50:2038–53. [45] bouhlel ma, he s, martins j. scalable gradient-enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes. struct multidiscip optim 2020;61:1363–76. [46] kwan ls, pitrou a, seibert s. numba: a llvm-based python jit compiler. in: proceedings of the second workshop on the llvm compiler infrastructure in hpc. 2015. [47] zaefferer m, horn d. a first analysis of kernels for kriging-based optimization in hierarchical search spaces. 2018, arxiv. [48] jin r, chen w, sudjianto a. an efficient algorithm for constructing optimal design of computer experiments. j statist plann inference 2005;2:545–54. [49] garnett r, osborne m, hennig p. active learning of linear embeddings for gaussian processes. in: uncertainty in artificial intelligence - proceedings of the 30th conference. 2013. [50] jones d. a taxonomy of global optimization methods based on response surfaces. j global optim 2001;21:345–83. [51] lafage r. egobox, a rust toolbox for efficient global optimization. j open source softw 2022;7:4737. [52] jones dr, schonlau m, welch wj. efficient global optimization of expensive black-box functions. j global optim 1998;13:455–92. [53] deng x, lin cd, liu k, rowe rk. additive gaussian process for computer models with qualitative and quantitative factors. technometrics 2017;59:283–92. [54] cuesta-ramirez j, le riche r, roustant o, perrin g, durantin c, gliere a. a comparison of mixed-variables bayesian optimization approaches. adv model simul eng sci 2021;9:1–29. [55] rebonato r, jaeckel p. the most general methodology to create a valid correlation matrix for risk management and option pricing purposes. j risk 2001;2:17–27. [56] rapisarda f, brigo d, mercurio f. parameterizing correlations: a geometric interpretation. ima j manag math 2007;18:55–73. [57] bouhlel ma, bartoli n, regis r, otsmane a, morlier j. an improved approach for estimating the hyperparameters of the kriging model for high- dimensional problems through the partial least squares method. math probl eng 2016;2016:6723410. [58] cheng gh, younis a, hajikolaei kh, wang gg. trust region based mode pursuing sampling method for global optimization of high dimensional design problems. j mech des 2015;137:021407. [59] karlsson r, bliek l, verwer s, de weerdt m. continuous surrogate-based optimization algorithms are well-suited for expensive discrete problems. in: artificial intelligence and machine learning. 2021. [60] pelamatti j, brevault l, balesdent m, talbi e-g, guerin y. bayesian optimization of variable-size design space problems. opt eng 2021;22:387–447. [61] hebbal a, brevault l, balesdent m, talbi e-g, melab n. bayesian optimization using deep gaussian processes with applications to aerospace system design. opt eng 2021;22:321–61. [62] wildberger n. a rational approach to trigonometry. math horiz 2007;15:16–20. [63] cho h, kim y, lee e, choi d, lee y, rhee w. basic enhancement strategies when using bayesian optimization for hyperparameter tuning of deep neural networks. ieee access 2020;8:52588–608. [64] zuniga mm, sinoquet d. global optimization for mixed categorical-continuous variables based on gaussian process models with a randomized categorical space exploration step. infor inf syst oper res 2020;58:310–41. [65] lindauer m, eggensperger k, feurer m, ab, deng d, benjamins c, et al. smac3: a versatile bayesian optimization package for hyperparameter optimization. j mach learn res 2022;23:1–9. [66] picheny v, berkeley j, moss h, stojic h, granta u, ober s, et al. trieste: efficiently exploring the depths of black-box functions with tensorflow. 2023, arxiv. [67] cowen-rivers ai, ly w, wang z, tutunov r, jianye h, wang j, et al. hebo: heteroscedastic evolutionary bayesian optimisation. 2020, arxiv. generalized black-box optimization. 2023, arxiv.16 p. saves et al. [69] kandasamy k, vysyaraju kr, neiswanger w, paria b, collins c, schneider j, et al. tuning hyperparameters without grad students: scalable and robust bayesian optimisation with dragonfly. j mach learn res 2020;21:3098–124. [70] roy s, crossley wa, stanford bk, moore kt, gray js. a mixed integer efficient global optimization algorithm with multiple infill strategy - applied to a wing topology optimization problem. in: aiaa scitech 2019 forum. 2019. [71] müller j, shoemaker ca, piché r. so-mi: a surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems. comput oper res 2013;40:1383–400. [72] tran t, sinoquet d, da veiga s, mongeau m. derivative-free mixed binary necklace optimization for cyclic-symmetry optimal design problems. opt eng 2021. [73] meliani m, bartoli n, lefebvre t, bouhlel ma, martins jrra, morlier j. multi- fidelity efficient global optimization: methodology and application to airfoil shape design. in: aiaa aviation 2019 forum. 2019. [74] lee h. gaussian processes. springer berlin heidelberg; 2011, p. 575–7. [75] lópez-lopera af, idier d, rohmer j, bachoc f. multioutput gaussian processes with functional data: a study on coastal flood hazard assessment. reliab eng syst saf 2022;218:108139. [76] berthelin g, dubreuil s, salaün m, bartoli n, gogu c. disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis. internat j numer methods engrg 2022;123:3594–626. [77] cardoso i, dubreuil s, bartoli n, gogu c, salaün m, lafage r. disciplinary surrogates for gradient-based optimization of multidisciplinary systems. in: eccomas aerobest. 2023. [78] platt j, penny s, smith t, chen t, abarbanel h. a systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics. neural netw 2022;153:530–52. [79] charayron r, lefebvre t, bartoli n, morlier j. multi-fidelity bayesian optimiza- tion strategy applied to overall drone design. in: aiaa scitech 2023 forum. 2023. [80] charayron r, lefebvre t, bartoli n, morlier j. towards a multi-fidelity and multi-objective bayesian optimization efficient algorithm. aerosp sci technol 2023;142:108673. [81] wold h. soft modelling by latent variables: the non-linear iterative partial least squares (nipals) approach. j appl probab 1975;12:117–42. [82] priem r, diouane y, bartoli n, dubreuil s, saves p. high-dimensional efficient global optimization using both random and supervised embeddings. in: aiaa aviation 2023 forum. 2023. [83] betz w, papaioannou i, straub d. numerical methods for the discretization of random fields by means of the karhunen–loève expansion. comput methods appl mech engrg 2014;271:109–29. [84] menz m, dubreuil s, morio j, gogu c, bartoli n, chiron m. variance based sen- sitivity analysis for monte carlo and importance sampling reliability assessment with gaussian processes. struct saf 2021;93:102116. [85] ginsbourger d, le riche r, carraro l. kriging is well-suited to parallelize optimization. springer berlin heidelberg; 2010, p. 131–62. mization strategy based on kriging properties suitable for material parameters identification. arch mech eng 2020;67.",
                    "content": null,
                    "references": "[1] mader ca, martins jrra, alonso jj, van der weide e. adjoint: an approach for the rapid development of discrete adjoint solvers. aiaa j 2008;46:863–73. [2] kennedy m, o’hagan a. bayesian calibration of computer models. j r stat soc ser b stat methodol 2001;63:425–64. [3] hwang jt, martins jrra. a fast-prediction surrogate model for large datasets. aerosp sci technol 2018;75:74–87. [4] martins jrra, ning a. engineering design optimization. cambridge university press; 2021. [5] bouhlel ma, hwang jt, bartoli n, lafage r, morlier j, martins jra. a python surrogate modeling framework with derivatives. adv eng softw 2019;135:102662. [6] bouhlel ma, martins j. gradient-enhanced kriging for high-dimensional problems. eng comput 2019;35:157–73. machine learning in python. j mach learn res 2011;12:2825–30.15 p. saves et al. [8] lataniotis c, marelli s, sudret b. uqlab 2.0 and uqcloud: open-source vs. cloud-based uncertainty quantification. in: siam conference on uncertainty quantification. 2022. [9] faraci a, beaurepaire p, gayton n. review on python toolboxes for kriging surrogate modelling. in: esrel. 2022. [10] krügener m, zapata usandivaras j, bauerheim m, urbano a. coaxial-injector surrogate modeling based on reynolds-averaged navier–stokes simulations using deep learning. j propuls power 2022;38:783–98. [11] ming d, williamson d, guillas s. deep gaussian process emulation using stochastic imputation. technometrics 2022;1–12. [12] eliáš j, vořechovsk`y m, sadílekv v. periodic version of the minimax distance criterion for monte carlo integration. adv eng softw 2020;149:102900. [13] drouet v, balesdent m, brevault l, dubreuil s, morio j. multi-fidelity algo- rithm for the sensitivity analysis of multidisciplinary problems. j mech des 2023;145:1–22. [14] karban p, pánek d, orosz t, petrášová i, doležel i. fem based robust design optimization with agros and ¯artap. comput math appl 2021;81:618–33. [15] kudela j, matousek r. recent advances and applications of surrogate models for finite element method computations: a review. soft comput 2022;26:13709–33. [16] chen y, dababneh f, zhang b, kassaee s, smith bt, liu k, et al. surrogate mod- eling for capacity planning of charging station equipped with photovoltaic panel and hydropneumatic energy storage. j energy res technol 2020;142:050907. [17] jasa j, bortolotti p, zalkind d, barter g. effectively using multifidelity optimization for wind turbine design. wind energy sci 2022;7:991–1006. [18] wang w, tao g, ke d, luo j, cui j. transpiration cooling of high pres- sure turbine vane with optimized porosity distribution. appl therm eng 2023;223:119831. [19] savage t, almeida-trasvina hf, del río-chanona ea, smith r, zhang d. an adaptive data-driven modelling and optimization framework for complex chemical process design. comput aided chem eng 2020;48:73–8. [20] chan a, pires af, polacsek t. trying to elicit and assign goals to the right actors. in: conceptual modeling: 41st international conference. 2022. [21] hutter f, osborne ma. a kernel for hierarchical parameter spaces. 2013, arxiv. [22] bussemaker jh, ciampa pd, nagel b. system architecture design space explo- ration: an approach to modeling and optimization. in: aiaa aviation 2020 forum. 2020. [23] fouda mea, adler ej, bussemaker j, martins jrra, kurtulus df, boggero l, et al. automated hybrid propulsion model construction for conceptual aircraft design and optimization. in: 33rd congress of the international council of the aeronautical sciences. 2022. [24] bussemaker jh, bartoli n, lefebvre t, ciampa pd, nagel b. effectiveness of surrogate-based optimization algorithms for system architecture optimization. in: aiaa aviation 2021 forum. 2021. [25] balandat m, karrer b, jiang d, daulton s, letham b, wilson a, et al. botorch: a framework for efficient monte-carlo bayesian optimization. adv neural inf process syst 2020;33:21524–38. [26] adams b, bohnhoff w, dalbey k, ebeida m, eddy j, eldred m, et al. dakota, a multilevel parallel object-oriented framework for design optimization, pa- rameter estimation, uncertainty quantification, and sensitivity analysis: version 6.13 user’s manual. technical report, albuquerque, nm (united states: sandia national lab.(snl-nm); 2020. [27] roustant o, ginsbourger d, deville y. dicekriging, diceoptim: two r packages for the analysis of computer experiments by kriging-based metamodeling and optimization. j stat softw 2012;51:1–55. [28] zhang y, tao s, chen w, apley d. a latent variable approach to gaus- sian process modeling with qualitative and quantitative factors. technometrics 2020;62:291–302. [29] chang th, wild sm. parmoo: a python library for parallel multiobjective simulation optimization. j open source softw 2023;8:4468. [30] garrido-merchán ec, hernández-lobato d. dealing with categorical and integer-valued variables in bayesian optimization with gaussian processes. neurocomputing 2020;380:20–35. [31] halstrup m. black-box optimization of mixed discrete-continuous optimization problems (ph.d. thesis), tu dortmund; 2016. [32] roustant o, padonou e, deville y, clément a, perrin g, giorla j, et al. group kernels for gaussian process metamodels with categorical inputs. siam j uncertain quant 2020;8:775–806. [33] zhou q, qian pzg, zhou s. a simple approach to emulation for computer models with qualitative and quantitative factors. technometrics 2011;53:266–73. [34] saves p, diouane y, bartoli n, lefebvre t, morlier j. a mixed-categorical correlation kernel for gaussian process. neurocomputing 2023;550:126472. [35] pelamatti j, brevault l, balesdent m, talbi e-g, guerin y. efficient global optimization of constrained mixed variable problems. j global optim 2019;73:583–613. [36] horn d, stork j, ler n-js, zaefferer m. surrogates for hierarchical search spaces: the wedge-kernel and an automated analysis. in: proceedings of the genetic and evolutionary computation conference. 2019. [37] hung y, joseph vr, melkote sn. design and analysis of computer experiments with branching and nested factors. technometrics 2009;51:354–65. [38] audet c, hallé-hannan e, le digabel s. a general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables. oper res forum 2023;4:1–37. [39] saves p, nguyen van e, bartoli n, diouane y, lefebvre t, david c, defoort s, morlier j. bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design. in: aiaa scitech 2022. 2022. [40] conde arenzana r, lópez-lopera a, mouton s, bartoli n, lefebvre t. multi- fidelity gaussian process model for cfd and wind tunnel data fusion. in: eccomas aerobest. 2021. [41] rufato rc, diouane y, henry j, ahlfeld r, morlier j. a mixed-categorical data-driven approach for prediction and optimization of hybrid discontinuous composites performance. in: aiaa aviation 2022 forum. 2022. [42] gorissen d, crombecq k, couckuyt i, dhaene t, demeester p. a surrogate modeling and adaptive sampling toolbox for computer based design. j mach learn res 2010;11:2051–5. [43] williams ck, rasmussen ce. gaussian processes for machine learning. ma: mit press cambridge; 2006. [44] bouhlel ma, bartoli n, regis r, otsmane a, morlier j. efficient global opti- mization for high-dimensional constrained problems by using the kriging models combined with the partial least squares method. eng optim 2018;50:2038–53. [45] bouhlel ma, he s, martins j. scalable gradient-enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes. struct multidiscip optim 2020;61:1363–76. [46] kwan ls, pitrou a, seibert s. numba: a llvm-based python jit compiler. in: proceedings of the second workshop on the llvm compiler infrastructure in hpc. 2015. [47] zaefferer m, horn d. a first analysis of kernels for kriging-based optimization in hierarchical search spaces. 2018, arxiv. [48] jin r, chen w, sudjianto a. an efficient algorithm for constructing optimal design of computer experiments. j statist plann inference 2005;2:545–54. [49] garnett r, osborne m, hennig p. active learning of linear embeddings for gaussian processes. in: uncertainty in artificial intelligence - proceedings of the 30th conference. 2013. [50] jones d. a taxonomy of global optimization methods based on response surfaces. j global optim 2001;21:345–83. [51] lafage r. egobox, a rust toolbox for efficient global optimization. j open source softw 2022;7:4737. [52] jones dr, schonlau m, welch wj. efficient global optimization of expensive black-box functions. j global optim 1998;13:455–92. [53] deng x, lin cd, liu k, rowe rk. additive gaussian process for computer models with qualitative and quantitative factors. technometrics 2017;59:283–92. [54] cuesta-ramirez j, le riche r, roustant o, perrin g, durantin c, gliere a. a comparison of mixed-variables bayesian optimization approaches. adv model simul eng sci 2021;9:1–29. [55] rebonato r, jaeckel p. the most general methodology to create a valid correlation matrix for risk management and option pricing purposes. j risk 2001;2:17–27. [56] rapisarda f, brigo d, mercurio f. parameterizing correlations: a geometric interpretation. ima j manag math 2007;18:55–73. [57] bouhlel ma, bartoli n, regis r, otsmane a, morlier j. an improved approach for estimating the hyperparameters of the kriging model for high- dimensional problems through the partial least squares method. math probl eng 2016;2016:6723410. [58] cheng gh, younis a, hajikolaei kh, wang gg. trust region based mode pursuing sampling method for global optimization of high dimensional design problems. j mech des 2015;137:021407. [59] karlsson r, bliek l, verwer s, de weerdt m. continuous surrogate-based optimization algorithms are well-suited for expensive discrete problems. in: artificial intelligence and machine learning. 2021. [60] pelamatti j, brevault l, balesdent m, talbi e-g, guerin y. bayesian optimization of variable-size design space problems. opt eng 2021;22:387–447. [61] hebbal a, brevault l, balesdent m, talbi e-g, melab n. bayesian optimization using deep gaussian processes with applications to aerospace system design. opt eng 2021;22:321–61. [62] wildberger n. a rational approach to trigonometry. math horiz 2007;15:16–20. [63] cho h, kim y, lee e, choi d, lee y, rhee w. basic enhancement strategies when using bayesian optimization for hyperparameter tuning of deep neural networks. ieee access 2020;8:52588–608. [64] zuniga mm, sinoquet d. global optimization for mixed categorical-continuous variables based on gaussian process models with a randomized categorical space exploration step. infor inf syst oper res 2020;58:310–41. [65] lindauer m, eggensperger k, feurer m, ab, deng d, benjamins c, et al. smac3: a versatile bayesian optimization package for hyperparameter optimization. j mach learn res 2022;23:1–9. [66] picheny v, berkeley j, moss h, stojic h, granta u, ober s, et al. trieste: efficiently exploring the depths of black-box functions with tensorflow. 2023, arxiv. [67] cowen-rivers ai, ly w, wang z, tutunov r, jianye h, wang j, et al. hebo: heteroscedastic evolutionary bayesian optimisation. 2020, arxiv. generalized black-box optimization. 2023, arxiv.16 p. saves et al. [69] kandasamy k, vysyaraju kr, neiswanger w, paria b, collins c, schneider j, et al. tuning hyperparameters without grad students: scalable and robust bayesian optimisation with dragonfly. j mach learn res 2020;21:3098–124. [70] roy s, crossley wa, stanford bk, moore kt, gray js. a mixed integer efficient global optimization algorithm with multiple infill strategy - applied to a wing topology optimization problem. in: aiaa scitech 2019 forum. 2019. [71] müller j, shoemaker ca, piché r. so-mi: a surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems. comput oper res 2013;40:1383–400. [72] tran t, sinoquet d, da veiga s, mongeau m. derivative-free mixed binary necklace optimization for cyclic-symmetry optimal design problems. opt eng 2021. [73] meliani m, bartoli n, lefebvre t, bouhlel ma, martins jrra, morlier j. multi- fidelity efficient global optimization: methodology and application to airfoil shape design. in: aiaa aviation 2019 forum. 2019. [74] lee h. gaussian processes. springer berlin heidelberg; 2011, p. 575–7. [75] lópez-lopera af, idier d, rohmer j, bachoc f. multioutput gaussian processes with functional data: a study on coastal flood hazard assessment. reliab eng syst saf 2022;218:108139. [76] berthelin g, dubreuil s, salaün m, bartoli n, gogu c. disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis. internat j numer methods engrg 2022;123:3594–626. [77] cardoso i, dubreuil s, bartoli n, gogu c, salaün m, lafage r. disciplinary surrogates for gradient-based optimization of multidisciplinary systems. in: eccomas aerobest. 2023. [78] platt j, penny s, smith t, chen t, abarbanel h. a systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics. neural netw 2022;153:530–52. [79] charayron r, lefebvre t, bartoli n, morlier j. multi-fidelity bayesian optimiza- tion strategy applied to overall drone design. in: aiaa scitech 2023 forum. 2023. [80] charayron r, lefebvre t, bartoli n, morlier j. towards a multi-fidelity and multi-objective bayesian optimization efficient algorithm. aerosp sci technol 2023;142:108673. [81] wold h. soft modelling by latent variables: the non-linear iterative partial least squares (nipals) approach. j appl probab 1975;12:117–42. [82] priem r, diouane y, bartoli n, dubreuil s, saves p. high-dimensional efficient global optimization using both random and supervised embeddings. in: aiaa aviation 2023 forum. 2023. [83] betz w, papaioannou i, straub d. numerical methods for the discretization of random fields by means of the karhunen–loève expansion. comput methods appl mech engrg 2014;271:109–29. [84] menz m, dubreuil s, morio j, gogu c, bartoli n, chiron m. variance based sen- sitivity analysis for monte carlo and importance sampling reliability assessment with gaussian processes. struct saf 2021;93:102116. [85] ginsbourger d, le riche r, carraro l. kriging is well-suited to parallelize optimization. springer berlin heidelberg; 2010, p. 131–62. mization strategy based on kriging properties suitable for material parameters identification. arch mech eng 2020;67.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1sMyx9uhMbAM0AeuqbEN5zSfT9bLrXo2G&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "4",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "abstract.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "4",
                    "title": "modelgame: a quality model for gamified software modeling learning ",
                    "institutions": "kleinner farias",
                    "authors": [
                        "ed wilson",
                        "vale",
                        "sul",
                        "sul"
                    ],
                    "abstract": "gamification has been adopted in software development tasks in recent years. this adoption seeks, for example, to improve the en- gagement of developers while creating uml models or writing code. empirical studies report that uml models suffer from incom- pleteness and inconsistency problems. this study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concern- ing uml models. the current literature lacks studies that explore gamification and uml model quality in the context of software modeling learning. this article, therefore, proposes modelgame, which is a quality model to support software modeling learning in a gamified way. it serves as a reference framework so that instructors can obtain a parameterized way to evaluate uml models created by learners. the quality of uml models can be improved by apply- ing gamified activities and providing guidelines aware of quality issues. a qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. the results show that (1) 94.7% recognize that the proposed model can improve the quality of uml models, indicating that they would adopt the modelgame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. the results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling. ccs concepts • software and its engineering → software design engineer- ing.",
                    "keywords": "model design, learning model, gamification",
                    "content": "gamification has been adopted in software development tasks in recent years. this adoption seeks, for example, to improve the en- gagement of developers while creating uml models or writing code. empirical studies [7, 9, 14] report that uml models suffer from incompleteness and inconsistency problems. lange [14] rein- forces that these defects bring potential risks that can cause mis- interpretation and communication failure, representing a risk to software quality. thus, finding formats that favor student learning and consequently in generating increasingly effective uml mod- els can become one of the main challenges faced by instructors that include uml (unified modeling language) as part of software modeling content. some studies [3, 12, 25] sought to understand how to apply gam- ification in software modeling teaching using some elements such as points, emblems and levels. however, instructors and researchers still find limitations when applying, evaluating, and measuring the use of this tool in the learning of software modeling students and, consequently, in the models developed by them, since in the current literature there is no “frame of reference” that guides them. this study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning uml models. the current literature lacks studies that explore gamification and model quality in the context of software modeling learning. this article, therefore, introduces modelgame, which is a quality model to support software modeling learning in a gamified way. it serves as a reference framework so that instructors can obtain a parameterized way to evaluate uml models created by learners. the quality of uml models can be improved by applying gami- fied activities and providing guidelines aware of quality issues. a reference framework would help to (1) establish parameters for eval- uating uml models created by learners; (2) provide guidelines to improve the quality of these artifacts; (3) to analyze which elements of gamification could be included in each of the phases of modeling using uml; (4) identify intrinsic and extrinsic aspects of students during the modeling stages, to improve the models; (5) to compare validated theories about the inclusion of gamification in software modeling teaching, taking into account the types of learning and methodologies used; and (6) contributing to the identification of gamification use objectives in modeling activities. a qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. the re- sults show that (1) 94.7% recognize that the proposed model can improve the quality of uml models, indicating that they would adopt it in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. these results are encourag- ing, showing the potential for applying and improving the teaching and learning of software modeling. the remainder of the paper is organized as follows. section 2 presents the main concepts discussed throughout the article. sec- tion 3 discusses the related work, highlighting research opportu- nities. section 4 introduces the proposed quality model. section 5 presents how the quality model was evaluated. section 6 points out some threats to validity. finally, section 7 presents some concluding remarks and future work. 2 background this section presents the essential concepts for understanding this work, including gamification and software engineering teaching (section 2.1), and software modeling and model quality (section 2.2). 2.1 gamification and software engineering teaching gamification aims to use game elements in the context of not game [5], bringing all positive aspects they provide as a way to encourage and engage “players,” thereby broadening their motivations. werbach [23] classifies gamification into three dimensions: dy- namics, mechanics, and components. dynamicsinclude all game aspects related to the emotional responses of “players” (e.g., rela- tionship, progression, and narrative).mechanics offer elements that promote the action of a game — usually elaborated via a rule-based development —, so that the player can interact with such elements, e.g., challenges, feedback, and rewards. components represent the aesthetic elements of gamification, whose goal is to present visual aspects with which players can perform the interaction, for example, points, scores, and emblems (badges). knowing that the teaching of software engineering should in- volve students to experience the professional practices of the area so that they can understand which practices and techniques are useful in several different situations [2]. the challenges of teaching new software engineers are not limited to learning programming, but also include paying attention to detail, considering the quality of created models, established schedule and defined budgets [1]. in addition to understanding the technical challenges, these future professionals must be up to date with nontechnical issues, including teamwork, communication and management. to meet these new demands of the current context, the format with exhibition classes is no longer considered enough and may even become demotivating and ineffective in learning students. in this sense, gamification has been increasingly used in the teach- ing of software engineering as a way to promote behavioral and psychological changes [11] providing an environment that favors communication, cooperation, feedback, reward, achievement and other recurring elements that are capable of improving perfor- mance, efficiency and engagement in educational activities , and can enhance, for example, the learning of software modeling. 2.2 software modeling and model quality software modeling encompasses the set of principles, concepts, and practices that lead to the development of a high-quality system or product. the principles of this activity establish a philosophy that guides the entire software development process. in this scenario, uml models play a crucial role in software development tasks, for example, documenting project decisions, understanding development details, promoting better communica- tion between teams, and generating greater efficiency in software development [19]. however, these models suffer problems of in- consistency and incompleteness [10, 18], as well as end up being overlooked within the modeling process, as pointed out in some empirical studies in the literature [14, 15]. class and sequence dia- grams, for example, present inconsistencies when sequence diagram objects are not found in the class diagram, consequently developers end up living with inconsistencies throughout the development process. a research challenge still open is how to evaluate these diagrams, both in industry and in the teaching process, in terms of quality, such as syntactic and semantic, for example. 3 related work the selection of related works was carried out following two steps: (1) search in digital repositories, such as google scholar and scopus (elsevier) of articles related to gamification, quality modeling, and modeling learning; and (2) filter selected articles considering the alignment of such works with the objective of the work (section 4). after selecting the works, they were analyzed (section 3.1) and compared (section 3.2), seeking to identify research opportunities. 3.1 analysis of related works porto et al. (2021) [4]. this work performed a systematic map- ping with the objective of characterizing how gamification has been adopted in noneducational contexts of software engineering activities. the main results of this study show that gamification provided benefits for activities such as requirements specification, development, testing, project management, and support process. in addition, he pointed out that the number of publications and new research initiatives has increased over the years, many posi- tive results have been achieved in software engineering activities. nevertheless, the study reinforced that gamification can still be explored for other tasks in this area, as empirical evidence is very limited. marin (2021) [17]. it performed the application of gamification on some topics of a software engineering course to engage students and increase their motivation and argued that, with due motiva- tion, students can better exercise the topics and obtain more solid knowledge. there were five games related to risk management, bpmn modeling, scrum process, design and inspection of class diagrams, and cosmic functional size measurement to assist in the learning process of the software engineering course. this study also presented the lessons learned about the application of gamification and serious games in software engineering, including limitations or disadvantages. jurgelaitis et al. (2018) [12]. this work conducted a research to investigate how gamification could be inserted into an information systems modeling course, which covers a range of topics on uml. as a result, an implementation of the gamified system modeling course in the moodle environment was presented, using additionalplugins for the use of the necessary gamified elements. the study showed good results and obtained a positive acceptance by the participating students. rodrigues et al. (2018) [22]. they investigated the use of games and game elements in software engineering education, through a research that had the participation of 88 instructors of this disci- pline. the results showed that most instructors are aware of these educational approaches, however, the games were adopted by only 21 participants and game elements were adopted only by 19. games are most often used to cover “software process” and “project man- agement”. the most commonly used game elements are points, quizzes, and challenges. the results also show that the main rea- sons for not adopting the resources are the lack of knowledge, information about games relevant to the engineering of teaching software, and the lack of time to plan and include these approaches in the classroom. cosentino et al. (2017) [3]. they present a model-based ap- proach to learning modeling in a gamified way. the approach in- cludes a new language to model the gamification process itself and an environment where it can be incorporated into current mod- eling tools to allow instructors and students to design and use a complete modeling framework, including gamification elements. in addition, the approach also had as a proposal to provide support to collect and analyze gamification data, thus facilitating monitoring activities. yohannis (2016) [25]. this research presents an exploration of game design as an approach to strengthening the student’s mas- tery in software modeling by developing their abstraction skills. it brought together concepts of gamification development, such as the lens of atoms of intrinsic skill and principles of pedagogical design of various theories and models of learning. the research follows the design science research methodology and explores the best practices of model oriented engineering. as a result, a modeling game design framework and generation structure and a series of produced games are presented. pedreira et al. (2015) [21]. they developed a systematic map- ping of gamification in software engineering based on 29 studies. the mapping revealed that software implementation is the area in which most studies focus, followed by software requirements, few others in different areas, such as project planning and software testing, and even to a lesser extent in activities involving software modeling. however, the highlight of this work was to highlight that gamification in software engineering is still at a very early stage and the evidence on its impact in this field remains inconclusive. 3.2 comparative analysis and opportunities five comparison criteria (cc) were defined selecting the most rele- vant variables to assist in the process of identifying similarities and differences between the proposed work and the selected articles. this comparison is crucial to make the process of identifying re- search opportunities using objective rather than subjective criteria. the criteria are described below: • context (cc01): works that explore the use of gamification in software modeling teaching/learning. • participant profile (cc02): studies that collected data from participants for screening and profile characterization. • applicability of gamification in uml (cc03): studies that evaluated how gamification can contribute to uml mod- els. • model creation (cc04): studies that have developed a model to improve factors that imply the non-adoption of uml. • instructor participation (cc05): studies that collected qualitative data through the participation of software mod- eling instructors. table 1 shows the comparison of the selected works, confronting this work. some gaps and research opportunities are observed: (1) only the proposed work was the only one to fully meet all compar- ison criteria; (2) although most of them targeted the application of gamification in software modeling teaching, they were not di- rected to the use of uml; (3) no study has developed a model to evaluate the learning and improvement of uml models developed by students; and (4) most of them did not have the participation of instructors to identify the difficulties and opportunities in the application of gamification in the teaching of software modeling. thus, the next section presents a quality model to explore these identified opportunities. related work comparison criterion cc1 cc2 cc3 cc4 cc5 proposed work � � � � � porto et al (2021) [4] � � � � � marin (2021) [17] � � � � � jurgelaitis et al (2018) [12] � � � � � rodrigues et al (2018) [22] � � � � � cosentino et al (2017) [3] � � � � � yohannis (2016) [25] � � � � � pedreira et al (2015) [21] � � � � � � completely meets � partially meets � does not attend table 1: comparative analysis of the selected related works 4 proposed quality model this section presents the proposed quality model to support soft- ware modeling learning in a gamified way. it serves as a frame of reference so that instructors can evaluate the uml models cre- ated by students through gamified activities. section 4.1 presents a proposal of a generic analytical framework. section 4.2 details the abstract syntax of the proposed quality model. section 4.3 ex- plains the quality notions related to the gamified software modeling learning. 4.1 generic analytical framework figure 1 presents the generic analytical framework for improving the quality of the models and serves as the basis for the creation of an evaluation scheme. the arrows (\"links\"), labeled as evaluation and gamified modeling, represent the questions that the evidence must answer; dotted lines represent associations; rectangles rep- resent the models (rounded corners) or the quality states (square corners) by which these bindings are measured. ellipses represent the adverse effects that can be generated from the evaluation and use of gamification. the numbers refer to the key questions and are connected with the concepts and relationships of the abstract syntax of the quality figure 1: generic analytical framework for gamified software modeling learning. model (presented in section 4.2), as follows: (1) are there tools that assist instructors in evaluating the models developed by stu- dents, thus reducing the poor quality and incompleteness of these artifacts? (2) what is the prevalence of characteristics that cause models to be at risk? (3) are there notions of quality to evaluate the models as a way to define parameters when performing their correction? (4) applying the use of gamification in models that need intervention would be a way to identify factors that could generate models with high quality levels? (5) does the application of gamification improve the quality of the model? (5.a) how are the models without gamification evaluated in relation to those with gamification? (5.b) are there reasons to expect that gamification models can have better quality results than those that are gener- ated without gamification? (6) is the output model really effective when associated with reducing the poor quality of the model? (7) does the absence of evaluation result in adverse effects? (7.a) is the evaluation acceptable for the model? (7.b) what are the potential harms, and how often do they occur? (8) does gamification result in adverse effects on models? fact is that it is not enough just to include this \"toolbox\" in the uml learning process, it is necessary to provide the instructor with a model (guide) that can serve as a reference to evaluate the quality of diagrams elaborated through gamified activities. for example, the instructor could create models predefining inconsistencies by making use of these questions raised to evaluate the models created by the students. the set of questions serves as the starting point for this evaluation. knowing that the adaptation of the gamification approach requires a significant effort [20], in this study we present the modelgame as a way to identify factors that contribute to the quality of these artifacts and, consequently, to the students’ learning. 4.2 abstract syntax following the specification pattern of the uml metamodel, figure 2 presents the abstract syntax of the proposed quality model for gamified software modeling learning (modelgame). it identifies the main concepts and relationships. the numbers represent the notions of quality that are discussed in section 4.3. the following are detailed each of these concepts and relationships. domain. the first concept presented in this study is the domain, which corresponds to a specific context of the application to be developed to solve the problem. in this process, the design template represents the solution given to the domain. association • contextualizes: challenges[*] each contextualise refers to the domain that will serve as the basis for the challenges launched. challenges. this concept represents the phase in which the problem is contextualized (domain-based), as well as what will be the missions, phases, scenarios, and other elements presented to the players, in this case the students, who must use the principles of software engineering to perform the modeling and reach the final goal. association • influences: design model[*] each influence represents that the proposed challenge interfered in aspects of the design model, causing the user to seek to make a continuous improvement. modeling language. software modeling is an important step for development to happen in a way that adheres to the require- ments established by the requester, for this, there is the modeling language, which offers a standardized way to document and design software. through the use of modeling languages, it is possible to achieve a high level of understanding about the software in ques- tion, improving the communication between all those involved in the process, thus avoiding implementation errors. it points out that software engineers use these languages to communicate design decisions and verify the feasibility of implementing the intended design. the uml was consolidated as the modeling language in the paradigm of object orientation, in which it is possible through visual notation generated from the diagrams- presented later in this study as design models- to perform the representation of various perspectives of the system. association figure 2: abstract quality model syntax. • expresses: design model[*] performs the representation of the intended design templates, in which the modeling language should be applicable to the domain type. user. this concept corresponds to the individual who performs the interpretation of the developed design models, whose objective is to be able to understand the domain in question. in the gamified context, the user has the role of player and it is he who performs the whole process, being able to perform the interpretation of ex- isting models or even creating new ones. the user can also identify and resolve inconsistencies that arise from compositions between models. association • creates: design model[1..*] represents the process in which the user creates a design template, which can be one or more. • interprets: design model[1..*] in this association, the user performs the interpretation of the design template. when interpreting the model, paths for the resolution of inconsistencies can be identified. • detects: inconsistency [*] represents the user’s discovery of design model inconsistencies, for example, those that are generated from identifying conflicts, whether a class is abstract or not. • resolves: inconsistency [*] each resolves equates to the resolution representation of the incon- sistencies by the user that happens after he analyzes and determines the best alternative to perform this action. • uses: modeling tools [*] determines that the user can use modeling tools to generate/update design models. association • without a directed relationship. modeling tool. this concept represents the applications that are used to carry out the construction of design models. there are several tools available, online and desktop, and it is up to the user to choose the one that will best meet their needs and adapt to the context in question, that is, they work in any domain that is being considered. design model. the design model refers to a visual notation (diagram) to represent static and dynamic aspects. these models are built according to a specific objective or task and tend to facilitate the logical interpretation of the software in several aspects. the most popular diagrams are use cases and classes, the first being static and representing a set of actions generated from functional requirements (use cases) and presenting the interactions generated with external users (actors). the second is a static diagram and makes the representation of the logical structure of the software involving the classes, their attributes, methods, and relationships between them [19]. association • describes: domain[1] each describes makes the representation of a specific domain and means that every design model must describe it. inconsistency. it corresponds to the defects found in the models developed by users. they may occur because of the nonidentifi- cation and correction of possible conflicts and even an erroneous interpretation. association • affects: design model[*] this association indicates that with each occurrence of the affect, a problem is presented harming the quality of the design model.points. this concept represents one of the most used game me- chanics in software engineering and functions as a quantitative reward for each action developed, in which it is possible to regulate the number of rewarded points of the player, defined here as user, based on the importance of each action. through this concept, it is possible to stimulate competition, collaboration, and creativity among users, stimulating learning. points appear as a derivation of the association affects, since when each inconsistency error is identified or not, the user will receive a score and the association describes, because the points will also be applied when making connections between the model and the domain. progress. the concept of progress emerges as a factor that makes the user able to perceive its evolution in the process, in this case, software modeling. progress emerges as a derivation of the associa- tion interprets, making the user know when they have performed a correct interpretation of the proposed design model or what still needs to be improved. feedback. feedback has the role of making the user realize that the proposed goal can be achieved and follow its evolution, includ- ing analyzing how to change or creating new strategies to achieve the goal. this concept emerges as a derivation between the associa- tions it creates, causing the user to receive a return to the model creation process. 4.3 quality notions as discussed in section 2, gamification can bring important ele- ments for learning software modeling and, therefore, the objective of this section is to produce the notions of quality of the model of this study. the modelgame is composed of ten counts, four of which are proposed in this study - scope, use, motivational and en- gagement - extracted from the main benefits that the gamification elements presented in figure 2 can bring to the models. the others are adaptations of previous works [6, 14, 15], they are, syntactic, semantic, social, effort, detection and resolution. scope quality (1). it seeks to determine how much the proposed challenge is contextualized with the design model, as well as the def- inition of the domain, problem, competencies, concepts, behaviors and attitudes that will be developed throughout the process. syntactic quality (2). this notion makes the representation of the process of correction of the design models that are produced by the modeling language, because if it is not used correctly, incon- sistencies will arise. it is important to insert this notion of quality into our study, since during the process of developing the models, users may come across the composition of two class diagrams, for example. semantic quality (3). it is necessary to verify that the design model and the problem domain match, so this notion performs this type of analysis. communication problems may occur between users if the semantic elements of the model are affected. social quality (4). design models are used to communicate between members of a team to inform all established decisions about software development [8]. if divergent interpretations occur, this communication will be greatly impaired. quality of effort (5). this notion refers to the production chal- lenges of the model that will be generated, including factors such as time and cost. quality of use (6). to produce design templates, users can use unusual tools such as paper, whiteboard, and more. however, most of the time they choose to use formal tools (cases) and can be online or desktop. this notion corresponds to the level of ease and applicability of the models elaborated when making use of these tools, it is also important to contribute to communication between users through collaboration-related functionalities. detection quality (7). this notion is referenced to the process of locating inconsistencies, since when users arise, they should perform traceability of them quickly. if the detection is complicated, it could hinder the process of correcting the models. resolution quality (8). it corresponds to the level of quality related to the effort that users take to look for alternatives to solve the identified problem. motivational quality (9). this notion refers to the motiva- tional factors involved during the learning and development of design models, which can be intrinsic and extrinsic. elements of gamification such as points, feedback and progress bring the user a degree of satisfaction in continuing their discovery and transfor- mations throughout the process. quality of engagement (10). the user in tracking their progress can feel committed to the objective in question, and this notion represents the measurement of the level of commitment of them during the development of design models. 5 evaluation this section describes the methodology followed to evaluate the proposed quality model. this methodology follows well-established empirical guidelines [24]. section 5.1 details the objective and re- search questions (rq). section 5.2 presents the questionnaire formu- lated to evaluate the proposed quality model. section 5.3 explains the context and selection of participants. section 5.4 describes the presentation of the model. section 5.5 presents the analysis of the collected data. 5.1 objective and research questions the objective (o) of this study is twofold: (o1) introduce model- game as a tool for teaching software modeling; and (o2) analyze the applicability of the quality model regarding the improvement of uml models. to analyze the different facets of the objectives, two research questions (rq) have been formulated: • rq1: how do instructors evaluate the use of gamification in software modeling? • rq2: what is the acceptance of modelgame by software modeling instructors? 5.2 questionnaire data was collected through an online questionnaire created through google forms1 following well-established guidelines described in [24]. this strategy was chosen because the questionnaire could be applied quickly and easily collect data from individuals in geo- graphically diverse locations. the questions of the questionnaire 1questionnaire: https://forms.gle/qjafdperetgdluww6were concerned with examining the research gaps of previous stud- ies and apprehending the structures of the previously developed questionnaire. part 1: participant profile. the first part of the questionnaire consisted of collecting data that are related to the characteristics and opinions of the participants. the creation of the participant profile through this data is important to make the selection of possible users of modelgame. without this profile, participants with an inadequate profile may generate inconsistent assessments. participants were asked to provide more general information, such as age, education level, academic background. information about the time of experience in teaching was also considered, including teaching software modeling and level of knowledge about uml models. part 2: tam questionnaire. the second part addressed ques- tions about the usability and acceptance of the technique, aiming to explore q3. to this end, this part of our questionnaire is based on the technology acceptance model (tam) [16]. this part contained nine questions, which were answered through the likert scale, in- cluding totally agree, partially agree, neutral, partially disagree, and totally disagree. the questions formulated (q) dealt with sev- eral topics, including perceived ease of use (q1-3), perceived utility (q4-7), attitude towards use (q8), and behavioral intention to use (q9). 5.3 selection of participants the participants were selected based on the following criteria: in- structors and/or professionals working in the teaching of software modeling in higher education institutions in brazil. using this cri- terion, we sought to select participants with academic training and practical experience in teaching. this finite set of all possible participants represents the target population [13]. this popula- tion represents those people who are in a position to answer the questions formulated and to whom the results of the survey apply [13]. in all, 19 people (n) answered the questionnaire. the partici- pants were invited via e-mail to participate in the study and each of them previously received the explanation/training about the model proposed through the researcher and there was no doubt, they could leave for the next step that consisted of completing the tam questionnaire. we discussed the experimental process in the next section. 5.4 experimental process figure 3 presents the experimental process used in this study, which is composed of three phases discussed below: phase 1: presentation. it has an activity, presentation, in which the researcher explained to the participants through a video detail about the quality model. this process took place individually and in a standard way, where space was also made available for par- ticipants to answer possible doubts about the proposed study and model, lasting an average of 20 minutes. phase 2: application of the tam questionnaire. it has two activities, the first being collect demographic data. the participants answered a list of questions (input) so that we could collect their characteristics and opinions about the modelgame. the demo- graphic data collected (output) became the result of this activity. the second activity apply tam questionnaire (input). participants received a list of questions about the perception of ease of use, per- ceived utility, attitudes, and intention of behavior, in relation to the modelgame. qualitative data (output) were generated, regarding the usability and acceptance of the model under the perspective of professionals who teach software modeling. this questionnaire followed the guidelines of the tam [16]. phase 3: analysis and result report. it has two activities. the first, analyze data sought to perform a thorough analysis of the data collected through the questionnaire and the researcher’s perception regarding the participants’ doubts during the presentation stage. for this, the collected data were analyzed separately, as well as con- fronted, aiming to perform a triangulation of them. subsequently, there was an evaluation data, as a way to understand in a more depth the context, the perceptions of the participants in relation to the proposed model as well as its applicability. figure 3: the experimental process. 5.5 result analysis 5.5.1 profile data of the participants. table 3 describes the profile data, reporting the characteristics and opinions of the participants. these data were collected from may 18 to june 5, 2021. in total, we had 19 participants. our participants are between 20 and 49 years old, most of them have a degree in computer science (52.6%), information systems (26.3%) or systems analysis (21.1%) and are specialists (36.8%), masters (36.8%) and doctors (15.8%). about the working time in teaching, the majority (42.1%) they have been teach- ing for more than 8 years and teach disciplines related to software modeling, including software engineering, systems analysis and software projects. a total of 47.4% have a full level of knowledge about uml and almost half of them (47.4%) has not yet used gamifi- cation in the teaching of software modeling. therefore, we considertotally agree partially agree neutral partially disagree totally disagree perceived ease of use i found the quality model easy to use 8 9 2 0 0 i found the quality model easy to learn 10 9 0 0 0 i found the quality model easy to master 6 12 0 1 0 perceived usefulness the model would make it easier to understand which elements of gamification can be used in modeling . 12 5 2 0 0 using the quality model would help increase productivity. 9 8 2 0 0 the model would provide an understanding of how to mitigate the incompleteness of uml diagrams. 5 8 5 1 0 the model would help compare theories about gamification in software modeling teaching. 13 4 2 attitude towards use using the quality model for gamified software modeling learning is a good idea. 13 5 1 0 0 behavioral intention to use i would use the quality model in software modeling classes. 10 7 2 0 0 table 2: collected data related to tam questionnaire. that although small, our sample is adequate to carry out an initial evaluation of the proposed approach. characteristic and opinion (n=19) answer # % age < 20 years 0 0.0% 20-29 years 4 21.1% 30-39 years 8 42.1% 40-49 years 5 26.3% > 49 years 2 10.5% education undergraduate* 0 0.0% specialization* 7 36.8% master* 7 36.8% phd* 3 15.8% others 2 10.6% undergraduate course information systems 5 26.3% computer science 10 52.6% computer engineering 0 0.0% system analysis 4 21,1% others 0 0.0% time of experience in teaching < 2 years 4 21.1% 2-4 years 2 10.5% 5-6 years 3 15.8% 7-8 years 2 10.5% > 8 years 8 42.1% experience in teaching < 2 years 3 15.8% software modeling 2-4 years 5 26.3% 5-6 years 3 15.8% 7-8 years 2 10.5% > 8 years 6 31,6% level of knowledge beginner 2 10.5% about uml models junior 5 26.3% full 9 47.4% senior 3 15.8% used gamification in teaching yes 9 47.4% no 9 47,4% maybe 1 5.3% gamification can contribute to the quality totally agree 10 52.6% of the models of uml diagrams generated partially agree 8 42.1% by students neutral 1 5.3% partially disagree 0 0.0% totally disagree 0 0.0% table 3: the profile data of the participants. 5.5.2 rq1: how do instructors evaluate the use of gamification in software modeling? table 3 presents the collected data related to the rq formulated. first, we begin the analysis by verifying how instructors visualize gamification in software modeling teaching. although most of them (47.4%) have not yet used gamification elements (scores, challenge, emblem, among others) in their classes, most (52.6%) totally agree and (42.1%) partially agree that the use of these can contribute to the quality of the models developed by the students. we consider the percentage of instructors who have not yet used gamification in their classes to be high and this may be tied to factors such as lack of knowledge, information about the tool, and even time to plan and include these approaches [22]. although they were based on software modeling teaching context, previous studies [3, 4, 12, 17, 25] they did not count on the participation of instructors and we understand that this participation is fundamental to understand the perceptions of these professionals since they will be at the forefront of the use of gamification. the modelgame proposed in this study could help them insert gamification into their classes, according to the software modeling learning design [25], based on the assumption that for this, it is necessary to develop a better understanding of the tasks, activities, skills and operations that the different elements of gamification can offer and how they can correspond to the desired learning outcomes by developing a more concrete and motivating presentation that can involve students and facilitate deep learning with uml. 5.5.3 rq2: what is the acceptance of the modelgame by software modeling instructors? using the tam questionnaire, we tried to evaluate the ease of use, perceived usefulness, attitude, and behav- ioral intention to use the quality model. table 2 shows the data obtained. our data obtained show that no one disagreed that the modelgame is easy to use, learn, and master. on the contrary, al- most 90% of participants find the model easy to use (42.1% totally agree and 47.4% partially agrees and 10.5% neutral), learn (52.6%fully agree and 47.4% partially agree) and master (31.6% fully agree, 63.2% partially agree and 5.3% partially disagree). the results are also favorable considering the perception of util- ity. most participants realized that the modelgame would make it easier to understand which elements of gamification can be used in each of the phases of modeling using uml(63.3% totally agree, 26.3% partially agree and 10.5% neutral), increase productivity (47.4% fully agree, 42.1% partially agree and 10.5% neutral), and the use of the quality model would provide an understanding of how to mitigate the incompleteness of uml diagrams (26.3% agree totalmen 42.1% partially agree, 26.3% neutral and 5.3% partially disagree). still in the useful aspect, we tried to know if the quality model would help to compare validated theories about the inclusion of gamification in software modeling teaching (68.4% totally agree, 21.1% partially agree and 10.5% neutral). considering the attitude towards use, participants believe that using the modelgame is a good idea (68.4% totally agree, 26.3% partially agree and 5.3% neutral), just as they are confident and would use the model in software modeling classes (52.6% totally agree, 36.8% partially agree and 10.5% neutral). these findings show the potential for acceptance by people with profiles similar to those of participants. the results are encouraging and show the potential to use the proposed approach in the educational scenario. 6 threats to validity this section discusses the possible threats to the validity of the study. internal validity. the main point affecting the internal validity of our study concerns the total time used for the exploratory phase. to mitigate this threat, we performed the video recording of a pilot explaining the operating details and objectives of the modelgame. in relation to the methods used, the threats related to internal validity relate to how we extract the perceptions of the discussions and whether they represent the perceptions of teachers about the use of the model. we try to reduce this threat by applying the tam questionnaire. external validity. we identified threats related to external va- lidity, such as the number of participants who never applied the use of gamification. this study was limited to 19 participants (teachers) from various educational institutions, of which 9 (47.4%) never used any element of gamification in their classes, this factor can interfere in the data, since the model intends to evaluate the quality of uml diagrams from gamified activities. conclusion validity. threats related to the validity of the con- clusion are related to treatment and outcome. we try to make the reduction by combining quantitative and qualitative data through different resources. these data were obtained through audio and questionnaires. we analyze this data to answer the research ques- tions. 7 conclusions and future work this study proposed an initial quality model (modelgame) that serves as a reference framework for instructors for qualitative eval- uations of uml models developed from gamified activities, the application of an empirical study with 19 participants was carried out to understand their vision in relation to gamification and the acceptance of the proposed model. it was identified that most have not yet used gamification in their classes, but agree that their use can contribute to the quality of the models developed by the stu- dents and were open to using the model. our findings can enhance the adoption of new teaching practices through gamification, result- ing in the improvement of software modeling learning using uml, and consequently the creation of models developed by students. these approaches can stimulate students’ immersion in the design of systems as future professionals during learning. finally, we hope to carry out in the future a series of experimental studies to analyze each stage of application of the modelgame and that this work represents a first step to better support the application of empirical studies on models of evaluation of the use of gamification in software modeling. we also hope that the questions described throughout the article will encourage other researchers to extend our study to different modeling languages and teaching methodologies.",
                    "references": "[1] rick adcock, edward alef, bruce amato, mark ardis, larry bernstein, barry boehm, pierre bourque, john brackett, murray cantor, lillian cassel, et al. 2009. curriculum guidelines for graduate degree programs in software engineering. acm. [2] mark ardis, david budgen, gregory w hislop, jeff offutt, mark sebern, and willem visser. 2015. se 2014: curriculum guidelines for undergraduate degree programs in software engineering. computer 48, 11 (2015), 106–109. [3] valerio cosentino, sébastien gérard, and jordi cabot sagrera. 2017. a model- based approach to gamify the learning of modeling. ceur workshop proceed- ings. [4] daniel de paula porto, gabriela martins de jesus, fabiano cutigi ferrari, and sandra camargo pinto ferraz fabbri. 2021. initiatives and challenges of using gamification in software engineering: a systematic mapping. journal of systems and software 173 (2021), 110870. [5] sebastian deterding, miguel sicart, lennart nacke, kenton o’hara, and dan dixon. 2011. gamification. using game-design elements in non-gaming contexts. in chi’11 extended abstracts on human factors in computing systems. 2425–2428. [6] ana fernández-saez et al. 2012. a systematic literature review on the quality of uml models. j. data. manage 22, 3 (2012), 46–70. [7] kleinner farias et al. 2012. evaluating the impact of aspects on inconsistency detection effort: a controlled experiment. in international conference on model driven engineering languages and systems. springer, 219–234. [8] kleinner frias et al. 2014. towards a quality model for model composition effort. in 29th annual acm symposium on applied computing. 1181–1183. [9] kleinner farias et al. 2015. evaluating the effort of composing design models: a controlled experiment. software & systems modeling 14, 4 (2015), 1349–1365. [10] kleinner farias et al. 2019. uml2merge: a uml extension for model merging. iet software 13, 6 (2019), 575–586. [11] juho hamari, jonna koivisto, and harri sarsa. 2014. does gamification work?– a literature review of empirical studies on gamification. in 2014 47th hawaii international conference on system sciences. ieee, 3025–3034. [12] mantas jurgelaitis, vaidotas drungilas, and lina čeponien˙e. 2018. gamified moodle course for teaching uml. baltic journal of modern computing 6, 2 (2018), 119–127. [13] barbara a kitchenham and shari l pfleeger. 2008. personal opinion surveys. in guide to advanced empirical software engineering. springer, 63–92. [14] christian franz josef lange. 2007. assessing and improving the quality of modeling: a series of empirical studies about the uml. (2007). [15] odd ivar lindland, guttorm sindre, and arne solvberg. 1994. understanding quality in conceptual modeling. ieee software 11, 2 (1994), 42–49. [16] nikola marangunić and andrina granić. 2015. technology acceptance model: a literature review from 1986 to 2013. universal access in the information society 14, 1 (2015), 81–95.[17] beatriz marín. 2021. lessons learned about gamification in software engineer- ing education. in latin american women and research contributions to the it field. igi global, 174–197. [18] kleinner oliveira, alessandro garcia, and jon whittle. 2008. on the quantitative assessment of class model compositions: an exploratory study. 1th esmde at models (2008). [19] omg. 2017. uml: infrastructure specification. https://www.omg.org/spec/uml/2.5.1/pdf. [20] sofia ouhbi and nuno pombo. 2020. software engineering education: challenges and perspectives. in ieee global engineering education conference. 202–209. [21] oscar pedreira, félix garcía, nieves brisaboa, and mario piattini. 2015. gamifica- tion in software engineering–a systematic mapping. information and software technology 57 (2015), 157–168. [22] pedro rodrigues, mauricio souza, and eduardo figueiredo. 2018. games and gamification in software engineering education: a survey with educators. in 2018 ieee frontiers in education conference (fie). ieee, 1–9. [23] kevin werbach and dan hunter. 2012. for the win: how game thinking can revolutionize your business. wharton digital press. [24] claes wohlin, per runeson, martin höst, magnus c ohlsson, björn regnell, and anders wesslén. 2012. experimentation in software engineering. springer science & business media. [25] alfa yohannis. 2016. gamification of software modelling learning.. in ds@ models.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1giitxQiD-AgCzUh4hBsEhi7L-cqPORwp&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "13",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "keywords.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "13",
                    "title": "available online 12 december 2023 0965-9978/© 2023 published by elsevier ltd. ",
                    "institutions": "improved stochastic subset optimization method for structural design optimization department of civil engineering, indian institute of technology delhi, new delhi 110016, india  a r t i c l e  i n f o",
                    "authors": [
                        "mohd aman khalid",
                        "sahil bansal"
                    ],
                    "abstract": null,
                    "keywords": "stochastic subset optimization voronoi tessellation stochastic simulation stochastic optimization optimization under uncertainty a b s t r a c t the stochastic subset optimization (sso) algorithm was proposed for optimal reliability problems that mini- mizes the probability of system failure over the admissible space for the design parameters. it is based on the simulation of samples of the design parameters from an auxiliary probability density function (pdf) and exploiting the information contained in these samples to identify subregions for the optimal design parameters within the original design space. this paper presents an improved version of sso, named isso to overcome the shortcomings in the sso. in the improved version, the voronoi tessellation is implemented to partition the design space into non-overlapping subregions using the pool of samples distributed according to the auxiliary pdf. a double-sort approach is then used to identify the subregions for the optimal design. the isso is presented as a generalized design optimization approach primarily tailored for the stochastic structural systems but also adaptable to deterministic systems. several optimization problems are considered to illustrate the effectiveness and efficiency of the proposed isso.  1.",
                    "content": "structural optimization may be defined as the rational establishment of an economical structural design with the available resources while satisfying specific performance criteria. in general terms, the economy may be characterized by minimum weight, minimum cost, maximum utility, or even minimum probability of failure. broadly, structural optimization can be categorized into deterministic and stochastic opti- mization [1,2]. the classical statement of unconstraint deterministic optimization is mathematically expressed as: minimize : φ∈φ g(φ) (1) where, φ = [φ1⋯φnφ]t ∈ φ⊂rnφ is a set of deterministic adjustable pa- rameters that define the structural design, referred to herein as design parameters, g(φ) : rnφ→r is the objective function to be minimized, and φ denotes the bounded admissible design space. the deterministic constraints can be considered by the appropriate definition of the ad- missible design space φ for deterministic design parameters φ, as mentioned in [3]. in the deterministic structural optimization problem, the uncertainties in parameters are ignored, and fixed values are assumed for all the parameters. there are numerous optimizations approaches available in the literature, however, but it’s worth noting that no one-size-fits-all optimization approach is ideal for all sorts of problems [4–7]. the choice of optimization method is often determined by the specific characteristics of the problem, such as its complexity, dimensionality, constraints, and the nature of the objective function. as a result, there is always a scope for new approaches to be developed or the adaptation of existing methods to better suit specific problem clas- ses. a detailed discussion of deterministic optimization approaches can be found in the literature [8,9]. in any practical situation, several parameters, such as loadings, structural parameters, geometric parameters, operation conditions, etc., are either not known at the design stage or are subjected to random fluctuations that give rise to performance variability and affect the performance of a system [10]. these parameters are characterized as uncertain parameters. deterministic structural optimization discards the impact of uncertainty and can result in improper design. therefore, it is desirable to account for the uncertainty in the parameters during opti- mization by using the rational methods of probabilistic structural analysis [11]. such structural optimization that accounts for un- certainties is called stochastic optimization [12]. although stochastic optimization refers to any method that employs randomness within some communities, in this paper, we will only consider settings where * corresponding author. e-mail addresses: mohdamankhalid@gmail.com (m.a. khalid), sahil@iitd.ac.in (s. bansal). contents lists available at sciencedirect advances in engineering software journal homepage: www.elsevier.com/locate/advengsoft https://doi.org/10.1016/j.advengsoft.2023.103568 received 5 june 2023; received in revised form 2 october 2023; accepted 24 november 20232 the objective function is random. stochastic optimization or optimal design under uncertainty has been widely applied in many practical engineering fields, including civil engineering structures [13–15], composite structures [16,17], and vehicles [18,19]. consider an engineering system that involves deterministic design parameters φ, and uncertain variables θ = [θ1⋯θnθ]t ∈ θ⊂rnθ following a joint pdf p(θ|φ), where θ denotes the parameter space of the uncer- tain variables. the classical statement of stochastic optimization is mathematically expressed as: minimize : φ∈φ eθ[h(φ, θ)] (2) where, h(φ, θ) : rnθ+nφ→r is the structural performance function, and eθ[ ⋅ ] denotes expectation with respect to the pdf for θ. note that the objective function in the optimization problem in (2) is the expectation eθ[h(φ, θ)] which is a deterministic function. it’s worth mentioning that stochastic optimization may also involve other stochastic measures such as variance or quantile values. however, these stochastic measures can rarely be evaluated analytically; therefore, several methods have been proposed for solving stochastic optimization problems. these special- ized methods include, for example, sample average approximation, stochastic approximation, stochastic subset optimization, and ap- proaches based on the use of taylor series expansion [15,20,21], response surface, and metamodels [22–25]. specific to structural engi- neering, there are two broad categories of problems involving design optimization under uncertainty [26–35]: reliability-based design optimization (rbdo) and robust design optimization (rdo). the objective of rbdo is to find an optimal solution that minimizes some deterministic, objective function under observance of probabilistic constraints instead of conventional deterministic constraints [36,37]. on the other hand, rdo aims to find an optimal solution that is insen- sitive (or less sensitive) to input variations. it improves the design quality by minimizing performance variation without eliminating un- certainty [29,38]. taflanidis and beck [39] introduced a novel algorithm for optimal reliability problem, the so-called sso. sso involves formulating an augmented problem where the design parameters are artificially considered uncertain and defining an auxiliary pdf that includes the structural performance function and the pdf of the uncertain variables. next, sso involves generating a pool of samples distributed according to this auxiliary pdf and identifying a subregion in the original design space, which, on average, improves the value of the objective function. by repeating this procedure several times, it is possible to determine at each step a smaller subregion in the design space, which in turn im- proves the value of the objective function. ultimately, this subregion will be sufficiently small to directly identify the optimal solution or provide sufficient information to launch another optimization algo- rithm, such as the sample average approximation or stochastic approx- imation. the implementation of the sso method closely resembles the subset simulation (ss) algorithm [40] for reliability analysis. since sso is based on simulation, it can deal with linear or nonlinear problems and, at least theoretically, an unbounded number of design parameters. the numerical effort for solving a given optimization problem is indepen- dent of the number of uncertain variables, and it grows linearly with the number of design parameters. since the introduction of sso, several extensions of sso have been proposed. an extension of sso termed non-parametric sso, which adopts kernel density estimation to approximate the objective function, is presented in [41]. in [42], efficient integration of the moving least squares approximation within sso is introduced to reduce the compu- tational effort in sso. in [3], an augmented formulation is presented for the rdo of structures using sso. sso or its variants have also been applied to solve structural optimization problems. sso has been used for reliability optimization and sensitivity analysis in system design in [39]. a framework for rdo of tuned mass dampers (tmd) by sso is dis- cussed in [43]. even though sso has proved to be efficient for meeting various challenging optimization problems, it has two shortcomings. first, the effectiveness of sso is dependent on the correct selection of the geometrical shape of the admissible subsets. here, it is pertinent to mention that choosing a geometrical shape that effectively investigates the sensitivity of the objective function to each design variable is essential. the shapes, such as hyper-rectangle and hyper-ellipse are suggested in the literature for the admissible subsets. however, as shown later via the illustrative example, these shapes fail to include the optimal solution in cases with complex design spaces or problems with multiple optimal solutions. and second, identifying the optimal subset that con- tains the smallest volume density involves a non-smooth optimization problem which is quite challenging. in this paper, an improved version of sso is developed to overcome the shortcomings of the original sso. this new version of the algorithm, as mentioned earlier, is named isso (improved sso). voronoi tessella- tion is implemented to partition the design space into non-overlapping subregions (a set of voronoi cells) using the pool of samples distrib- uted according to the auxiliary pdf. the admissible set (a set of all admissible subregions) is then defined as a set containing all subsets of the set of voronoi cells. this approach is able to capture the regions with lower objective function values even if they are disjointed or when the design space is complex. the details of the voronoi tessellation are presented in appendix a. a double-sort algorithm is then implemented to identify the optimal subset containing the smallest volume density. in the next section, the original sso is reviewed. section 3 presents the general theoretical and computational framework for the isso al- gorithm. section 4 considers several optimization problems to illustrate the effectiveness and efficiency of the proposed isso algorithm. 2. original stochastic subset optimization in sso, say at the i + 1th iteration, the design space is represented by a subset i(i), where i(i) ∈ i(i − 1)⋅⋅⋅ ∈ i(0) ∈ φ. following the augmented formulation concept initially discussed in [44] for rbdo, the design parameters φ, are artificially considered uncertain variables with a prescribed pdf p(φ|i(i)) over the design space i(i) [45]. for convenience, p(φ|i(i)) = 1/v(i) is considered, where v(i) is the volume of i(i). in this setting of the augmented stochastic design problem, the auxiliary pdf is defined as: π ( φ, θ ⃒⃒i(i)) = h(φ, θ)p ( φ, θ ⃒⃒i(i)) eφ,θ[hs(φ, θ)] ∝h(φ, θ)p ( φ, θ ⃒⃒i(i)) (3) where, p(φ, θ|i(i)) = p(θ|φ)p(φ|i(i)). note that if h(φ, θ)≤ 0, it must be suitably transformed to ensure that π(φ, θ|i(i)) ≥ 0. one way to do this is to define hs(φ,θ) = h(φ, θ) − s, since eθ[hs(φ,θ)] = eθ[h(φ,θ)] − s, that is, the two expected values differ only by a constant, and the optimization of the expected value of h( ⋅ ) is equivalent, in terms of the optimal design choice, to optimization for the expected value for hs( ⋅ ). in the above equation, the denominator is a normalizing constant given by: eφ,θ[h(φ, θ)] = ∫ φ ∫ θ h(φ, θ)p ( φ, θ ⃒⃒i(i)) dθdφ. (4) although this expected value is not explicitly needed, it can be determined using any state-of-the-art stochastic simulation method. the objective function eθ[hs(φ, θ)] in this context of the auxiliary pdf is expressed as: eθ[h(φ, θ)] = π ( φ ⃒⃒i(i)) p ( φ ⃒⃒i(i))eφ,θ[h(φ, θ)], (5) where, the marginal π(φ|i(i)) is given by: π ( φ ⃒⃒i(i)) = ∫ i(i)π(φ, θ)dθ. (6) m.a. khalid and s. bansal3 eθ[h(φ, θ)] is equivalent to minimization of j(φ), which is equal to: j ( φ ⃒⃒i(i)) = eθ[hs(φ, θ)] eφ,θ[hs(φ, θ)] = π ( φ ⃒⃒i(i)) p ( φ ⃒⃒i(i)). (7) the estimation of the marginal π(φ|i(i)) in (7) is necessary to mini- mize j(φ|i(i)). analytical approximations of π(φ|i(i)) based on kernel density approaches or the maximum entropy method might be arduous in case of complex problems, such as when design parameters nφ are large, or the sensitivity for some design parameters is complex [44]. in the sso framework, such approximation of π(φ|i(i)) is avoided. in sso, samples distributed as π(φ|i(i)) are obtained, and the information in these samples is exploited to identify a smaller subset of the design space with a high likelihood of containing the optimal design parameters. samples distributed as π(φ, θ|i(i)) are obtained using any appropriate stochastic sampling algorithm, such as markov chain monte carlo (mcmc) sampling [46]. the φ component of these samples then cor- responds to samples from the marginal distribution π(φ|i(i)). the sensitivity of objective function eθ[hs(φ, θ)] to φ is determined by evaluating the average value (or equivalently volume density) of j(φ| i(i)) over any subset i in i(i), which is denoted by h(i) and defined as: h(i) = 1 vi ∫ i j ( φ|i(i)) dφ = 1 vi ∫ i π ( φ|i(i)) p ( φ|i(i)) dφ = vi(i) vi ∫ i π ( φ|i(i)) dφ (8) where, vi is the volume of subset i. based on the samples distributed according to π(φ|i(i)) belonging to i(i), an estimate of h(i) is provided by: h(i) = ni/vi ni(i)/vi(i), (9) where, ni(i) is the number of samples distributed as π(φ|i(i)) belonging to i(i), and ni denotes the number of samples from π(φ|i(i)) belonging to the i (ni < ni(i−1)since i⊂i(k − 1)). say ni = p0ni(i−1). a smaller value of ρ re- sults in a faster decrease in the size of the identified subsets but with poorer accuracy. the use of ρ equal to 0.1 - 0.2 is suggested in the literature [39]. a deterministic optimization, based on the estimate h(i) of h(i), is next performed to identify the subset i ∈ a(i+1) ρ , where a(i+1) ρ is a set of admissible subsets in i(i), that contains the smallest volume density ni/ vi, that is, i(i+1) = argmin i∈aρh(i) = arg min i∈a(i+1) ρ ni / vi a(i+1) ρ = { i⊂i(i) : ρ = ni / n(i)} . (10) the effectiveness of sso is dependent on the correct selection of the geometrical shape and size of the admissible subsets. choosing a geometrical shape that effectively investigates the sensitivity of the objective function to each design variable is essential. the optimization in (10) determines the subset with the smallest average value of j(φ|i(i)) (or equivalently eθ[hs(φ,θ)]) within the admissible set a(i+1) ρ . i(i + 1) is a subset of the design space i(i) with a high likelihood of containing the optimal design parameters. the above steps are repeated until the stopping criterion is met. this way, sso adaptively converges to a relatively small subregion within the original design space. the imple- mentation of sso is demonstrated in fig. 1. the reader may refer to the original publication for a detailed explanation of sso [39]. h(i(i)) expresses the average relative sensitivity of eθ[h(φ,θ)] to φ. a low value of h(i(i)) indicates that eθ[h(φ,θ)] is more sensitive to φ, and vice versa. a high value of h(i(i)), close to 1 corresponds to a sample density in design space i(i) that approximates a uniform distribution and suggests that the identified subset i(i) has a low likelihood of containing φ* [39]. therefore, the sso is stopped when h(i(i)) exceeds a threshold value. a threshold value of 0.75–0.80 has been found to give satisfactory results [39]. 3. proposed approach in the proposed approach, the voronoi tessellation is implemented to partition the design space into non-overlapping subregions (a set of voronoi cells) using the pool of samples distributed according to this auxiliary pdf. conceptually, voronoi tessellation involves partitioning a space into convex polygons, called voronoi cells, such that each cell contains exactly one sample, called a cell-generating sample. every sample in a given polygon is closer to its generating sample compared to any other. in the proposed approach, the admissible set (a set of all admissible subspaces) is defined as a set containing all subsets of the set of voronoi cells. an alternative approach to identify the optimal subset without performing any non-smooth deterministic optimization is also presented. the general theoretical and computational framework for the isso algorithm is presented in the following subsections, and the m.a. khalid and s. bansal4 algorithm is demonstrated in fig. 2. 3.1. partitioning of design space in the proposed approach, at the i + 1th iteration, say n(i) is the number of samples distributed as π(φ|i(i)) belonging to the design space i(i). let nv = n(i) /(1 + γ), γ ≥ 0 be the number of unique samples. if sampling techniques such as accept rejection, importance sampling, etc., are used, then γ = 0, and each sample in the design space will be unique. however, if mcmc sampling techniques are used, the resulting samples will be correlated, that is γ > 0, and we will have repeated samples. assume that the design space i(i) is divided into v(i) k , k = 1⋅⋅⋅nv, voronoi cells using nv unique samples, and say the voronoi cell v(i) k contains η(i) k repeated samples, then, an estimate of π(φ|i(i)) is provided by: π ( φ|i(i)) = η(i) k n(i)v(i) k ≥ 0, ∀ φ ∈ v(i) k , (11) where, v(i) k is the volume of the kth voronoi cell. obviously, ∫ i(i)π(φ|i(i)) dφ = 1. similar to the original sso, the sensitivity of the objective function j (φ|i(i)) to φ is determined by evaluating the average value of j(φ|i(i)) over any subspace i of the design space i(i). subset i is any subset of nvvoronoi cells (these cells may be disjointed). since the design space is partitioned into nv subspaces or voronoi cells, the number of admissible subsets (proper subsets) is given by 2nν − 1. based on the estimate π(φ|i(i)) provided in (11), an estimate of h(i) is provided as: h(i) = v(i) vi ∫ i π ( φ|i(i)) dφ = v(i) vi ∑ iη(i) i n(i) = v(i) vi ni n(i) (12) where, vi is the volume of the subset i and ni is the number of samples belonging to it. let i = {v(i) (1), v(i) (2)⋯v(i) (s)}, where s is the number of vor- onoi cells defining the subset i. note that the parentheses are used in the subscript to differentiate between the voronoi cell number defined in the previous section from the voronoi cell index describing the subset i. an estimate of h(i) is then provided as: h(i) = v(i) n(i) [ η(i) (1) + η(i) (2) + ⋯ + η(i) (s) v(i) (1) + v(i) (2) + ⋯ + v(i) (s) ] . (13) 3.2. identification of an optimal subset a deterministic optimization needs to be performed to identify a subset i that contains the smallest volume density ni/vi. in the case of unique samples, since η(i) (⋅) = 1, the solution to the minimization problem in (10) is a set of ρn(i) voronoi cells with the largest volume. for the case with repeated samples, the optimization can be performed using methods appropriate for non-smooth optimization problems, such as sub-gradient methods, bundle methods, gradient sampling methods, etc. in this study, we propose an alternative approach to identify the optimal subset without performing any non-smooth deterministic opti- mization. a double-sort algorithm is proposed, which involves sorting the voronoi cells in ascending order of the sample counts and then in groups of cells with the same sample count in descending order of cell volume. finally, the top cells containing ρn(i)samples are selected as an approximate optimal solution from the sorted list. one may argue that the optimal subset can be obtained by first sorting the voronoi cells in ascending order of the cell density, defined as η(i) k /v(i) k , and then by selecting the top cells containing ρn(i) samples from the sorted list. however, this argument is erroneous because the objective is to minimize ∑s s=1η(i) (s)/ ∑s s=1v(i) (s) and not ∑s s=1(η(i) (s) /v(i) (s)). the effectiveness of the proposed double-sort algorithm is demonstrated in section 4 with the help of examples. 3.3. simulation of conditional samples at the i + 1th iteration, ρn(i) samples distributed as π(φ|i(i + 1)) are available from the previous iteration. using these samples as seeds, additional (1 − ρ)n(i + 1) are simulated. the proposed method to simulate additional samples involves two steps: (a) randomly selecting a voronoi cell within the subset i(i + 1) based on the estimate π(φ|i(i)) and (b) applying the metropolis-hastings algorithm within the selected voronoi cell. a voronoi cell is selected according to the following weights in the first step: w(i) k = η(i) k / v(i) k ∑ k η(i) k / v(i) k . (14) m.a. khalid and s. bansal5 to simulate a new sample within a selected voronoi cell, the sample that generated the selected voronoi cell or the last simulated sample in the selected voronoi cell is used as the seed sample, and the metropolis- hastings algorithm is implemented. a candidate sample [φc,θc] is simulated using the proposal q(φc,θc|φ,θ) and is accepted with the probability min(1, a0), where, a0 is given as: a0 = h(φc, θc)p(φc, θc)q(φ, θ|φc, θc) h(φ, θ)p(φ, θ)q(φc, θc|φ, θ) . (15) in the present study, the proposed pdf is equal to the uniform pdf for design parameters and the initial pdf for uncertain variables, i.e., q (φ, θ|φc,θc) = p(φ, θ). therefore, on simplifying (15), a0 is given as: a0 = h(φc, θc) h(φ, θ) . (16) 3.4. stopping criteria a new stopping criterion is proposed in this study. the convergence of the expected value of the performance measure h(φ, θ) with respect to the pdf for φ and θ in consecutive iterations is used as the stopping criterion. mathematically the proposed stopping criterion is represented by: ⃒⃒eφ,θ[h(φ, θ)]i − eφ,θ[h(φ, θ)]i−1⃒⃒ ≤ ε (17) where, ε is a user-specified tolerance limit. other stopping criteria, as indicated in [39,47], can also be chosen. 3.5. implementation issues an important issue for the effective implementation of the isso is the creation of the voronoi cells at the current iteration bounded within the voronoi cell created at the previous iterations. although it is possible to create such bounded voronoi cells, due to the geometrical complexities, it is usually unfeasible for the higher dimensional problems (nφ>2). an alternative approach is proposed in the present study for creating the voronoi cells at any iteration of the isso. the proposed approach in- volves creating voronoi cells using the samples generated at the current and all previous iterations and then by considering voronoi cells cor- responding to the samples from the current iteration. this is shown in fig. 3, where fig. 3(a) shows the n samples at the first iteration and the corresponding voronoi cells. fig. 3(b) shows the ρn selected voronoi cells leading to the smallest volume density and the additional (1 − ρ)n samples being generated using these ρn samples as seeds. fig. 3(c) shows that the voronoi cells are generated using all n + (1 − ρ)n samples that are generated in the two iterations. the voronoi cells corresponding to the n samples for consideration at the second iteration are also highlighted in fig. 3(c). fig. 3(d) shows a zoomed-in version of fig. 3(c) where it can be observed that the area covered by the n voronoi cells considered in the second iteration is not the same as the area covered by the ρn voronoi cells selected in the first iteration. on the contrary, the area covered by the voronoi cells in the second iteration is more than the area covered by the voronoi cells corresponding to the seed samples from the first iteration. this is because a new sample within the voronoi cell between an existing sample and the existing voronoi cell edge results in the relocation of the voronoi cell edge in a m.a. khalid and s. bansal6 direction away from the new sample. the increase at each iteration in- troduces a bias in the estimate of π(φ|i(i)) in (11). however, this does not affect the performance of the proposed approach as the objective is not to simulate the samples distributed as π(φ|i(i)) but to identify the subsets for an optimal solution. in addition, the increase is not substantial, as seen later in the illustrative examples in section 4. 3.6. special case: deterministic optimization in the isso framework, a deterministic optimization problems can also be handled with the vector of uncertain variables θ set equal to a null vector (nθ = 0). since the determination of the subset at each isso iteration is solely dependent on the samples distributed as π(φ), no modification to the isso algorithm is required to solve a deterministic optimization problem, and the entire formulation remains valid. 4. illustrative examples in this section, typical optimization problems are considered to demonstrate the effectiveness and efficiency of the proposed approach. first, deterministic optimization problems are considered. these prob- lems include several local and global minima. next, stochastic optimi- zation problems are illustrated. the second example presents an rdo problem of the tmd. in this example, the variance minimization of the protected structure’s displacement (tmd attached to the structure) is performed. in the third example, the mean minimization of 120 bars truss problems is explored to demonstrate the applicability of the pro- posed approach to a high-dimensional stochastic design problem. finally, the fourth example investigates the reliability-based optimiza- tion of a base isolation system for a 10-story building. in this study, after implementing isso, the optimal design solution is identified as follows. let θj, j = 1⋅⋅⋅n be a set of independent, identically distributed realizations of θ, and let h(φ, θj) be the structural perfor- mance function realization for θj. the expected structural performance function is approximated by the average of the realizations as: eθ[h(φ, θ)] ≈ 1 n ∑ n j=1 h ( φ, θj ) . (18) eθ[h(φ, θ)] is evaluated for all unique φ samples obtained at the last iteration of the isso, and the φ sample resulting in the smallest value of eθ[h(φ, θ)] is taken as the optimal solution. alternatively, as the right- hand side of (18) is deterministic, any deterministic optimization method can also be used to solve the optimization problem with the approximate expectation. in the following examples, both isso and sso are implemented with n = 1000nφ, ρ = 0.20 and the stopping criteria as stated in (17). here, a value of ε = 10−3 is adopted. m.a. khalid and s. bansal7 4.1. multimodal deterministic optimization problems in this section, three two-dimensional benchmark deterministic optimization problems are considered. results are also compared with the sso. the test functions are: a) griewank function: minh(φ) = ∑ d i=1 φ2 i 4000 − ∏ d i=1 cos (φi̅̅ i √ ) + 1, s.t.φ = [ − 10, 10] (19) b) cross-in-tray function: minh(φ) = −0.0001 (⃒⃒⃒⃒⃒sin(φ1)sin(φ2)exp (⃒⃒⃒⃒⃒100 − ̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅ φ2 1 + φ2 2 √ π ⃒⃒⃒⃒⃒ )⃒⃒⃒⃒⃒ + 1 )0.1 , s.t.φ = [ − 10, 10] (20) c) holder table function: minh(φ) = − ⃒⃒⃒⃒⃒sin(φ1)cos(φ2)exp (⃒⃒⃒⃒⃒1 − ̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅ φ2 1 + φ2 2 √ π ⃒⃒⃒⃒⃒ )⃒⃒⃒⃒⃒, s.t.φ = [ − 10, 10] (21)) the results for the griewank function are presented in fig. 4. fig. 4(a, b) shows that the function has multiple closely spaced local minima with a single global minimum. fig. 4(c, d) shows the sso optimization using hyper-rectangle and hyper-ellipse as shapes of admissible subsets. it is seen that these shapes fail to capture the region containing the optimal design due to the presence of multiple local minima. next, the isso is implemented, where the voronoi cells selected at the first and last iteration are shown in fig. 4(e, f). it is observed that at the first iteration, the selected voronoi cells effectively capture both the local and global minima and in the subsequent iterations, the selected cells are more concentrated near the global minimum. the region selected at the last iteration captures the optimal global solution. the cross-in-tray function has a relatively complex design space compared to the griewank function. fig. 5(a, b) shows multiple local and global minima. minimization by using sso is demonstrated in fig. 5 (c, d). it is found that both the hyper-rectangle and hyper-ellipse are trapped around any one of the global minima. at the same time, the isso is able to capture the regions that include all of the global minima, as m.a. khalid and s. bansal8 seen in fig. 5(e, f). the holder table function has multiple local and global minima; the global minima are placed at the boundary of the design space, as shown in fig. 6(a, b). once again, it is seen that both the hyper-rectangle and hyper-ellipse are trapped around any of one of the global minima, and on the other hand, the isso is able to capture the regions that include all of the global minima, as seen in fig. 6(e, f). the results from the three examples demonstrate that the proposed isso is able to capture the regions containing the optimal solution effectively. next, the statistics of the results of 50 independent runs, both for sso and isso are presented in table 1. it also includes the results obtained by using state-of-the-art approaches, such as the genetic algorithm, particle swarm optimization, and the gradient based optimization approach (interior-point algorithm). the proposed isso outperforms all other approaches as more successes in determining the optimal solution are observed in all three optimization problems. it is also seen that both sso and isso result in a similar value of volume reduction for the same stopping criterion; however, with sso, the number of iterations required to achieve this volume reduction are relatively higher. the proposed approach outperformed the state-of-the-art approaches, as indicated by the number of successes. these examples demonstrate that the main advantage of implementing voronoi tessellation is an effective explo- ration of the design space. next, the performance of the proposed \"double sort algorithm\" for selecting the optimal subset is studied by using the above-mentioned three functions. fig. 7 shows the value of h(i(1)) for the 50 indepen- dent simulation runs, which is estimated by implementing the proposed double sort algorithm and by using the genetic algorithm. it can be noted that for each run, the h(i(1)) values obtained using the proposed double sort algorithm and genetic algorithm are well matched, thereby confirming the adequacy of the proposed double sort algorithm. at any iteration of isso, new samples are simulated using the seed samples. in the proposed approach, the volume of the voronoi cells corresponding to the seed and new samples is greater than the volume of the voronoi cells corresponding only to the seed samples. fig. 8 shows this change in volume v(seeds+new)−v(seeds) v(seeds) due to the creation of voronoi cells at any generation of isso using the procedure mentioned in section 3.4. the increase is observed to be small which further reduces with an in- crease in the iteration number. it is also observed that the increase in volume decreases with an increase in sample size at each iteration and increases with an increase in the dimension of the problem. 4.2. robust design optimization of the tuned mass damper this example considers a stochastic design problem involving a tuned mass damper (tmd) attached to a single degree of freedom (sdof) system. the problem is taken from [48] and is shown in fig. 9. in this problem, the system is excited by a white noise signal with a mean zero and unit variance. the performance measure is the variance of the displacement of the system σ2 xs. the mass ms, stiffness ks, and m.a. khalid and s. bansal9 damping cs of the system are taken as uncertain parameters, following independent gaussian distribution. the mean value of these variables is taken to be 105 kg, 107 n/m, and 4 × 104 ns/m respectively. to account for uncertainty, the c.o.v value for each variable taken is 0.05. the frequency ratio β = ωt /ωs and damping ξt of the tmd are considered design parameters. the tmd has a mass ratio, mt/ms, of 0.10. the parameters mt,ωt,andωs are, in order, the mass of the tmd, the natural frequency of the tmd ( ̅̅̅̅̅̅̅̅̅̅̅̅̅̅ kt/mt √ ), and the natural frequency of the structure ( ̅̅̅̅̅̅̅̅̅̅̅̅ ks/ms √ ). the optimization problem is written as: minimize : φ∈φ,φ∈φ eθ[h(θ, φ, φ)] = eθ [( σ2 xs(θ, φ) − φ )]2 , (22) table 1 statistics of optimization results for multimodal deterministic optimization problems. example sso isso ga* pso* gba* hyper-rectangle hyper-ellipse griewank nf 30 32 5 38 35 47 ns 20 18 45 12 15 3 bv 0 0 0 0 0 0 wv 0.1028 0.1161 0.0270 0.0296 0.0232 0.0296 av 0.0190 0.0292 0.0110 0.0094 0.0057 0.0173 c.o.v 1.1663 1.0168 0.5137 0.9603 0.8763 0.05971 fe 22,702 15,223 12,426 3385 1432 33 gen 7 5 4 n/a vr 94.025 84.79 93.56 cross-in-tray nf 50 50 1 50 50 50 ns 0 0 49 0 0 0 bv −2.0576 −2.0626 −2.0624 −2.0626 −2.0626 −2.0626 wv −2.0472 −2.0481 −2.0260 −2.0626 −2.0626 −1.3853 av −2.0527 −2.0621 −2.0522 −2.0626 −2.0626 −1.7360 c.o.v 0.0133 0.001 0.0042 0 0 0.0977 fe 27,563 21,595 9982 3178 933 32 gen 9 7 3 n/a vr 99.89 98.95 88.71 holder-table nf 50 50 3 50 50 50 ns 0 0 47 0 0 0 bv −19.2085 −17.5025 −19.2085 −19.2085 −19.2085 −19.2085 wv −18.8916 −1.1419 −17.3030 −9.5047 −15.1402 −1.1831 av −19.0916 −8.432 −18.8798 −19.0144 −18.9745 −6.5493 c.o.v 0.0025 0.3898 0.0182 0.0722 0.0443 0.8358 fe 40,700 24,684 18,142 3413 988 30 gen 13 8 6 n/a vr 99.59 99.42 94.37 ga = genetic algorithm, pso = particle swarm optimization, gba = gradient-based optimization approach, nf = no. of. failure, ns = no. of. success, bv = best value, wv = worst value, av = average value, c.o.v = coefficient of variation, fe = no. of. function evaluations, gen = generations, vr = volume reduction percentage, * = efficiently applicable only for deterministic problems. fig. 7. comparison of double sort algorithm and genetic algorithm results. m.a. khalid and s. bansal10 where, 0.01 ≤ β ≤ 1.5, 0.01 ≤ ξt ≤ 1.0, 0 ≤ φ ≤ 1000. (23) table 2 presents the optimal design parameter values as well as the objective function value that solve the optimization problem in (22). results obtained using sso, sample average approximation (saa), and isso are shown. saa is applied with a sample size of 103, as mentioned in [43]. the results demonstrate that isso is effective in locating the optimal solution. sso implemented with hyper-ellipse gives an optimal solution but has a higher computational cost. 4.3. 120-bars truss structure the third example involves minimizing the mean of the compliance of a 120-bar linear elastic truss structure shown in fig. 10 under the weight constraint w ≤ 15, 000kg. because of structural symmetry, design parameters corresponding to the cross-sectional areas of elements are divided into seven groups, each with a minimum area of 10−4 m2. the young’s modulus for the bar groups are assumed as uncorrelated normal random variables with mean values equal to 210 gpa and the c. o.v equal to 0.10 respectively. the density of the material is 7971.89 kg/ m3. the dome is subjected to concentrated vertical loads acting down- ward at the top node, normally distributed with a mean equal to 60 kn and c.o.v equal to 0.20. in addition, the mass of bars is concentrated at the nodes. the problem is taken from [48]. table 3 presents the best of 10 independent run results obtained with sso and isso. once again, the sso and isso solutions agree well, thereby demonstrating the effectiveness of the proposed approach. at the same time, the number of function evaluations is substantially less in the case of isso, indicating the efficiency of the proposed approach. 4.4. reliability-based design of a base isolated structure this example, adapted from [49], involves the reliability-based fig. 9. tmd attached to a sdof system [48]. table 2 variance minimization of tmd-structure. method admissible subset shape design parameters eθ[h(θ,φ,φ)] (× 10−16 mm4) fe ns nf β ξt sso [48] hyper-rectangle 0.551 0.623 41.324 7433 0 50 hyper-ellipse 0.749 0.221 1.7586 8245 34 16 saa n/a 0.749 0.221 1.7587 3 × 106 4 46 isso voronoi tessellation 0.749 0.221 1.7586 6198 50 0 fig. 10. 120-bar dome truss structure [48]. table 3 results for the 120 bars truss structure. method design parameters μg(̃φ∗) (nm) σ2 g (̃φ∗) (nm)2 fe a1 (cm2) a2 (cm2) a3 (cm2) a4 (cm2) a5 (cm2) a6 (cm2) a7 (cm2) sso [48] 47.2 67.5 42.7 9.4 30.1 55.6 13.1 242.3 5294.2 54,700 saa 47.3 68.3 40.7 10.5 30.3 49.5 14.8 243.5 5407.4 3 × 106 m.a. khalid and s. bansal11 optimization of a base-isolation system attached to a 10-story building as shown in fig. 11. this optimization problem includes maximizing the reliability of the base-isolated structure which is performed by the minimization of its failure probability and mathematically expressed as: minimize : φ∈φ p(f|φ) = eθ[if(φ, θ)] = ∫ θ if(φ, θ)p(φ, θ)dθ, (24) where, if(φ,θ) is the function that indicates failure, and it equals 1 when the system fails, i.e., when unacceptable performance occurs. notably, in this problem h(φ, θ) = if(φ,θ). the 10-story building is considered as a shear structure with un- certain inter-story stiffness and damping. each story has a total mass of 207 ton. the inter-story stiffness ki of all stories are parameterized by ki = ̂kiθi, i = 1, …, 10 where the most probable values of the inter-story stiffness are [̂ki] = [687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6, 225.6, 184.5, 104.5] mn/m. the entity θi is a set of non-dimensional uncertain variables that are considered to be correlated gaussian vari- ables with a unit mean value ̂θi = 1, ∀i and a covariance matrix defined as: e [ (θi − ̂θi) ( θj − ̂θj )] = (0.2)2exp [ − (j − i)2 / 22] . (25) the damping ratios are considered independent gaussian variables with mean values of 0.025 and c.o.v of 0.10 for all modes. the kanai- tajimi model is used to simulate the ground excitation modelled as a filtered white noise process, with the power spectral density function given as: s(ω) = s0 ω4 g + 4ζ2 gω2 gω2 ( ω2 g − ω2 )2 + 4ζ2 gω2 gω2 , (26) s0 = σ2 ω 2ζg πωg ( 4ζ2 g + 1 )m2 / s3, (27) where, ωg, ζgand σω are the resonant frequency, damping, and rms of the acceleration input of the filter, respectively. these are also considered uncertain variables with mean values of [2πrad/s, 0.5, 0.2g] and a c.o.v equal to 0.20. the non-stationarity of the excitation is modeled by multiplying the filter output with the envelope function as: e(t) = λ3tλ1exp( − λ2t), (28) with parameters λ1 = 1.25, λ2 = 0.2 and λ3 = 0.353 chosen to simulate strong earthquake excitation for a duration of 40 s with a sampling time of 0.02 s. the base-isolation system considered is a lead–rubber bilinear isolator with an additional viscous damper. the base has a 247-ton mass. the design parameters φ for the base isolation structure system are the stiffness before yielding kprand after yielding kp, the yield force is fy, and the damping coefficient cd. the reader may refer to [39,50] for additional details regarding the base isolation structure system adopted in this study. failure is indicated when any of the normalized base displacements or inter-story drifts exceeds unity. the normalization constants are 0.5 m and 0.033 m respectively. the design interval for each variable is specified as kpr = [50, 600] mn/m, fy = [1, 8] mn, kp = [5, 60] mn/m, and cd = [0.1, 10]mns/m. in this example, isso and sso are imple- mented with six number of iterations. table 4 shows the optimization results for the best 10 independent simulation runs. the comparison of the results obtained using sso, saa (with a sample size of 103), and isso shows that the optimal design obtained using the proposed approach isso is in good agreement. the failure probability of the structure is reduced from 0.95 (without the base isolation system) to 0.0326 after installing the optimally designed base isolation system. 5. conclusion this study attempts to provide an optimization approach called \"isso\", which is an improved version of sso, primarily for stochastic optimization problems while it retains utility for deterministic optimi- zation problems as well. two novel ideas are introduced in this study: first, a better characterization of the design space is offered by parti- tioning the design space into non-overlapping subregions using voronoi fig. 11. (left) 10-story base isolated shear model, and (right) force-deformation of bilinear isolator [49]. table 4 base isolation structure system optimization results (best of 10 independent runs). method design parameters (φ*) failure probability pf(φ*) kpr (mn/ m) fy (mn) kp (mn/ m) cd (mns/ m) sso [39] 425.33 1.20 15.52 6.54 0.0340 m.a. khalid and s. bansal12 tessellation which improves the effectiveness and efficiency of the pro- posed isso considerably in comparison to sso. second, a novel \"double sort\" approach is proposed, eliminating the need for optimization to identify the subregions for the optimal design at each isso iteration. several mathematical and engineering design examples, including tmd, 120 bars truss structure, and base-isolated structure, are included in this study to demonstrate the efficacy of the proposed isso. the results show that the proposed isso effectively identifies the reduced design space for complex design problems with multiple global and local minima. this is attributable to the voronoi tessellation, which eliminates the require- ment of the presumed admissible design space form to resemble the contour of the original design. voronoi tessellation enabled better design space exploration, allowing multiple global minima scattered throughout the design pace to be effectively identified. due to the dis- cretization of the design space via voronoi tessellation, computation demand is significantly reduced as the number of function evaluations for all examples is lower vis-a-vis the original sso. moreover, the novel idea of the double sort approach achieves the requisite precision in identifying the subregions for optimal solutions and makes isso implementation simple and effective. the applicability of the approach is dependent on the creation of the voronoi cells. at present the methods available in the literation for creating the voronoi tessellation are computationally demanding when considering problems of very high dimension. future work will focus on developing a method for creating the voronoi tessellation in higher di- mensions, particularly those greater than ten. credit authorship contribution statement mohd aman khalid: investigation, methodology, formal analysis, software, visualization, writing – original draft. sahil bansal: conceptualization, methodology, supervision. declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. data availability no data was used for the research described in the article. appendix-a: voronoi tessellation voronoi tessellation is a mathematical concept named after the russian mathematician georgy voronoi. it is also known as the voronoi diagram or dirichlet tessellation. a voronoi tessellation of a set of points p in a plane is a partition of the plane into a set of non-overlapping convex polygons, with each polygon including precisely one point of p and each point in a polygon being closer to its associated point in p than to any other point in p. each polygon is referred to as a voronoi cell or a dirichlet region. the boundary of each cell is constituted of points that are equidistant to two or more points in p. fig. 12 shows the voronoi diagram in a two-dimensional design space. fig. 12. voronoi diagram in 2-dimensional space. there are several efficient algorithms for creating voronoi diagrams. one such basic algorithm is to start with a set of points and then compute the voronoi cells by dividing the space into regions based on the distance to the nearest point. the bowyer-watson algorithm [51], which generates a delaunay triangulation in any number of dimensions, can be applied while creating a voronoi diagram. the delaunay triangulation is a triangulation of the point in which no point falls within the circumcircle of any triangle. the polygon generated by the intersection of the half-planes defined by the edges of the delaunay triangles enclosing the point is therefore obtained as the voronoi cell of a point. it can be summarized that voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to a set of points. voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and microstructure study [52]. the matlab command \"voronoin\" from the \"parallel computing toolbox\" [54] has been used in this study to create the voronoi cells.",
                    "references": "references [1] marti k. stochastic optimization methods. berlin: springer; 2008. [2] tsompanakis y, lagaros nd, papadrakakis m. structural design optimization considering uncertainties. crc press; 2008. [3] khalid ma, bansal s, ramamohan v. an augmented formulation for robust design optimization of structures using stochastic simulation method. res eng des 2023; 34:179–200. https://doi.org/10.1007/s00163-022-00405-z. [4] meng z, li g, wang x, sait sm, rıza a. a comparative study of metaheuristic algorithms for reliability ‑ based design optimization problems. arch comput methods eng 2021;28:1853–69. https://doi.org/10.1007/s11831-020-09443-z. [5] abualigah l, elaziz ma, khasawneh am, alshinwan m. meta-heuristic optimization algorithms for solving real-world mechanical engineering design problems : a comprehensive survey, applications, comparative analysis, and results. neural comput appl 2022;34:4081–110. https://doi.org/10.1007/s00521- 021-06747-4. [6] katebi j, shoaei m, nguyen s, trung t, khorami m. developed comparative analysis of metaheuristic optimization algorithms for optimal active control of m.a. khalid and s. bansal13 structures. eng comput 2020;36:1539–58. https://doi.org/10.1007/s00366-019- 00780-7. [7] alorf a. engineering applications of artificial intelligence a survey of recently developed metaheuristics and their comparative analysis. eng appl artif intell 2023;117:105622. https://doi.org/10.1016/j.engappai.2022.105622. [8] kirsch u. structural optimization: fundamentals and applications. springer-verlag; 2012. [9] floudas ca, pardalos pa. encyclopedia of optimization. springer; 2008. [10] kiureghian ad, ditlevsen o. aleatory or epistemic? does it matter? struct saf 2009;31:105–12. https://doi.org/10.1016/j.strusafe.2008.06.020. [11] schu¨eller gi, jensen ha. computational methods in optimization considering uncertainties - an overview. comput methods appl mech eng 2008;198:2–13. https://doi.org/10.1016/j.cma.2008.05.004. [12] schneider j, kirkpatrick s. stochastic optimization. springer; 2007. [13] do b, ohsaki m. a random search for discrete robust design optimization of linear- elastic steel frames under interval parametric uncertainty. comput struct 2021; 249:106506. https://doi.org/10.1016/j.compstruc.2021.106506. [14] asadpoure a, tootkaboni m, guest jk. robust topology optimization of structures with uncertainties in stiffness - application to truss structures. comput struct 2011. https://doi.org/10.1016/j.compstruc.2010.11.004. [15] doltsinis i, kang z. robust design of structures using optimization methods. comput methods appl mech eng 2004;193:2221–37. https://doi.org/10.1016/j. cma.2003.12.055. [16] carneiro g, das n, ant´onio cc. dimensional reduction applied to the reliability- based robust design optimization of composite structures. compos struct 2021; 255. https://doi.org/10.1016/j.compstruct.2020.112937. [17] an h, youn bd, kim hs. reliability-based design optimization of laminated composite structures under delamination and material property uncertainties. int j mech sci 2021. https://doi.org/10.1016/j.ijmecsci.2021.106561. [18] li z, duan lb, cheng ag, yao zp, chen t, yao w. lightweight and crashworthiness design of an electric vehicle using a six-sigma robust design optimization method. eng optim 2019. https://doi.org/10.1080/0305215x.2018.1521396. [19] gholinezhad h, torabi sh. reliability-based multidisciplinary design optimization of an underwater vehicle including cost analysis. j mar sci technol 2021. https:// doi.org/10.1007/s00773-021-00804-2. [20] lee kh, park gj. robust optimization considering tolerances of design variables. comput struct 2001;79:77–86. https://doi.org/10.1016/s0045-7949(00)00117-6. [21] anderson tv, mattson ca. propagating skewness and kurtosis through engineering models for low-cost, meaningful, nondeterministic design. j mech des trans asme. 2012. https://doi.org/10.1115/1.4007389. [22] zhou q, wang y, choi sk, jiang p, shao x, hu j, shu l. a robust optimization approach based on multi-fidelity metamodel. struct multidiscip optim 2018. https://doi.org/10.1007/s00158-017-1783-4. [23] wang gg, shan s. review of metamodeling techniques in support of engineering design optimization. j mech des trans asme. 2007;129:370–80. https://doi.org/ 10.1115/1.2429697. [24] chatterjee t, chakraborty s, chowdhury r. a critical review of surrogate assisted robust design optimization. arch comput methods eng 2019;26:245–74. https:// doi.org/10.1007/s11831-017-9240-5. [25] chatterjee t, friswell mi, adhikari s, chowdhury r. a global two-layer meta- model for response statistics in robust design optimization. eng optim 2021. https://doi.org/10.1080/0305215x.2020.1861262. [26] guo x, zhao x, zhang w, yan j, sun g. multi-scale robust design and optimization considering load uncertainties. comput methods appl mech eng 2015;283: 994–1009. https://doi.org/10.1016/j.cma.2014.10.014. [27] jerez dj, jensen ha, beer m. reliability-based design optimization of structural systems under stochastic excitation: an overview. mech syst signal process 2022. https://doi.org/10.1016/j.ymssp.2021.108397. [28] li w, gao l, xiao m. multidisciplinary robust design optimization under parameter and model uncertainties. eng optim 2020;52:426–45. https://doi.org/10.1080/ 0305215x.2019.1590564. [29] beyer hg, sendhoff b. robust optimization - a comprehensive survey. comput methods appl mech eng 2007;196:3190–218. https://doi.org/10.1016/j. cma.2007.03.003. [30] motta r, de s, afonso smb. an efficient procedure for structural reliability-based robust design optimization. struct multidiscip optim 2016;54:511–30. https://doi. org/10.1007/s00158-016-1418-1. [31] yildiz ar. comparison of evolutionary-based optimization algorithms for structural design optimization. eng appl artif intell 2013;26:327–33. https://doi. org/10.1016/j.engappai.2012.05.014. [32] beck at, gomes wjds. a comparison of deterministic, reliability-based and risk- based structural optimization under uncertainty. probab eng mech 2012;28:18–29. https://doi.org/10.1016/j.probengmech.2011.08.007. [33] acar, e., bayrak, g., jung, y., lee, i., ramu, p., ravichandran, s.s.: modeling, analysis, and optimization under uncertainties: a review, (2021). 10.1007/s001 58-021-03026-7. [34] georghiou a, kuhn d, wiesemann w. the decision rule approach to optimization under uncertainty: methodology and applications. comput manag sci 2019. https://doi.org/10.1007/s10287-018-0338-5. [35] braydi o, lafon p, younes r. study of uncertainties and objective function modeling effects on probabilistic optimization results. asce asme j risk uncertain eng syst part b mech eng 2019. https://doi.org/10.1115/1.4044152. [36] liu ws, cheung sh. reliability based design optimization with approximate failure probability function in partitioned design space. reliab eng syst saf 2017;167: 602–11. https://doi.org/10.1016/j.ress.2017.07.007. [37] chiralaksanakul a, mahadevan s. first-order approximation methods in reliability- based design optimization. j mech des trans asme 2005. https://doi.org/ 10.1115/1.1899691. [38] doltsinis i, kang z, cheng g. robust design of non-linear structures using optimization methods. comput methods appl mech eng 2005;194:1779–95. https://doi.org/10.1016/j.cma.2004.02.027. [39] taflanidis aa, beck jl. stochastic subset optimization for optimal reliability problems. probab eng mech 2008. https://doi.org/10.1016/j. probengmech.2007.12.011. [40] au sk, beck jl. estimation of small failure probabilities in high dimensions by subset simulation. probab eng mech 2001;16:263–77. https://doi.org/10.1016/ s0266-8920(01)00019-4. [41] jia gf, taflanidis aa. non-parametric stochastic subset optimization for optimal- reliability design problems. comput struct 2013;126:86–99. https://doi.org/ 10.1016/j.compstruc.2012.12.009. [42] taflanidis aa. stochastic subset optimization incorporating moving least squares response surface methodologies for stochastic sampling. adv eng softw 2012;44: 3–14. https://doi.org/10.1016/j.advengsoft.2011.07.009. [43] khalid ma, bansal s. framework for robust design optimization of tuned mass dampers by stochastic subset optimization. int j struct stab dyn 2023;23. https:// doi.org/10.1142/s0219455423501559. [44] au sk. reliability-based design sensitivity by efficient simulation. comput struct 2005;83:1048–61. [45] taflanidis aa, beck jl. an efficient framework for optimal robust stochastic system design using stochastic simulation. comput methods appl mech eng 2008. https:// doi.org/10.1016/j.cma.2008.03.029. [46] robert cp, casella g. monte carlo statistical methods. new york, ny: springer; 2004. [47] li hs. subset simulation for unconstrained global optimization. appl math model 2011;35:5108–20. https://doi.org/10.1016/j.apm.2011.04.023. [48] khalid ma, bansal s, ramamohan v. an augmented formulation for robust design optimization of structures using stochastic simulation method. res eng des 2022. https://doi.org/10.1007/s00163-022-00405-z. [49] taflanidis aa, beck jl. an efficient framework for optimal robust stochastic system design using stochastic simulation. comput methods appl mech eng 2008;198: 88–101. https://doi.org/10.1016/j.cma.2008.03.029. [50] kandemir ec, mortazavi a. optimization of seismic base isolation system using a fuzzy reinforced swarm intelligence. adv eng softw 2022;174:103323. https://doi. org/10.1016/j.advengsoft.2022.103323. [51] rebay s. efficient unstructured mesh generation by means of delaunay triangulation and bowyer-watson algorithm. j comput phys 1993;106:125–38. [52] wade n, graham-brady l. estimating microstructural feature distributions from image data using a bayesian framework. j microsc 2023:1–16. https://doi.org/ 10.1111/jmi.13184. [53] duan x, li l, ge y, liu b. exact voronoi diagram for topographic spatial analysis. giscience remote sens 2023;60. https://doi.org/10.1080/ 15481603.2023.2171703. [54] matlab and parallel computing toolbox release. natick, massachusetts, united states: the mathworks, inc.; 2021.  m.a. khalid and s. bansal",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1sgq4PBI2h1px9oNcqFPDbpklNcpTB0Cz&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "1",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "abstract.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "1",
                    "title": "AI model for computer games based on case based reasoning and ai planning ",
                    "institutions": "athens information technology 0.8km markopoulou ave. peania, 19002, greece dimitrios metafas athens information technology 0.8km markopoulou ave.  peania, 19002, greece",
                    "authors": "vlado menkovski",
                    "abstract": "making efficient ai models for games with imperfect information can be a particular challenge. considering the large number of possible moves and the incorporated uncertainties building game trees for these games becomes very difficult due to the exponential growth of the number of nodes at each level. this effort is focused on presenting a method of combined case based reasoning (cbr) with ai planning which drastically reduces the size of game trees. instead of looking at all possible combinations we can focus only on the moves that lead us to specific strategies in effect discarding meaningless moves. these strategies are selected by finding similarities to cases in the cbr database. the strategies are formed by a set of desired goals. the ai planning is responsible for creating a plan to reach these goals. the plan is basically a set of moves that brings the player to this goal. by following these steps and not regarding the vast number of other possible moves the model develops game trees which grows slower so they can be built with more feature moves restricted by the same amount of memory. categories and subject descriptors i.2.1 [applications and expert systems]: games general terms algorithms, performance.",
                    "keywords": "game ai, case based reasoning, ai planning, game trees  1.",
                    "content": "the goal of this effort is to explore a model for design and implementation of an ai agent for turn based games. this model provides for building more capable computer opponents that rely on strategies that closely resemble human approach in solving problems opposed to classical computational centric heuristics in game ai. in this manner the computational resources can be focused on more sensible strategies for the game play. with the advancement in computer hardware increasingly more computing power is left for executing ai algorithms in games. in the past ai in games was mainly a cheating set of instructions that simulated the increasing difficulty in the game environment so that the player had the illusion of real counterpart. improvement in available memory and processing power allows implementation of more intelligent algorithms for building the game environment as well as direct interaction with the human players. in this particular research the emphasis is put on the interaction between the ai agent and a computer player in the realm of the game rules. it is particularly focused on turn based games that have the elements of uncertainty like dice or concealed information. at the beginning a description of game ai algorithms are given; such as game trees and minimax. the following section describes an approach of using ai planning to improve building game trees in games with imperfect information where game trees tend to be very large with high growth ratio. section 4 discusses another approach that provides a significant reduction to the number of considered moves in order to find the favorable strategy of the ai player. this approach uses ai planning techniques and case base reasoning (cbr) to plan for different scenarios in predetermined strategies which would be analogous to human player experience in the particular game. the cbr database illustrates a set of past experiences for the ai problem and the ai planning illustrates the procedure to deal with the given situation in the game. in the next two sections implementations and evaluations of both approaches are given. the ai planning approach is implemented with the tic-tac-toe game and the combined ai planning and cbr approach is implemented with a model for the monopoly game. the last part contains conclusions and future work ideas. 2. game trees and minimax game trees are common model for evaluating how different combinations of moves from the player and his opponents will affect the future position of the player and eventually the end result of the game. an algorithm that decides on the next move by evaluating the results from the built game tree is minimax [1]. minimax assumes that the player at hand will always choose the best possible move for him, in other words the player will try to select the move that maximizes the result of the evaluation function over the game state. so basically the player at hand needs to choose the best move overall while taking into account that the next player(s) will try to do the same thing. minimax tries to maximize the minimum gain. minimax can be applied to multiple permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. dimea’08, september 10–12, 2008, athens, greece. copyright 2008 acm 978-1-60558-248-1/08/09... $5.00 interactive and adaptable media 295 the minimax theorem states: for every two-person, zero-sum game there is a mixed strategy for each player, such that the expected payoff for both is the same value v when the players use these strategies. furthermore, v is the best payoff each can expect to receive from a play of the game; that is, these mixed strategies are the optimal strategies for the two players. this theorem was established by john von neumann, who is quoted as saying \"as far as i can see, there could be no theory of games … without that theorem … i thought there was nothing worth publishing until the minimax theorem was proved\" [2]. a simple example of minimax can be observed by building a game tree of the tic-tac-toe game. the tic-tac-toe game is a simple game which can end by the first player wining, the second player wining or a tie. there are nine positions for each of the players in which at each turn the player puts x or o sign. if the player has three adjacent signs in a row, column or the two diagonals he or she wins. this game has limited number of position and it is well suited for building the whole game tree. the leaves of this tree will be final positions in the game. a heuristics evaluation function will also need to be written to evaluate the value of each node along the way. 3. ai planning for building game trees 3.1.1 ai planning ai planning also referred as automated planning and scheduling is a branch of artificial intelligence that focuses on finding strategies or sequences of actions that reach a predefined goal [3]. typical execution of ai planning algorithms is by intelligent agents, autonomous robots and unmanned vehicles. opposed to classical control or classification ai planning results with complex solutions that are derived from multidimensional space. ai planning algorithms are also common in the video game development. they solve broad range of problems from path finding to action planning. a typical planner takes three inputs: a description of the initial state of the world, a description of the desired goal, and a set of possible actions. some efforts for incorporating planning techniques for building game trees have also shown up, similar to the approach explored in this effort. in addition cased based reasoning [4] techniques are also gathering popularity in developing strategies based in prior knowledge about the problems in the games. one of the benefits from hierarchical task network (htn) [5] planning is the possibility to build game trees based on htn plans; this method is described in the following section. 3.2 game trees with ai planning an adaptation of the htn planning can be used to build much smaller and more efficient game trees. this idea has already been implemented in the bridge baron a computer program for the game of contact bridge [6]. computer programs based on game tree search techniques are now as good as or better than humans in many games like chess [7] and checkers [8], but there are some difficulties in building a game tree for games that have imperfect information and added uncertainty like card or games with dice. the main problem is the enormous number of possibilities that the player can choose from in making his move. in addition some of the moves are accompanied with probabilities based on the random elements in the games. the number of possible moves exponentially grows with each move so the depth of the search has to be very limited to accommodate for the memory limitations. the basic idea behind using htn for building game trees is that the htn provides the means of expressing high level goals and describing strategies how to reach those goals. these goals may be decomposed in goals at lower level called sub-goals. this approach closely resembles the way a human player usually addresses a complex problem. it is also good for domains where classical search for solution is not feasible due to the vastness of the problem domain or uncertainties. 3.2.1 hierarchical task networks the hierarchical task network, or htn, is an approach to automated planning in which the dependency among actions can be given in the form of networks [9] [figure 1]. a simple task network (or just a task network for short) is an acyclic digraph � � ��� �� in which u is the node set, e is the edge set, and each node � � � contains a task ��. the edges of � define a partial ordering of u. if the partial ordering is total, then we say that � is totally ordered, in which case � can be written as a sequence of tasks � � ���� ��� � � ���. figure 1: simple hierarchical task network a simple task network (stn) method is a 4-tuple of its name, task, precondition and a task network. the name of the method lets us refer unambiguously to substitution instances of the method, without having to write the preconditions and effects explicitly. the task tells what kind of task can be applied if the preconditions are met. the preconditions specify the conditions that the current state needs to satisfy in order for the method to be applied. and the network defines the specific subtasks to accomplish in order to accomplish the task. a method is relevant for a task if the current state satisfies the preconditions of a method that implements that task. this task can be then substituted with the instance of the method. the substitution is basically giving the method network as a solution for the task. if there is a task “go home” and the distance to home is 3km [figure 2] and there exists a method walk-to and this method has a precondition that the distance is less than 5km, then a substation to the task “go home” can be made with this method instance. figure 2: htn method buy milk go to (shop) purchase go to (home) go-to (from, to) walk (to) if (to – from) < 5km 3rd international conference on digital interactive media in entertainment and artsfigure 3: htn method 2 an stn planning domain is a set of operatio methods m. a stn planning problem is a 4-tu state s0, the task network w called initial task stn domain. a plan � � ���� � � ��� is a soluti problem if there is a way to decompose w into π and each decomposition is applicable in the ap the world. the algorithm that is capable to networks into plans is called total-forward-deco [9] or partial-forward-decomposition (pfd). h cases where one does not want to use a forwa procedure. htn planning is generalization of s gives the planning procedure more freedom construct the task networks. in order to provide this freedom, a bookke is needed to represent constraints that the plann not yet enforced. the bookkeeping is done by unenforced constraints explicitly in the task netw the htn generalizes the definition of a stn. a task network is the pair � � ��� �� w task nodes and c is a set of constraints. eac specifies a requirement that must be satisfied by a solution to a planning problem. the definition of a method in htn also definition used in stn planning. a htn pla name, task, subtasks, and constraints. the s constraints form the task network. the htn plan identical to stn planning domains except they u instead of stn methods. compared to classical planners the prim htn planners is their sophisticated knowledge r reasoning capabilities. they can represent and non-classical planning problems; with a good guide them, they can solve classical planning p magnitude more quickly than classical or neoc the primary disadvantage of htn is the nee author to write not only a set of planning opera of methods. 3.2.2 htn planning in building game for a htn planning algorithm to be adap trees we need to define the domain (set of h operators) which is the domain of the game. thi a knowledge representation of the rules of the environments and possible strategies of game pla in this domain the game rules as well as kn tackle specific task are defined.   the implem tree building with htn is called tign implementation uses a procedure simila decomposition, but adapted to build up a game drive(to if(t go-to (from, to) if(to – from) < 5km walk (to) hod instance needs ons o and a set of uple of the initial k network and the ion for a planning π if π is executable ppropriate state of decompose these omposition (tfd) however there are ard-decomposition stn planning that m about how to eeping mechanism ning algorithm has y representing the work. a task network in where � is a set of h constraint in c y every plan that is o generalizes the an is a 4-tuple of subtasks and the nning domains are use htn methods mary advantage of representation and solve a variety of d set of htns to problems orders of classical planners. ed of the domain ators but also a set trees ted to build game htn methods and is is in some sense e game, the game ay. nown strategies to mentation of game num2 [9]. this ar to forward- tree rather than a plan. the branches of the game tree rep the methods. tignum2 applies all met state of the world to produce new continues recursively until there are n have not already been applied to th world. in the task network generated by tignu actions will occur is determined by th by listing the actions in the order network can be “serialized” into a gam 4. case based reasoning in 4.1 case based reasoning case-based reasoning (cbr) is a artificial intelligence (ai), both as problems and as a basis for standalone case-based reasoning is a paradigm solving and learning that has became applied subfield of ai of recent yea intuition that problems tend to recur. i are often similar to previously en therefore, that past solutions may be of [10]. cbr is particularly applicable to probl available, even when the domain is n for a deep domain model. helpdesks, systems have been the most successfu to determine a fault or diagnostic attributes, or to determine whether or repair is necessary given a set of past s figure 5: game tree built fr figure 4: htn to game tr ) to – from) < 200km present moves generated by thods applicable to a given w states of the world and no applicable methods that he appropriate state of the um2, the order in which the e total-ordering constraints. they will occur, the task me tree [figure 4] [figure 5]. n game strategies well established subfield of a mean for addressing ai ai technology. m for combining problem- one of the most successful ars. cbr is based on the it means that new problems ncountered problems and, f use in the current situation lems where earlier cases are not understood well enough , diagnosis or classification ul areas of application, e.g., an illness from observed r not a certain treatment or olved cases [11]. rom htn ree algorithm 3rd international conference on digital interactive media in entertainment and artscentral tasks that all cbr methods have to deal with are [12]: \"to identify the current problem situation, find a past case similar to the new one, use that case to suggest a solution to the current problem, evaluate the proposed solution, and update the system by learning from this experience. how this is done, what part of the process that is focused, what type of problems that drives the methods, etc. varies considerably, however\". while the underlying ideas of cbr can be applied consistently across application domains, the specific implementation of the cbr methods –in particular retrieval and similarity functions– is highly customized to the application at hand. 4.2 cbr and games many different implementations of cbr exist in games. cbr technology is nicely suited for recognizing complex situations much easier and more elegant than traditional parameter comparison or function evaluation. there are especially evident cases in real time strategies where different attack and defense of global strategies are nicely defined by cbr datasets and later used in the running games. also intelligent bots behavior is also another typical example. depending on the number of enemy bots the layout of the terrain and position of human players the cbr system finds the closest cbr case and employs that strategy against the human players which in prior evaluation was proved to be highly efficient. 5. game trees with ai planning – tic-tac-toe in order to show the expressive power of ai planning in defining strategies for games, and the use of these plans to build game trees i implemented an algorithm that builds game trees for the tic-tac-toe game. the game tree of tic-tac-toe shows 255,168 possible games of which 131,184 are won by x (the first player), 77904 are won by o and the rest 46,080 are draw [13]. all these games can be derived from building a complete game tree. even though it is possible to build a complete game tree of tic-tac-toe it is definitely not an optimal solution. many of the moves in this tree would be symmetrical and also there are a many moves that would be illogical or at least a bad strategy to even consider. so what strategy should x (the first player) choose in order to win the game? there are few positions that lead to certain victory. these positions involve simultaneous attack on two positions so the other player could not defend, basically the only trick in tic-tac- toe. figure 6: tic-tac-toe winning strategy positions position 1 leads to victory if the two of the three fields: top middle, bottom left corner and bottom right corner are free [figure 6]. position 2 lead to victory if two of the three fields: top right corner, bottom right corner and bottom middle are free [figure ]. and in the third position if the two of center, middle top and middle left are available the position is a certain victory. there are many different arrangements of the player’s tokens that give equivalent positions as these three positions. by using planning we do not need to consider all possible layouts but just consider these three similar to what a human would consider. the game starts from an empty table. the two relevant strategies that would lead to these positions are to take one corner or to take the center [figure 7]. figure 7: tic-tac-toe two starting moves the center position as we can see in the simulation results lead to a bigger number of victorious endings but it is also a straight forward strategy with obvious defense strategy. at this point we need to consider the moves of the opponent. if we take the left branch the opponent moves can be a center, a corner or a middle field. we also need to differentiate with a move to a corner adjacent with our like top left or bottom right or across the center to bottom right [figure 8]. figure 8: tic-tac-toe opponent response to corner move in cases one and two, we have a clear path to executing strategy 3 so we need to capture the diagonally opposite field. and as for the third case the best way to go is to capture the center and go for strategy 1 or 2 depending of the opponent’s next move. figure 9: tic-tac-toe move 2 after corner opening the first move leads to certain victory, o will have to go to the center and x will achieve strategy 3 [figure 9]. the second move is a possible way to strategy 3 if o makes a mistake in the next loop, so x goes to the opposite corner. for the third case since o is playing a valid strategy the only move that leaves a possible mistake from o would be to take the center and wait for o to go to the middle and then achieve strategy 1 or 3 which will be a symmetric situation to the one that we will find if we branched with the center. figure 10: tic-tac-toe opponent response to center move if we go back to the second branch [figure 10], a possible way for the second player to engage is corner or middle. the first 3rd international conference on digital interactive media in entertainment and artsmove is a valid strategy for o and can be mee corner move from x to try a mistake from o in the same as in the third case above from the pre another move would be go to the middle wh achieves strategy 1 or 2. figure 11: tic-tac-toe move 2 after cent the fist move will lead to win if o moves draw if it goes for the corners [figure 11]. in t has to block the lower left corner which leave middle left or corner left which are strategy 1 and to sum the strategies for the planning, first corner strategy for the beginning. then for the ce the corners with the particularly the one oppo holds. if the center is empty for the second strate we go for the opposite corner. after this point w opponent or try to implement strategies 1, 2 or victory. plan 1: take center preconditions: center empty plan 2: take corner preconditions: all corners empty plan 3: take corner after center preconditions: we have center take corner oppos opponent has plan 4: take diagonal corner preconditions: we have a corner, the opponent ha the corner opposite to the one we have is free. plan 5: block precondition: the opponent has tree tokens in a r agonal plan 6: win preconditions: we have two tokens in a row, colu nd the third place is free plan 7: tie preconditions: if all places are taken, it’s a tie. 5.1 hierarchical task network top level task is play [figure 12]. this is a can be derived into: win, block, tie or sear search for plan is derived to both plan 1 and pla plan 4, which later leads to a call for the oppon recursive call to play. figure 12: tic-tac-toe ht et with a opposite the future exactly evious branch, and here x eventually ter opening to the middle or a the second case o es x to go for the d 2. we have center or enter we try to get osite to the one o egy we go for it or we either block the r 3 which lead to site to the  one the as the ce−nter and row, colu−mn or di mn or dia−gonal a a complex task and rch for plan. the an 2 or plan 3 and nent’s move and a tn this htn when executed will re game scenarios. by creating nodes from them with branches with the move of t tree for the tic-tac-toe game over whi algorithm. this set up with 7 plans with 3 ta for tic-tac-toe which considers all pos player with only 457 games, 281 of w and 0 where the second opponent w reduction over the 255, 168 possible g tree. these reductions can be very use computing capabilities but also we pr that planning can be very efficient if d trees by applying reasoning very reasoning. further improvements to the gam the opponents moves are also planned all the meaningless and symmetrical m 6. game ai in monopoly 6.1 overview of the ai imp the ai agent is responsible for players in the game. the core principle a game tree with all the sensible move make from the current point of time minimax algorithm the agent selects t would bring the computer player mo with the highest probability. building that would be big enough to consider is obstructed by the vastness of poss with all the possible random landings nodes of the game tree exponentially tackle this problem the ai agents discussed technologies: case based re the technologies are employed first the agent searches the cbr datab largest similarity with the current state associated with a playing strategy. th that the planner needs to build plans f consecutive player moves that bring th way only moves that are part of that str being a small fraction of the overall po edges of the game tree at each level dec at each level of the game tree the of a single player. after the strateg considered the response to those strate by the opponent(s). the move of the probability distribution of the dice as player. a more general strategy needs opponent’s (human player) moves sin the expertise of the opponent. this ge more plausible moves than the focused after covering all opponents t deducting a feature move of the com cbr selected plan strategy. after strategies and reaching a reasonable s into account the memory limits an probabilities that the move is possible the dice the building of the game tre algorithm searches the game tree favorable move for the ai player usi the process is repeated each time the a esult with plans for possible m each position and linking the player we create a game ich we can run the minimax arget strategies creates a tree ssible moves for the second which x wins 176 are draw wins. this is a significant ames with a complete game eful for devices with limited rove a very important point designing meaningful game similar to human player me tree are also possible if d, in other words if we drop moves of the opponent. plementation the moves of the artificial e of the ai agent is building es that all the players would e forward. then using the the move that in the future ost favorable game position a game tree in this game sufficient number of moves sible moves in combination of the dice. the number of y grows at each level. to incorporates two already easoning and ai planning. in the following manner. base to find the case with the e of the board. this case is he strategy consists of goal for, and the plans consist of he player to that goal. this rategy are considered, those ossible moves the number of creases immensely. e model considers the moves gies of the ai player are egies needs to be considered opponent(s) depends of the well as the strategy of the s to be implemented for the nce we cannot be aware of eneral strategy would bring d strategy of the ai player. the agent comes back to mputer player by using the creating several loops of size of a game tree taking nd the rapidly decreasing e due to the distribution of ee stops. then the minimax and decides on the most ing the minimax algorithm. ai player is up. 3rd international conference on digital interactive media in entertainment and artsbuying, auctioning and trading game moves are always accompanied by return of investment calculations in making the plans. these calculations represent adaptation of the more general planning associated with the cases in the cbr database. these adaptations are necessary due to the fact that the cases do not identically correspond to the situation on the table. in addition calculating the game position value of each node of the game tree is done by heuristic functions that incorporate economic calculations of net present value, cash, and strategic layout and so on. for example railroads in monopoly are known to be strategically effective because they bring constant income even though the income can be smaller than building on other properties. 6.2 details on the cbr implementation the implementation of the cbr is by using the jcolibri2 platform.  jcolibri2 is an object-oriented framework in java for building cbr systems that is an evolution of previous work on knowledge intensive cbr [14]. for this implementation we need to look into three particular classes of the jcolibri2 platform. the standardcbrapplication, connector, cbrquery. for a jcolibri2 implementation the standardcbrapplication interface needs to be implemented. the cbr cycle executed accepts an instance of cbrquery. this class represents a cbr query to the cbr database. the description component (instance of casecomponent) represents the description of the case that will be looked up in the database. all cases and case solutions are implementing the casecomponent interface. the jcolibri2 platform connects to the cbr database via a connector class. each connector implements all the necessary methods for accessing the database, retrieval of cases, storing and deletion of cases. this implementation uses a custom xml structure for holding the cbr cases. since the game will not update the cbr database only read it, a xml solution satisfies the needs. the xml file to a certain extent is similar to the xml representation of the board. we are interested in finding one cbrcase that is the most similar case to the situation in the game at the time of the search. this procedure is done in the cycle method of the cbrapplication. the jcolibri2 cbr comparison is done by nearest neighbor (nn) search method. jcolibri2 offers implementations for nn search algorithms of simple attributes. these implementations are called local similarities. for complex attributes like in our case global customized similarity mechanisms need to be implemented. the monopolydescription class [figure 13] is basically a serialization of the gamestate. it holds all the information about the state of the board, the players, their amount of cash etc. figure 13: class diagram of the monopoly case component models on the other hand the monopolysolution class holds the three particular attributes that are needed for the planning, the planning domain, state and tasklist. the game is implemented by using the model-view- controller software development pattern. the controller is responsible for implementing the game rules and handling all of the events in the game like roll of dice, input commands for trading, auctioning and etc from the players. the view layer is responsible for displaying the board and all of the input widgets on to the game screen, and the models are data structures representing the game state [figure 14]. figure 14: class diagram of the monopoly models 6.2.1 complex similarity representation in cbr the similarity measurement part of the nearest neighbor algorithm jcolibri2 is implemented by implementing the localsimiralrityfunction and the globalsimiralityfunction interface. a local similarity function is applied to simple attributes by the nn algorithm, and a global similarity function is applied to compound attributes. in the case of our implementation the attributes of the monopolydescription are compound attributes describing the state of the board, number of players, amount of cash for every player and etc. since monopolydescription is a custom casecomponent a global similarity function needs to be implemented to accurately find the distance between different cbr cases. the similarity mechanism is inseparable core element of the cbr system. this mechanism represents how the cbr decides which strategy is best suited for the particular situation by 3rd international conference on digital interactive media in entertainment and artsfor the monopoly implementation we need to consider several basic strategies. monopoly is based on investing in properties and receiving revenues from those investments. one of the basic strategies of the game is to build a set of properties that will bring constant income larger than the one of the opponents. so in time the opponents will have to declare bankruptcy. but on the other hand over investment can lead to too stretched resources with low income that will eventually drove the player to bankruptcy. to decide on these two we need a clear separation into two groups of cases in the cbr database. the first group of cases will represent a situation on the board where the player has significant income per loop formed of one or more color group properties, maybe railroads, some buildings on them and so on. it is important to note that in this case the player is better situated than his opponents so he only needs to survive long enough to win the game. in the other group of cases either the opponent is not well positioned on the board or its opponents are better situated. in this case further investments are necessary to improve the situation so the player can have a chance of winning in the long run. these metrics can be owning color groups, valuing groups of railroads, evaluating the other opponents as well, and considering the amount of cash. as it is obvious in monopoly the number of streets is not as nearly as important as the combination of streets the player owns. it is also important to note that one cbr case does not hold only a single strategy in place, but its solution can have multiple different strategic goals. for example one cbr case might simultaneously say buy this land to form a color group but also trade some other unimportant property to increase cash amount. the cases do not represent all possible combinations of board positions. they are only representation of typical game scenarios. the cbr case solutions do not give exact instructions in general but rather strategic goals. for example one cbr solution might say trade the streets that you only have one of each for the ones that you have two of that color already. then the planner based on the situation on the board needs to decompose this high level task to a low level operations. like offer \"mediterranean avenue\" for \"reading railroad\" and offer $50. the exact amounts and actual streets are left to the planer to evaluate. the monopoly cbr database is currently in development on a monopoly clone game called spaceopoly. the cases are architected based on human player experience and knowledge. there is a plan of making a number of slightly different strategies that differ on the style of playing and then running simulation tests that would determine the particular validity of each database as well as validity of certain segments of the strategy or even particular cases in the database. the actual execution of the strategies will not differ from strategy to strategy since the plan execution is more related to the structure and rules of the game than to the actual playing strategy. 6.3 details on the planning implementation for the purpose of planning this implementation uses a modification of the jshop2 planner. the java simple hierarchical ordered planner 2 is a domain independent htn planning system [15]. jshop2 uses ordered task decomposition in reducing the htn to list of primitive tasks which form the plans. an ordered task decomposition planner is an htn planner that plans for tasks in the same order that they will be executed. this reduces the complexity of reasoning by removing a great deal of uncertainty about the world, which makes it easy to incorporate substantial expressive power into the planning algorithm. in addition to the usual htn methods and operators, the planners can make use of axioms, can do mixed symbolic/numeric conditions, and can do external function calls. in order for the jshop2 planer to generate plans it needs tree crucial components: domain, state and tasks. the domain defines all the functionalities that the particular domain offers. these are simple and complex tasks. the complex tasks also called methods create the hierarchy with the fact that they can be evaluated by simple tasks of other complex tasks. this is how a hierarchical structure of tasks is formed. the problem reduction is done by reducing the high level complex tasks to simpler until all the tasks are primitive. the list of primitive tasks forms the plan. the state represents the state of the system. it is a simple database of facts that represent the state of the system. the state is necessary to determine the way the problems or tasks are reduced to their primitive level. the reduction is done by satisfying different prerequisites set in the methods; these prerequisites are defined in the state. the tasks are high level tasks or methods defined in the domain. the planner based on the state and the goals selects one or more high level tasks that need to be reduced to plans [figure  15]. figure 15: diagram of a planner the plans then generate the game moves. the number of moves generated by the plans is just a fraction of the possible moves at that point. this reduces the game tree providing the opportunity to generate smaller and deeper game trees and making more efficient decisions in general. 7. conclusion even though the results from the cbr database are not complete at this time partial strategies are implemented as cases and recognized during game play by the cbr system. these smaller local strategies coupled with more global higher level strategies that are particularly important at the beginning of the game would form a complete cbr database and represent a knowledge engineered style of playing of the ai player. the ai planning approach is a proven method by the tic-tac- toe experiment and is suitable for implementing the strategies associated with the cbr cases. this approach in general benefits from both technologies, cbr as well as ai planning and comprises an elegant solution. even though ai planning can be enough as a single technology for some simpler problems like tic-tac-toe the complexity of monopoly would mean that the planner would have to incorporate core planner tasks plan state 3rd international conference on digital interactive media in entertainment and artslarge and complex domain and a very big state model. the cbr application helps reduce this complexity by focusing the planning on smaller domain of the game. basically the cbr reduces the overall goal of the play (wining the game) to smaller more concrete goals suitable to the particular state of the game, thus reducing the need for global planning strategies and complex planning domain. furthermore this symbiosis of technologies gives way for more precise and finely tuned strategies which can be difficult to include into global plan for the whole game. one simple example for the monopoly game would be this: sometimes it’s better to stay in jail because rolling double increases the probability of landing on some field (two, four, six, eight, ten or twelve steps from the jail) that can be of great importance to the rest of the game. these and similar small local strategies can be easily recognized by similar cases in the cbr database. in other words the system is flexible enough so that new strategies can be incorporated easily missing strategies can be also recognized by the distance metrics as well as wrong assumptions in the strategies can be easily recognized. one other important property of the system is that is highly configurable. the game its self can be diversely different depending on the configuration of the board. even though the platform is restricted to monopoly type of games, changing the layout and values of the fields effectively brings completely different properties of the game. in addition the cbr database represents the entire experience of the ai player. it can be filled with rich set of strategies or even configured with different flavors of difficulties of play, this of course coupled with the domain of the planner which can differ from a case to a case as well. 8. future work further exploration of this technology would go towards complete implementation of an ai aware agent for monopoly. initial results from the local cases with more specific strategies show cbr as a capable tool for representing expertise in playing the game. completing the more general strategies and coupling them with the planning domain will give precise results on the benefits from this architecture. there is also need for exploring the planning of strategies of opponents. this task is to some extent different because we cannot always expect the opponent to select the best move we think. in the tic-tac-toe example all possible moves of the opponent were taken into consideration, if we used the same planner for the opponent only tie games would result from the game tree. in other words mistakes of the players also need to be considered. the cbr platform brings other functionalities well worth of exploring as well. the revision stage of the jcolibri2 platform is basically capable of fine tuning strategies or even developing new strategies for the games. a well written underlying ai planning model with a capable feedback of the game tree evaluation back to the cbr revision capability can be an interesting concept in automatic experience acquisition for the ai model. there are also many other fields were combined cbr and planning approach can be incorporated into a problem solution. this combination is analogous in a big extent to a human way of reasoning. people in addition to logic of reasoning in situations with lack of information rely to planning strategies and prior experience, exactly the intuition behind cbr – ai planning architecture. 9. acknowledgments we would like to thank prof. sofia tsekeridou for her involvement in the valuable discussions we had on the topic of cbr.  10.",
                    "references": "references [1] minimax. wikipedia. [online] [cited: april 23, 2008.] http://en.wikipedia.org/wiki/minimax. [2] von neumann, j: zur theorie der gesellschaftsspiele math. annalen. 100 (1928) 295-320 [3] automated planning. wikipedia. [online] [cited: april 23, 2008.] http://en.wikipedia.org/wiki/automated_planning. [4] sanchez-ruiz, antonio, et al. game ai for a turn-based strategy game with plan adaptation and ontology-based retrieval. [5] k. erol, j. hendler, and d. nau (1994). semantics for hierarchical task-network planning. technical report tr-94- 31, umiacs. [6] smith, s. j. j. and dana s. nau, t. a. throp. a planning approach decrarer play in contract bridge. computational intelligence. 1996, vol. 12, 1. [7] one jump ahead: challenging human supremacy in checkers. j.schaeffer. s.l. : springer-verlag, 1997. [8] ibm. how deep blue works. [online] 1997. [cited: april 23, 2008.] http://www.research.ibm.com/deepblue/meet/html/d.3.2.html [9] ghallab, malik, nau, dana and traverso, paolo. automated planning theory and practice. s.l. : morgan kaufmann publishers, may 2004. isbn 1-55860-856-7. [10] case based reasoning. experiences, lessons and future. leake, david. s.l. : aaai press. mit press., 1997. [11] applying case-based reasoning: techniques for enterprise systems. watson, i. san francisco, ca, usa : morgan kaufmann publishers inc., 1998. [12] plaza, a. aamodt and e. case-based reasoning: foundational issues, methodological. ai communications. 1994, 7(i). [13] tic-tac-toe. wikipedia. [online] [cited: april 23, 2008.] http://en.wikipedia.org/wiki/tic-tac-toe. [14] díaz-agudo, b. and gonzález-calero, p. a. an architecture for knowledge intensive cbr systems. advances in case-based reasoning – (ewcbr’00). new york : springer-verlag, berlin heidelberg, 2000. [15] ilghami, okhtay and nau, dana s. a general approach to synthesize problem-specific planners. 2003.  3rd international conference on digital interactive media in entertainment and arts",
                    "state": "done",
                    "url": "https://drive.google.com/uc?id=1WKPaJSxQSiZTohZ6M7twraioUQVo8RFa&export=download",
                    "date": "2024-02-05"
                }
            },
            {
                "_index": "articles_index",
                "_id": "2",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "abstract.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "2",
                    "title": "towards a quantum software modeling language ",
                    "institutions": "university of kent canterbury, kent, united kingdom universidad autónoma de san luis potosí  san luis potosí, slp, méxico",
                    "authors": [
                        "carlos a. pérez-delgado∗",
                        "g. perez-gonzalez"
                    ],
                    "abstract": "we set down the principles behind a modeling language for quan- tum software. we present a minimal set of extensions to the well- known unified modeling language (uml) that allows it to effec- tively model quantum software. these extensions are separate and independent of uml as a whole. as such they can be used to ex- tend any other software modeling language, or as a basis for a completely new language. we argue that these extensions are both necessary and sufficient to model, abstractly, any piece of quantum software. finally, we provide a small set of examples that showcase the effectiveness of the extension set. ccs concepts • general and reference → general conference proceedings; design; • software and its engineering → system descrip- tion languages; unified modeling language (uml); software design engineering; • theory of computation → quantum computation theory; quantum information theory.",
                    "keywords": "quantum computing, software engineering, uml",
                    "content": "quantum computation rose to prominence after the discovery of quantum algorithms[5, 7] that can efficiently perform tasks that are intractable classically. these discoveries propelled research and interest in quantum computation. today, there exists prototype quantum hardware with computational capabilities beyond that of any classical machine[1]. further applications of quantum theory to computation have also been made in several areas of theory of computing, such as models of computation[6], data structures[8], and cryptography[2]. ∗both authors contributed equally to this research. permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. icsew’20, may 23–29, 2020, seoul, republic of korea © 2020 copyright held by the owner/author(s). publication rights licensed to acm. acm isbn 978-1-4503-7963-2/20/05...$15.00 https://doi.org/10.1145/3387940.3392183 quantum computation has, until today, been studied almost exclusively ‘in the small.’ a general understanding of quantum computation, or, quantum programming ‘in the large’ is yet to be developed. here we aim to set the foundations of a general frame- work for studying, developing, and conveying quantum programs. we aim to do so by developing a universal modeling language for quantum software. rather than develop such a language from scratch, we have decided to start from the well-known unified modeling language (uml)[3], and introduce a minimum set of extensions that allow it to effectively model quantum software. assuming uml to be a shared common-language upon which we can build, allows us to convey our original extensions much more succinctly. our extension set can, however, be applied with little or no modification to any other modeling language. 2 q-uml before discussing in depth the extensions we are introducing, we make a few fundamental observations on which we base the guiding principles for our extension set. our first observation is about the nature of quantum computa- tion. the central difference between quantum and classical com- putation is in how it achieves its goals. quantum computers have access to quantum algorithms[7], and quantum data-structures[8], that are unavailable to classical computers—hence their perfor- mance advantage. algorithms and data-structures are, however, implementation details. algorithms are an essential design choice while programming in the small. however, they are more often than not completely ignored in large-scale software architectural design. for instance, uml diagrams seldom portray algorithms and data-structures beyond a very high-level design perspective. it would seem then that quantum computation introduces noth- ing to computation that needs to be captured in a software design diagram. this is not the case, and the reason for this is our second observation. quantum computation changes the very nature of in- formation itself. quantum information is much richer than classical information. it is also much more challenging to store, transmit, and receive. if a module (class, object, etc.) needs to store, transmit or receive quantum information, then this is an important design consideration—which needs to be included in any effective software design. a third observation here is that the classical vs. quantum nature of the information used by a module is an important consideration both when discussing its internal implementation and its interface. furthermore, these two are separate and independent considera- tions. a classical module, implementing some classical behavior, would have no need, or capability, to communicate quantum data. a quan- tum module may or may not have to; i.e. a module’s quantum behavior may be completely part of its internal implementation 442 and not appear as part of its interface. for instance, take a module implementing shor’s algorithm. shor’s algorithm uses quantum effects to efficiently factor a large integer into its prime factors. the implementation of this module must necessarily be quantum. both the input (the large integer) and the output (the prime factors), consist of classical information. and hence, the interface of such a module can be strictly classical. more generally, we can conceive of quantum software modules that have all classical inputs and outputs (like the above example), all quantum inputs and outputs, or a mix of both. a quantum soft- ware design must address, for each individual interface element, whether it is classical input/output, or if it is quantum. in short, whether a module communicates classically or via quantum infor- mation, and whether its internal implementation requires quantum hardware are important considerations that need to be captured in a design document. the importance of such labelling should be clear. quantum data can only be stored and transmitted with special hardware designed to do so. more importantly, from an abstract, device-independent, strictly software perspective: quantum and classical information are not interchangeable. classical information is clone-able and admits fanout operations, while quantum information (in general) does not. on the other hand, quantum information has a much larger state-space. finally, it is true that quantum information is strictly a super-set of classical information—and hence a quantum module can commu- nicate any classical information it desires using a quantum interface element. we argue, however, that using a quantum interface ele- ment and messaging when classical would suffice is bad quantum software design, for the reasons stated above. in summary, the guiding principles behind any quantum software modeling language must include the following: (1) (quantum classes): whenever a software module makes use of quantum information, either as part of its internal state/implementation, or as part of its interface, this must be clearly established in a design document. (2) (quantum elements): each module interface element (e.g. public functions/methods, public variables) and internal state variables can be either classical or quantum, and must be labelled accordingly. (a) (quantum variables): each variable should be labelled as classical or quantum. if the model represents data types, the variables should also specify the classical (e.g. integer, string) or quantum (e.g. qubit, qubit array, quantum graph state) data type, (b) (quantum operations): for each operation, both the in- put and output should be clearly labelled as either classical or quantum. whether the operation internally operates quantumly should also be labelled. (3) (quantum supremacy): a module that has at least one quantum element is to be considered a quantum software module, otherwise it is a classical module. quantum and classical modules should be clearly labelled as such. (4) (quantum aggregation): any module that is composed of one or more quantum modules will itself be considered a quantum module, and must be labelled as such. (5) (quantum communication): quantum and classical mod- ules can communicate with each other as long as their inter- faces are compatible, i.e. the quantum module has classical inputs and/or outputs that can interface with the classical module. we will argue in sec. 2.3 how these extensions are not only nec- essary, but also sufficient in order to design and represent quantum software. first, in the following two sections we put these principles into practice as a set of concrete extensions to uml. 2.1 class diagram extensions uml is a very graphical language, meant to convey a lot of meaning in a very small amount of space. as such, it makes sense to use a graphical way to represent quantum software elements. we chose to do this by use of bold text to denote quantum elements, and double lines to denote a quantum relationship or quantum communication. figure 1: q-uml class diagram of shor’s algorithm. quan- tum classes and interface elements are presented in bold text, and quantum relationships use double-lines. for attributes, the name will be bold if it is represented using quantum information. for methods, we use the following conven- tion. if any of the inputs are quantum, these are bold. if the output or datatype of the method is quantum, then the datatype should also be bold. for backwards compatibility with regular uml, whenever the input or output datatypes of a method are omitted, these will be assumed to be classical in nature. if a class/object has any quantum attributes or methods then it itself is considered quantum, and its name shall also be bold. relationships between classes will use double-lines whenever the relationship is quantum in nature. for inheritance, if the superclass is quantum then the subclass, and the inheritance relationship, will also be quantum. (the converse is not necessarily true however). in the case of aggregation and composition, if a class/object being aggregated/composed is quantum, then the class/object to which it is aggregated/composed into, as well as that relationship willalso be quantum. association relationships do not have any special rules, beyond the need of a quantum class/object to have a classical interface if it is to associate with classical classes/objects. fig. 1 showcases a q-uml diagram that exemplifies the above rules. 2.2 sequence diagram extensions sequence diagrams in uml allow us to portray the dynamic rela- tionship between modules in a software program. as we did before for static relationships, we extend the existing language in order to allow us to differentiate between classical and quantum messages. as previously discussed, this is essential information. quantum information behaves differently from classical information; it can store/portray different data; it admits different operations; and, it requires different hardware to store, send, and receive. figure 2: q-uml sequence diagram of shor’s algorithm. quantum classes are presented in bold text, and quantum messages use double-lines. like before, we make use of bold text to markup quantum mod- ules, and double lines to portray quantum messages. fig. 2 shows a q-uml sequence diagram. note how even though the relationship between shorfactor and shororder is quantum, the messaging between them is not. this illustrates an important point. a module is marked as quantum if it uses quantum resources in any form, either directly as part of its internal implementation or as part of an aggregated module. if a sub-module (in uml a composed class or object) is quantum, then the encompassing module must also be marked as quantum. in a static (e.g. class) diagram, the quantum composition relationships inform us—especially in the case of a seemingly classical module that does not in itself use quantum resources—which composed modules are using quantum resources. also, note the communication between the objects shororder and qft_n. the module qft_n operates on a quantum state. hence, both ‘set’ messages are quantum. likewise, the return mes- sages ρ and ρ′ are quantum states. however, the request to perform a quantum fourier transform (qft) or a qft inverse operation can (and therefore should) be communicated classically. this dia- gram showcases the level of granularity available to us using these diagrams with the proposed extensions. 2.3 discussion we have proposed a minimal series of extensions to existing soft- ware modeling languages. we exemplify our additions in uml, but these extensions are easily applicable to any other modeling language, or be used as the basis for a new modeling language. we’ve argued the necessity of each of the extensions in previous sections. we can argue as well, that these extensions are not only necessary, but also sufficient to fully model quantum software. to make this argument, we appeal to the fact that all quantum computation is simulable using classical computation albeit with an efficiency loss. other than their use of quantum information and algorithms, quantum computers are indistinct from classical ones. hence, from a high-level design perspective, the only information element that needs to be considered when developing quantum software is when quantum (rather than classical) information is being used. the one remaining information element we have not discussed is algorithm efficiency. if quantum computation is to be used, it will most likely be due to the efficient algorithms at its disposal. that said, algorithm efficiency is not a solely quantum consider- ation. uml itself does not inherently have language elements for algorithm efficiency (beyond user-defined notes). it does, however, have several extensions used and proposed for this purpose(see e.g.[4]). other modeling languages may also have definite algorithm efficiency elements. we argue that it is best to use existing language elements when they are available. acknowledgments cp-d would like to acknowledge funding through the epsrc quan- tum communications hub (ep/t001011/1). the authors would also like to thank joanna i. ziembicka for useful comments during the preparation on this manuscript.",
                    "references": "[1] frank arute et. al. 2019. quantum supremacy using a programmable supercon- ducting processor. nature 574, 7779 (2019), 505–510. https://doi.org/10.1038/ s41586-019-1666-5 [2] charles h bennett and gilles brassard. 2014. quantum cryptography: public key distribution and coin tossing. theor. comput. sci. 560, 12 (2014), 7–11. [3] grady booch, james rumbaugh, and ivar jacobson. 2005. unified modeling lan- guage user guide, the (2nd edition) (addison-wesley object technology series). addison-wesley professional. [4] c. canevet, s. gilmore, j. hillston, m. prowse, and p. stevens. 2003. performance modelling with the unified modelling language and stochastic process algebras. iee proceedings - computers and digital techniques 150, 2 (march 2003), 107–120. https://doi.org/10.1049/ip-cdt:20030084 [5] lov k. grover. 1996. a fast quantum mechanical algorithm for database search. in proceedings of the twenty-eighth annual acm symposium on the- ory of computing (stoc ’96). acm, new york, ny, usa, 212–219. https: //doi.org/10.1145/237814.237866 [6] carlos a. pérez-delgado and donny cheung. 2007. local unitary quantum cellular automata. phys. rev. a 76 (sep 2007), 032320. issue 3. https://doi.org/10.1103/ physreva.76.032320 [7] peter w shor. 1994. algorithms for quantum computation: discrete logarithms and factoring. in proceedings 35th annual symposium on foundations of computer science. ieee, 124–134. [8] liming zhao, carlos a. pérez-delgado, and joseph f. fitzsimons. 2016. fast graph operations in quantum computation. phys. rev. a 93 (mar 2016), 032314. issue 3. https://doi.org/10.1103/physreva.93.032314",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=10lmkP91csb88F_iOpPGyVVzs8MQrtVo4&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "9",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "abstract.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "9",
                    "title": "a prototype implementation of an orthographic software modeling environment ",
                    "institutions": "colin atkinson university of mannheim, germany mannheim.de university of mannheim, germany mannheim.de university of mannheim, germany mannheim.de jacques robin universidade federal de pernambuco, recife, brasil",
                    "authors": [
                        "colin atkinson\nuniversity",
                        "dietmar stoll",
                        "christian tunjic"
                    ],
                    "abstract": "orthographic software modeling (osm) is a view-centric software engineering approach that aims to leverage the or- thographic projection metaphor used in the visualization of physical objects to visualize software systems. although the general concept of osm does not prescribe speciﬁc sets of views, a concrete osm environment has to be speciﬁc about the particular views to be used in a particular project. at the university of mannheim we are developing a prototype osm environment, naomi, that supports the views deﬁned by the kobra 2.0 method, a version of kobra adapted for osm. in this paper we provide an overview of the kobra 2.0 metamodel underpinning naomi and give a small example of its use to model a software system. categories and subject descriptors d.1.7 [programming techniques]: visual programming; d.2.2 [design tools and techniques]: computer-aided software engineering (case); d.2.6 [software engineer- ing]: programming environments—graphical environments",
                    "keywords": "orthographic software modeling, view-based modeling  1.",
                    "content": "orthographic software modeling (osm) is based on three fundamental hypotheses — (a) that it is feasible to inte- grate the many diﬀerent kinds of artifacts used in contempo- rary software engineering methods within a single coherent methodology in which they are treated as views, (b) that it permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. vao ’13, july 2, 2013, montpellier, france copyright 2013 acm 978-1-4503-2041-2 ...$15.00. is feasible to create an eﬃcient and scalable way of support- ing these views by generating them dynamically, on-the-ﬂy, from a single underlying model (sum) using model-based transformations and (c) that it is feasible to provide an in- tuitive metaphor for navigating around these many views by adapting the orthographic projection technique under- pinning the cad tools used in other engineering disciplines. figure 1: orthographic projection. as shown in figure 1, the main advantages of using the idea of orthographic projection to deﬁne the views used to visualize and described a system are that they (a) can be organized according to a simple and easy-to-understand metaphor and (b) collectively represent all the properties of a system with minimal overlap and redundancy. in practice this translates into a set of “dimensions”, each containing well deﬁned choices (or so called “dimension elements”) that can be used to select individuals views. as shown in figure 2, the main advantage of making the artifacts used to describe a software system views of a sum is that the number of pairwise coherence relationships that have to be maintained is reduced and new views can be in- troduced by simply deﬁning their relationship to the sum. moreover, the importance of this advantage grows quickly as the size of the system and the complexity of the deployed development methodology increase. another important ad- vantage is that the dominance of one particular kind of view over the development process (e.g. code) at the expense of other kinds of views (e.g. graphical models) is reduced so that any appropriate type of views can be used to enrich the underlying description of the system, depending on the needs and skills of the stakeholder involved. this makes it possible to subsume all view types under the same, overarch- sum sum / view centric environment artifact / tools centric environment figure 2: consistency dependencies in artifact-oriented versus view-oriented environments. ing development process and methodology (e.g. agile-driven, focusing on small development cycles, or model-driven de- velopment, based on transformations between abstraction levels). although the details of how the views are created from the sum and how the sum is updated from the views are not central to the approach, a natural implementation is to use the visualization and transformation technologies oﬀered by model driven software engineering (mdse). to explore the validity of these hypotheses at the uni- versity of mannheim we have been developing a prototype osm modeling environment based on an enhanced version of the kobra method for model-driven, component-oriented development, kobra 2.0 [1]. this was chosen as a basis for the prototype, known as the open, adaptable, orthographic modeling environment (naomi) [13] because its views were designed with the precise goals of being (a) genuine pro- jections of a subject containing carefully selected subsets of information about that subject, (b) minimalistic in the sense that they should overlap to the smallest extent possible and contain the minimum necessary models elements, and (c) selectable via a set of independent “dimensions” which reﬂect diﬀerent fundamental concerns of development (i.e. abstraction levels, composition or variants). in other words, kobra already provided one of the “most orthogonal” sets of views for visualizing software systems of any contempo- rary method. more details about the actual views and di- mensions deﬁned in kobra are presented in the following sections. more information on osm can be found in [2] and [3]. naomi is implemented as an eclipse plugin using the eclipse modeling framework (emf) as the underlying mod- eling platform and uml 2.0 tools [4] to generate and edit views. the kobra 2.0 metamodel on which the current version of naomi is based is a specialization of the uml metamodel composed of three separate packages — one for the sum, one for the views and one for the transformations (figure 3). the uml was chosen as the base language be- cause of its maturity and widespread acceptance, making the environment usable to the largest possible body of develop- ers. uml elements not needed in kobra 2.0 are excluded using ocl constraints while new elements or properties are kobra2 transformation sum views figure 3: kobra 2.0 top level packages. introduced by specializing existing elements. the unique contribution of this paper is to elaborate on the structure of the kobra 2.0 metamodel and how it is used to drive naomi. the three following sections each focus on one of the three main components of the metamodel — the sum, the views and the transformations . this is followed by a brief overview of the osm navigation paradigm in sec- tion 5 before a small example of the approach is presented in section 6. section 7 then concludes the paper with related and future work. 2. sum package figure 4 depicts the internal structure of the sum pack- age which is based on the uml metamodel. there are three main subpackages, two containing the structural and behav- ioral constructs respectively, and one containing the con- straints that ensure that the metaclasses are used according to the kobra conventions and rules. the classes subpackage of the structure package contains some of the most fundamental elements of the kobra meta- model, such as class and componentclass. the internal structure of this package is illustrated in figure 5. com- ponentclass represents objects with complex and reusable behaviors, while class captures simple “data type” objects that have only very simple or non-reusable behaviors. the modeler has to decide whether it is necessary to model a speciﬁc part of the system as a componentclass and include state charts and activity diagrams, or whether it is suﬃcient to use a class (which is limited to using ocl constraints). componentclass inherits (indirectly via class) from com- munications so it also has the isactive attribute. this makes  kobra2::sum::constraint::behavioral kobra2::sum::constraint::structural kobra2::sum::constraint kobra2::sum::constraint::common kobra2::sum::behavior::protocolstatemachines kobra2::sum::behavior::common kobra2::sum::behavior::activities kobra2::sum::behavior::actions kobra2::sum::behavior kobra2::sum::structure::classes kobra2::sum::structure::types kobra2::sum::structure::instances kobra2::sum::structure::elements kobra2::sum::structure kobra2::sum::constraint::oclexpressions <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> <<merge>> figure 4: kobra 2.0 sum package. it possible to model whether its instances are active or pas- sive. active objects, which can be used to model threads and processes ([8] p. 438), start to execute their behavior as soon as they are created and perform operations spontaneously. a componentclass may exhibit complex behavior. in ko- bra, this behavior may be speciﬁed in the form of uml state diagrams (deﬁning acceptable operation invocation sequences), and in the form of activities (deﬁning algorithms of operations). uml interaction elements (in sequence dia- grams) can be derived from the activity elements and thus are not included in the sum. as kobra aims to facilitate automatic checking of allowed sequences of operation calls, protocol state machines are supported instead of general state machines. since the latter include a large variety of elements not needed for specifying acceptable operation se- quences or automatic checking, ocl constraints are used to prohibit the use of unwanted features. context componentclass -- only allow activity elements or protocolstatemachines inv: ownedbehavior ->forall( ocliskindof( actitivity) or ocliskindof ( protocolstatemachine )) for example, since kobra has no concept of roles for com- ponents, the use of role also needs to be prohibited. the part association refers to owned properties of components whose attribute iscomposite is true. as kobra uses associations like nests and creates for components, part, required and provided are not needed. connectors (i.e. delegation and assembly) are not used in kobra either so ownedconnector is excluded.  class kobra2::sum::structure::classes generalizationset associationclass componentclass property usage association operation packageable element parameter acquires creates nests uml::component::packagingcomponents::component uml::commonbehaviors::communications::class +ownedoperation * +class 0..1 +supplier 1..* {subsets supplierdependency} +supplierusage * +client 1..* {subsets clientdependency} +clientusage * +ownedattribute * +class 0..1 +powertype 0..1 +powertypeextent * +packagedelement * {subsets component} +componentclass 0..1 +/superclass figure 5: kobra 2.0 classes package. context componentclass inv: role ->union(part)->union( ownedconnector ) ->union( collaborationuse )-> union( representation ) ->union( realization)->union(required) ->union(provided)->isempty () 3. views package the structure of the views package is illustrated in figure 6. again, since most of the views deﬁned in kobra 2.0 are based on uml diagrams, the view metamodels have similar elements to the sum metamodel. the big diﬀerence to the sum is that there are no restrictions on the use of the view metamodel elements. for instance, views for a particular purpose such as supporting model checkers can be supported by adding elements unrelated to the uml. the substructure of the views package reﬂects the types and organization of the kobra views according to the view “dimensions” supported in naomi (cf. example in section 6). at the top level, the views package is thus decomposed into the speciﬁcation and realization options of the encap- sulation dimension. these, in turn are both decomposed into the structural, behavioral and operational options of the projection dimension. finally, with the exception of the behavioral option, these are also all subdivided into the service and type options of the granularity dimension. this dimension, with its two options, is an addition to the original version of kobra. the service view shows the direct, publicly visible rela- tionships of the subject componentclass to other compo- nentclasses, while the type view shows the publicly visi- ble relationships of the subject to simple classes. as with the sum, constraints have been deﬁned to control what can go into each view and when they are well formed. for ev- ery view, a constraint enumerates all allowed elements (not shown in this paper). in the following, some of the other constraints for the service view are elaborated. since this view is a black-box view, the internals of componentclasses (nestedclassiﬁer) are not shown. classes are only allowed if they are generalizations of com- ponentclasses, (or any of its superclasses, since a compo- nentclass may inherit from a class as shown in the con- straints with context class. the following invariants ensure that only publicly visible attributes and operations are in this view, for both classes and componentclasses (which inherit from class).    class service type instance service type structural specification operational service type protocol behavioral kobra2::views::derived componentclassdependencies operationdependencies instance service type class service type structural realization operational service type behavioral algorithm views concretesyntax subject <<import>> <<merge>> <<merge>> <<import>> <<merge>> <<import>> figure 6: kobra 2.0 views package nesting. context class -- only allow classes that are direct or indirect generalizations of componentclasses in this view def: ccgeneralization : generalization .specific -> exists( ocliskindof ( componentclass )) inv: generalization .specific ->select( oclistypeof ( class))->exists(s|s. ccgeneralization ) or ccgeneralization -- only public attributes in this view inv: ownedattribute ->forall(visibility =# public) -- only public operations are allowed in the specification inv: ownedoperation ->forall(visibility =# public) only operation signatures are shown in this view, so pre-, post- and bodyconditions, as well as activities are omitted, which is reﬂected in the last constraint. context operation -- only the signature of the operation is shown , not its behavior (role name \"method\" refers to the activities of the operation), or dependencies inv: method ->union( precondition )->union(body)->union( postcondition )->isempty () 4. transformations package the package allviews provides the foundation for speci- fying the transformations between the sum and the views in both directions. part of the package’s contents are shown in figure 7. the abstraction concept (which is in fact a kobra2::transformation::common::allviews abstraction transformationexpression viewelement sumelement view kobra2::sum::structure::elements::element kobra2::views::concretesyntax::element kobra2::sum::constraint::behavioral::exp ressioninocl kobra2::views::subject::view {subsets mapping} 0..1 0..1 {subsets clientdependency} +abstraction 1 {subsets client} +ve 1 1..* 1 {subsets supplier} +se 1 {subsets supplierdependency} +abstraction 1..* figure 7: transformation abstractions. dependency reused from the uml but with additional con- straints) plays the key role in relating elements from the sum to elements of a view. abstraction is actually mapped to expressioninocl. when appearing in transformations, the equals sign links elements in the sum to the respective elements in the view, and vice versa. for instance, equal- ity of the general meta-association of a generalization in a transformation invariant means that, when following gen- eral, there must be an element in the sum and in the view for which similar transformation expressions are speciﬁed. in the case of kobra 2.0, which has many projections that just select a subset of elements using one-to-one abstrac- tions, this allows concise declarative transformationexpres- sions. together with the view constraints, a case tool can be implemented which uses a transformation language of the implementor’s choice, for instance the atlas transformation language (atl) [11] or qvt [9]. the role names se and ve are short for sumelement and viewelement, respectively. these roles subset the client and supplier roles from the uml. sum elements are translated into uml elements with stereotypes, so that the views are easy to manage for de- velopers familiar with the uml. the bidirectional mappings between stereotyped view elements and non-stereotyped sum elements are expressed in the constraints of the association- abstraction, a subclass of the abstraction from the allviews package. this is also an example of a transformation which is reused in other views. context associationabstraction inv: ve.memberend = se.memberend inv: ve.ownedend = se.ownedend ivn: ve. navigableownedend = se. navigableownedend inv: se. ocliskindof(acquires) implies ve. hasstereotype (’acquires ’) inv: ve. hasstereotype (’acquires ’) implies se. ocliskindof (aquires) inv: se. ocliskindof(nests) implies ve. hasstereotype (’ nests ’) inv: ve. hasstereotype (’nests ’) implies se. ocliskindof (nests) inv: se. ocliskindof (creates) implies ve. hasstereotype (’creates ’) inv: ve. hasstereotype (’creates ’) implies se. ocliskindof (creates) figure 8 shows the main elements involved in the trans- formation of the black box structural view for component- classes. the ﬁrst transformation constraint is on the view and declares the starting point for the transformation. it states that the subject componentclass and its generaliza- tions (using a sum utility function, superclosure) are in the view. the following transformation rules illustrate how to create the output (i.e. view) elements from the input (i.e. sum) el- ements, such as the publicly visible attributes and operations of the componentclass and the acquired componentclasses. the ﬁrst constraint for componentclassabstraction states that",
                    "references": "[1] c. atkinson, j. bayer, c. bunse, e. kamsties, o. laitenberger, r. laqua, d. muthig, b. paech, j. w¨ust, and j. zettel. component-based product line engineering with uml. addison wesley, reading, massachusetts, usa, 1st edition, november 2001. [2] c. atkinson, d. stoll, and p. bostan. orthographic software modeling: a practical approach to view-based development. in evaluation of novel approaches to software engineering, volume 69 of communications in computer and information science, pages 206–219. springer berlin heidelberg, 2010. [3] c. atkinson, d. stoll, and c. tunjic. orthographic service modeling. in proceedings of 15th ieee edoc conference workshops (edocw), helsinki, finland, 2011. [4] eclipse foundation. uml2tools. http://wiki.eclipse.org/mdt-uml2tools, 2013. [5] iso/iec and itu-t. the reference model of open distributed processing. rm-odp, itu-t rec. x.901-x.904 / iso/iec 10746. http://standards.iso.org/ ittf/publiclyavailablestandards/index.html, 1998. [6] j. i. j. jose raul romero and a. vallecillo. realizing correspondences in multiviewpoint speciﬁcations. in proceedings of the thirteenth ieee international edoc conference, 1 - 4 september 2009, auckland, new zealand, september 2009. [7] m. lankhorst. enterprise architecture at work. springer berlin heidelberg, 2009. [8] object management group (omg). omg uniﬁed modeling language (omg uml), superstructure, v2.1.2. http://www.omg.org/cgi-bin/doc?formal/07-11-02, november 2007. [9] object management group (omg). meta object facility (mof) 2.0 query/view/transformation, v1.0. http://www.omg.org/spec/qvt/1.0/pdf/, april 2008. [10] c. seybold, m. glinz, s. meier, and n. merlo-schett. an eﬀective layout adaptation technique for a graphical modeling tool. in proceedings of the 2003 international conference on software engineering, portland, 2003. [11] the atlas transformation language (atl). oﬃcial website. http://www.eclipse.org/atl/, 2013. [12] the open group. togaf version 9 - the open group architecture framework. http://www.opengroup.org/architecture/ togaf9-doc/arch/index.html, feb 2009. [13] university of mannheim - software engineering group. naomi - open, adaptable, orthographic modeling environment. http://eclipselabs.org/p/naomi. [14] j. a. zachman. the zachman framework: a primer for enterprise engineering and manufacturing. http://www.zachmaninternational.com, 2009.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=11TNlEEG28F1OmSjkbp5PizD_3_ls5Jj9&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "6",
                "_score": 1.0,
                "_ignored": [
                    "institutions.keyword",
                    "abstract.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "6",
                    "title": "generating diverse code explanations using the gpt-3 large language model ",
                    "institutions": "stephen macneil temple university philadelphia, pa, usa andrew tran temple university philadelphia, pa, usa dan mogil temple university philadelphia, pa, usa temple university philadelphia, pa, usa erin ross temple university philadelphia, pa, usa university of california—san diego la jolla, ca, usa",
                    "authors": [
                        "stephen macneil\nstephen.macneil@temple.edu",
                        "dan mogil\ndaniel.mogil@temple.edu",
                        "seth bernstein",
                        "ross\nerinross@temple.edu\ntemple university\nphiladelphia",
                        "ziheng huang"
                    ],
                    "abstract": "good explanations are essential to efficiently learning introductory programming concepts [10]. to provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing error-specific feedback [10, 16]. however, these ap- proaches often require manual effort to configure and only explain a single aspect of a given code segment. large language models (llms) are also changing how students interact with code [7]. for example, github’s copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. instead, our work focuses on llms’ potential to support learning by explain- ing numerous aspects of a given code snippet. this poster features a systematic analysis of the diverse natural language explanations that gpt-3 can generate automatically for a given code snippet. we present a subset of three use cases from our evolving design space of ai explanations of code. 2 use cases to understand the types of explanations gpt-3 [2] can generate, we issued over 700 prompts across numerous code snippets. an example prompt and resulting explanation is shown in figure 1. we discovered eight explanation types and figure 2 includes three permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for third-party components of this work must be honored. for all other uses, contact the owner/author(s). icer 2022, august 7–11, 2022, lugano and virtual event, switzerland © 2022 copyright held by the owner/author(s). acm isbn 978-1-4503-9195-5/22/08. https://doi.org/10.1145/3501709.3544280 explanation types to illustrate the explanatory power of gpt-3. the additional types include: 1) tracing the execution of code, 2) fixing bugs and explaining how they were fixed, 3) generating analogies to real world settings, 4) listing relevant programming concepts, and 5) predicting the console output. figure 1: a prompt and explanation based on analogy. 2.1 analyzing and explaining time complexity instructors rate time complexity as the most difficult programming topic [17]. however, understanding time complexity is important [6, 13] because it facilitates decision-making so students choose an appropriate algorithm for a given problem. this use case shows gpt-3 can identify and explain time complexity. 2.2 identifying common mistakes made by beginner programmers commonality exists in how students solve programming prob- lems [15] and the mistakes they make [1, 11]. pedagogical tech- niques, such as the ‘muddiest point’ highlight these common and most confusing concepts [3, 14]. gpt-3 can automatically create a checklist of common mistakes students might make regarding a given code snippet. figure 2: three example explanations automatically generated by gpt-3 for an ‘anonymized’ binary search code snippet. 2.3 summarizing code at multiple levels of abstraction before understanding how a code snippet executes, it is often useful to understand the purpose of the code [5]. the summary gener- ated by gpt-3 and shown in figure 2 defines the goal, traces the execution, and highlights relevant cs concepts such as arrays. 3 discussion our three use cases demonstrate the potential for gpt-3 to explain code for intro cs students. our poster presentation will feature all eight explanation types as a design space of explanations to convey the diversity of explanations that can be generated by llms. we will highlight best practices for generating effective explanations and pitfalls that lead to less effective explanations. we are evaluating the usefulness of these explanations in a series of summer classes. references [1] amjad altadmri and neil cc brown. 2015. 37 million compilations: investigating novice programming mistakes in large-scale student data. in proceedings of the 46th acm technical symposium on computer science education. 522–527. [2] tom brown, benjamin mann, nick ryder, melanie subbiah, jared d kaplan, prafulla dhariwal, arvind neelakantan, pranav shyam, girish sastry, amanda askell, et al. 2020. language models are few-shot learners. advances in neural information processing systems 33 (2020), 1877–1901. [3] adam carberry, stephen krause, casey ankeny, and cynthia waters. 2013. “unmuddying” course content using muddiest point reflections. in 2013 ieee frontiers in education conference (fie). ieee, 937–942. [4] mark chen, jerry tworek, heewoo jun, qiming yuan, henrique ponde de oliveira pinto, jared kaplan, harri edwards, yuri burda, nicholas joseph, greg brockman, et al. 2021. evaluating large language models trained on code. arxiv preprint arxiv:2107.03374 (2021). [5] kathryn cunningham, yike qiao, alex feng, and eleanor o’rourke. 2022. bring- ing \"high-level\" down to earth: gaining clarity in conversational program- mer learning goals. in proceedings of the 53rd acm technical symposium on computer science education v. 1 (providence, ri, usa) (sigcse 2022). as- sociation for computing machinery, new york, ny, usa, 551–557. https: //doi.org/10.1145/3478431.3499370 [6] elvina elvina and oscar karnalim. 2017. complexitor: an educational tool for learning algorithm time complexity in practical manner. comtech: computer, mathematics and engineering applications 8, 1 (2017), 21–27. [7] james finnie-ansley, paul denny, brett a. becker, andrew luxton-reilly, and james prather. 2022. the robots are coming: exploring the implications of ope- nai codex on introductory programming. in australasian computing education conference (virtual event, australia) (ace ’22). acm, new york, ny, usa, 10–19. https://doi.org/10.1145/3511861.3511863 [8] philip j guo. 2013. online python tutor: embeddable web-based program visual- ization for cs education. in proceeding of the 44th acm technical symposium on computer science education. 579–584. [9] andrew head, codanda appachu, marti a hearst, and björn hartmann. 2015. tutorons: generating context-relevant, on-demand explanations and demonstra- tions of online code. in 2015 ieee symposium on visual languages and human- centric computing (vl/hcc). ieee, 3–12. [10] samiha marwan, ge gao, susan fisk, thomas w. price, and tiffany barnes. 2020. adaptive immediate feedback can improve novice programming engagement and intention to persist in computer science. in proceedings of the 2020 acm conference on international computing education research (virtual event, new zealand) (icer ’20). association for computing machinery, new york, ny, usa, 194–203. https://doi.org/10.1145/3372782.3406264 [11] davin mccall and michael kölling. 2014. meaningful categorisation of novice pro- grammer errors. in 2014 ieee frontiers in education conference (fie) proceedings. ieee, 1–8. [12] greg l nelson, benjamin xie, and amy j ko. 2017. comprehension first: eval- uating a novel pedagogy and tutoring system for program tracing in cs1. in proceedings of the 2017 acm conference on international computing education research. 2–11. [13] miranda parker and colleen lewis. 2014. what makes big-o analysis difficult: understanding how students understand runtime analysis. journal of computing sciences in colleges 29, 4 (2014), 164–174. [14] daniel perez, leila zahedi, monique ross, jia zhu, tiffany vinci-cannava, laird kramer, and maria charters. 2020. wip: an exploration into the muddiest pointsand self-efficacy of students in introductory computer science courses. in 2020 ieee frontiers in education conference (fie). ieee, 1–5. [15] chris piech, mehran sahami, jonathan huang, and leonidas guibas. 2015. au- tonomously generating hints by inferring problem solving policies. in proceedings of the second (2015) acm conference on learning@ scale. 195–204. [16] thomas w price, yihuan dong, and dragan lipovac. 2017. isnap: towards intelligent tutoring in novice programming environments. in proceedings of the 2017 acm sigcse technical symposium on computer science education. 483–488. [17] carsten schulte and jens bennedsen. 2006. what do teachers teach in introductory programming?. in proceedings of the second international workshop on computing education research. 17–28.",
                    "keywords": "large language models, natural language processing, code explana- tions, computer science education",
                    "content": null,
                    "references": "[1] amjad altadmri and neil cc brown. 2015. 37 million compilations: investigating novice programming mistakes in large-scale student data. in proceedings of the 46th acm technical symposium on computer science education. 522–527. [2] tom brown, benjamin mann, nick ryder, melanie subbiah, jared d kaplan, prafulla dhariwal, arvind neelakantan, pranav shyam, girish sastry, amanda askell, et al. 2020. language models are few-shot learners. advances in neural information processing systems 33 (2020), 1877–1901. [3] adam carberry, stephen krause, casey ankeny, and cynthia waters. 2013. “unmuddying” course content using muddiest point reflections. in 2013 ieee frontiers in education conference (fie). ieee, 937–942. [4] mark chen, jerry tworek, heewoo jun, qiming yuan, henrique ponde de oliveira pinto, jared kaplan, harri edwards, yuri burda, nicholas joseph, greg brockman, et al. 2021. evaluating large language models trained on code. arxiv preprint arxiv:2107.03374 (2021). [5] kathryn cunningham, yike qiao, alex feng, and eleanor o’rourke. 2022. bring- ing \"high-level\" down to earth: gaining clarity in conversational program- mer learning goals. in proceedings of the 53rd acm technical symposium on computer science education v. 1 (providence, ri, usa) (sigcse 2022). as- sociation for computing machinery, new york, ny, usa, 551–557. https: //doi.org/10.1145/3478431.3499370 [6] elvina elvina and oscar karnalim. 2017. complexitor: an educational tool for learning algorithm time complexity in practical manner. comtech: computer, mathematics and engineering applications 8, 1 (2017), 21–27. [7] james finnie-ansley, paul denny, brett a. becker, andrew luxton-reilly, and james prather. 2022. the robots are coming: exploring the implications of ope- nai codex on introductory programming. in australasian computing education conference (virtual event, australia) (ace ’22). acm, new york, ny, usa, 10–19. https://doi.org/10.1145/3511861.3511863 [8] philip j guo. 2013. online python tutor: embeddable web-based program visual- ization for cs education. in proceeding of the 44th acm technical symposium on computer science education. 579–584. [9] andrew head, codanda appachu, marti a hearst, and björn hartmann. 2015. tutorons: generating context-relevant, on-demand explanations and demonstra- tions of online code. in 2015 ieee symposium on visual languages and human- centric computing (vl/hcc). ieee, 3–12. [10] samiha marwan, ge gao, susan fisk, thomas w. price, and tiffany barnes. 2020. adaptive immediate feedback can improve novice programming engagement and intention to persist in computer science. in proceedings of the 2020 acm conference on international computing education research (virtual event, new zealand) (icer ’20). association for computing machinery, new york, ny, usa, 194–203. https://doi.org/10.1145/3372782.3406264 [11] davin mccall and michael kölling. 2014. meaningful categorisation of novice pro- grammer errors. in 2014 ieee frontiers in education conference (fie) proceedings. ieee, 1–8. [12] greg l nelson, benjamin xie, and amy j ko. 2017. comprehension first: eval- uating a novel pedagogy and tutoring system for program tracing in cs1. in proceedings of the 2017 acm conference on international computing education research. 2–11. [13] miranda parker and colleen lewis. 2014. what makes big-o analysis difficult: understanding how students understand runtime analysis. journal of computing sciences in colleges 29, 4 (2014), 164–174. [14] daniel perez, leila zahedi, monique ross, jia zhu, tiffany vinci-cannava, laird kramer, and maria charters. 2020. wip: an exploration into the muddiest pointsand self-efficacy of students in introductory computer science courses. in 2020 ieee frontiers in education conference (fie). ieee, 1–5. [15] chris piech, mehran sahami, jonathan huang, and leonidas guibas. 2015. au- tonomously generating hints by inferring problem solving policies. in proceedings of the second (2015) acm conference on learning@ scale. 195–204. [16] thomas w price, yihuan dong, and dragan lipovac. 2017. isnap: towards intelligent tutoring in novice programming environments. in proceedings of the 2017 acm sigcse technical symposium on computer science education. 483–488. [17] carsten schulte and jens bennedsen. 2006. what do teachers teach in introductory programming?. in proceedings of the second international workshop on computing education research. 17–28.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1waClmbcgBUfSP7vA7tfalpSnGGj-hnVR&export=download",
                    "date": null
                }
            },
            {
                "_index": "articles_index",
                "_id": "10",
                "_score": 1.0,
                "_ignored": [
                    "content.keyword",
                    "references.keyword"
                ],
                "_source": {
                    "article_id": "10",
                    "title": "numerical computing in engineering mathematics ",
                    "institutions": "firuz kamalov department of electrical engineering canadian university dubai dubai, uae ho-hon leung department of mathematical sciences united arab emirates university",
                    "authors": [
                        "al ain"
                    ],
                    "abstract": null,
                    "keywords": null,
                    "content": "the 4th industrial revolution has had a dramatic impact on the engineering profession. the modern technologies such as artiﬁcial intelligence, the internet of things, and advanced robotics have altered engineering systems and processes. to- day’s engineers are expected to be able to leverage these resources to produce their products. to meet the new profes- sional requirements, engineering educational institutions have revised their curricula. the changes in the curricula include both updating the existing programs as well as introduc- ing completely new programs. given the rapid technological progress, universities and colleges around the world are con- tinuously adapting to the ever-changing environment. while a signiﬁcant progress in modernizing the engineering curriculum has been achieved, there still remains room for improvement. catalyzed by the exponential increase in computational power and interconnectedness, the modern industrial revolu- tion has reshaped the skills and competencies required of the engineers. the changes in engineering curricula in response to industry 4.0 have been threefold: i) modernizing the existing programs, ii) introduction of new programs, and iii) revising the pedagogical approach. modernizing the existing programs involves introduction of new courses in the study plan related to emerging technologies. in addition, existing courses can be updated with new content. fresh new programs in emerging © 2022 ieee. personal use of this material is permitted. permission from ieee must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works technologies are also introduced by universities and colleges. many institutions now offer degrees in artiﬁcial intelligence and mechatronics which were not there 20 years ago. finally, universities have revised their approaches to course delivery. student-centered learning, project-based learning, and applied learning have become popular in the new engineering educa- tional paradigm. while signiﬁcant effort has been made to revise the core engineering courses, the auxiliary courses in mathematics and sciences received little consideration. the mathematics and sciences courses play a key role in the engineering curriculum. given their importance, the curriculum updates must also be extended to the auxiliary courses. by implementing a com- prehensive update of the engineering curriculum that includes both the core and auxiliary courses, a more effective outcome can be achieved. the goal of this paper is to propose a modernized engineer- ing mathematics curriculum in line with the broader efforts to update engineering education to adapt to industry 4.0. the key feature of the new curriculum is the introduction numerical computing in the existing mathematics courses. the latest industrial revolution has been driven largely by the dramatic increase in computational power. therefore, today’s engineers must be well-equipped to leverage the computing power in their work. since mathematics courses are usually taken at the begin- ning of the study plan, it offers a natural avenue for introducing numerical computing to students. furthermore, many problems in mathematics can be solved numerically making it natural to integrate numerical computing in mathematics courses. by studying numerical computing in mathematics courses, students will acquire the necessary theoretical and practical skills to apply in their downstream, specialized engineering courses. this paper is structured as follows. section 2 provides an overview of the existing efforts to update the engineering curriculum in response to industry 4.0. section 3 discusses the current approaches to integrate scientiﬁc computing in mathematics courses. in section 4, we present our proposal for modernizing the mathematics curriculum to integrate nu- merical computing. section 5 concludes the paper with ﬁnal remarks.engineering departments in colleges and universities have made signiﬁcant changes in their curricula in response to the new environment created by the recent, rapid advances in technology. in particular, the existing programs have been updated to include courses that target emerging technologies. completely new programs related to ai and mechatronics have also been adopted by universities. innovations in the ﬁeld of engineering education continue to take place with new developments on the horizon. there exist several studies investigating the modern en- gineering curricula and evaluating their effectiveness. it is argued in [6] that engineering educators must prepare their students to face three key challenges: sustainability, the 4th industrial revolution, and employability. the authors ﬁnd that colleges and universities are responding to these challenges by emphasizing student-centered learning, integration of theory and practice, digital and online learning, and the deﬁnition of professional competencies. in particular, response to the needs of industry 4.0 require interdisciplinary collaboration across several programs and disciplines. interaction and integration of technologies plays a key role in this process [10], [12]. in- terdisciplinary engineering education requires sound pedagogy and teaming experiences to encourage student in collaborative and interdisciplinary practice [23]. digital and online learning have become an important part of modern education including in the ﬁeld of engineering. information technologies play a vital role in delivering digital learning to students. colleges and universities have made signiﬁcant investments to improve their information and com- munication technology (ict) capacities [7]. in response to the needs of industry 4.0, some univer- sities have adopted the framework of education 4.0 [15], [19]. the new education framework consists of four main components: i) competencies, ii) learning methods, iii) ict, and iv) infrastructure. students competencies are based on technological knowledge and skills for successful workplace performance, while the learning methods are based on problem solving and challenge-based learning. in particular, active and project-based learning plays an important role in education 4.0 [4], [8]. other innovative approaches to learning such as virtual-reality based engineering education can help improve the learning process related to industry 4.0 [20]. in addition to technological progress, socio-cultural shifts must be taken into account in revising engineering curriculum. the new generation of students has its unique worldview which needs to be considered by the educators. in particular, the new generation is signiﬁcantly affected by mobile devices and digital media. educational content must be tailored to the new student p",
                    "references": "[1] abdul majid, m., huneiti, z., balachandran, w., & al-naafa, m. a. (2012) a study of the effects of using matlab as a pedagogical tool for engineering mathematics students. 15th international conference on interactive collaborative learning (icl), 10.1109/icl.2012.6402183, villach, austria. [2] abdul majid, m., huneiti, z. a., balachandran, w., & balarabe, y. (2013) matlab as a teaching and learning tool for mathematics: a literature review. international journal of arts and sciences, 6(3), 23-44. [3] brake, m. l. (2007) matlab as a tool to increase the math self- conﬁdence and the math ability of first-year engineering technology students. the scholarship of teaching and learning at emu: vol. 1, article 5. [4] chen, j., kolmos, a., & du, x. (2021). forms of implementation and challenges of pbl in engineering education: a review of literature. european journal of engineering education, 46(1), 90-115. [5] cretchley, p., harman, c., ellerton, n., & fogarty, g. (2000) matlab in early undergraduate mathematics: an investigation into the effects of scientiﬁc software on learning. mathematics education research journal, 12(3), 219-233. [6] hadgraft, r. g., & kolmos, a. (2020). emerging learning environments in engineering education. australasian journal of engineering educa- tion, 25(1), 3-16. [7] hernandez-de-menendez, m., & morales-menendez, r. (2019). tech- nological innovations and practices in engineering education: a review. international journal on interactive design and manufacturing (ijidem), 13(2), 713-728. [8] hern´andez-de-men´endez, m., guevara, a. v., mart´ınez, j. c. t., alc´antara, d. h., & morales-menendez, r. (2019). active learning in engineering education. a review of fundamentals, best practices and ex- periences. international journal on interactive design and manufacturing (ijidem), 13(3), 909-922. [9] kamalov, f., moussa, s., zgheib, r., & mashaal, o. (2020, december). feature selection for intrusion detection systems. in 2020 13th interna- tional symposium on computational intelligence and design (iscid) (pp. 265-269). ieee. [10] kamalov, f., sulieman, h., & santandreu calonge, d. (2021). machine learning based approach to exam cheating detection. plos one, 16(8), e0254340. [11] kilicman, a., hassan, m. a., & said husain, s. k. (2010) teaching and learning using mathematics software ’the new challenge’. procedia social and behavioral sciences, 8, 613-619. [12] lorenz, m., r¨ußmann, m., strack, r., lueth, k. l., & bolle, m. (2015). man and machine in industry 4.0: how will technology transform the industrial workforce through 2025. the boston consulting group, 2. [13] maur´ıcio, r. d. a., veado, l., moreira, r. t., figueiredo, e., & costa, h. (2018). a systematic mapping study on game-related methods for software engineering education. information and software technology, 95, 201-218. [14] mezhennaya, n. m. & pugachev, o. v. (2019) on perception of computer algebra systems and microsoft excel by engineering students. problems on education in the 21st century, 77(3), 379-395. [15] miranda, j., navarrete, c., noguez, j., molina-espinosa, j. m., ram´ırez- montoya, m. s., navarro-tuch, s. a., ... & molina, a. (2021). the core components of education 4.0 in higher education: three case studies in engineering education. computers & electrical engineering, 93, 107278. [16] moore, k., & frazier, r. s. (2017). engineering education for generation z. american journal of engineering education (ajee), 8(2), 111-126. [17] ortiz-rojas, m., chiluiza, k., & valcke, m. (2019). gamiﬁcation through leaderboards: an empirical study in engineering education. computer applications in engineering education, 27(4), 777-788. [18] puhak, r. i. (2011). teaching applied calculus utilizing matlab, proceedings of the twenty-third annual international conference on technology in collegiate mathematics, isbn 0-321-68984-4, copyright (c) 2012 by pearson education, inc. [19] ramirez-mendoza, r. a., morales-menendez, r., iqbal, h., & parra- saldivar, r. (2018, april). engineering education 4.0:—proposal for a new curricula. in 2018 ieee global engineering education conference (educon) (pp. 1273-1282). ieee. [20] salah, b., abidi, m. h., mian, s. h., krid, m., alkhalefah, h., & abdo, a. (2019). virtual reality-based engineering education to enhance manufacturing sustainability in industry 4.0. sustainability, 11(5), 1477. [21] strayhorn, t. l. (2006). college in the information age: gains associated with students’ use of technology. journal of interactive online learning, 5(2), 143-155. [22] thabtah, f., kamalov, f., & rajab, k. (2018). a new computational intelligence approach to detect autistic features for autism screening. international journal of medical informatics, 117, 112-124. [23] van den beemt, a., macleod, m., van der veen, j., van de ven, a., van baalen, s., klaassen, r., & boon, m. (2020). interdisciplinary engineering education: a review of vision, teaching, and support. journal of engineering education, 109(3), 508-555.",
                    "state": "pending",
                    "url": "https://drive.google.com/uc?id=1T_fGkuCKnWrcHO1a0ZmXcBY5LtpBZYnx&export=download",
                    "date": null
                }
            }
        ]
    }
}